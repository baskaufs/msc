{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This script is based on instructions given in [this lesson](https://github.com/HeardLibrary/digital-scholarship/blob/master/code/scrape/pylesson/lesson2-api.ipynb). \n",
    "\n",
    "## Import libraries and load API key from file\n",
    "\n",
    "The API key should be the only item in a text file called `flickr_api_key.txt` located in the user's home directory. No trailing newline and don't include the \"secret\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "from time import sleep\n",
    "import webbrowser\n",
    "\n",
    "# define some canned functions we need to use\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def write_dicts_to_csv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "home = str(Path.home()) #gets path to home directory; supposed to work for Win and Mac\n",
    "key_filename = 'flickr-api-keys-tang-song.txt'\n",
    "api_key_path = home + '/' + key_filename\n",
    "\n",
    "try:\n",
    "    with open(api_key_path, 'rt', encoding='utf-8') as file_object:\n",
    "        api_key = file_object.read()\n",
    "        # print(api_key) # delete this line once the script is working; don't want the key as part of the notebook\n",
    "except:\n",
    "    print(key_filename + ' file not found - is it in your home directory?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a test API call to the account\n",
    "\n",
    "We need to know the user ID. Go to flickr.com, and search for vutheatre. The result is https://www.flickr.com/photos/123262983@N05 which tells us that the ID is 123262983@N05 . There are a lot of kinds of searches we can do. A list is [here](https://www.flickr.com/services/api/).  Let's try `flickr.people.getPhotos` (described [here](https://www.flickr.com/services/api/flickr.people.getPhotos.html)).  This method doesn't actually get the photos; it gets metadata about the photos for an account.\n",
    "\n",
    "The main purpose of this query is to find out the number of photos that are available so that we can know how to set up the next part. The number of photos is in `['photos']['total']`, so we can extract that from the response data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"photos\": {\n",
      "        \"page\": 1,\n",
      "        \"pages\": 205,\n",
      "        \"perpage\": 1,\n",
      "        \"total\": 205,\n",
      "        \"photo\": [\n",
      "            {\n",
      "                \"id\": \"52938328347\",\n",
      "                \"owner\": \"30365320@N04\",\n",
      "                \"secret\": \"e3341fdd7a\",\n",
      "                \"server\": \"65535\",\n",
      "                \"farm\": 66,\n",
      "                \"title\": \"jiashan \\u5047\\u5c71\",\n",
      "                \"ispublic\": 1,\n",
      "                \"isfriend\": 0,\n",
      "                \"isfamily\": 0,\n",
      "                \"ownername\": \"tgmill\",\n",
      "                \"dateadded\": \"1685494158\"\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"stat\": \"ok\"\n",
      "}\n",
      "\n",
      "Number of photos:  205\n"
     ]
    }
   ],
   "source": [
    "user_id = '14665661@N20' # vutheatre's ID\n",
    "endpoint_url = 'https://www.flickr.com/services/rest'\n",
    "method = 'flickr.groups.pools.getPhotos'\n",
    "filename = 'miller-metadata.csv'\n",
    "\n",
    "param_dict = {\n",
    "    'method' : method,\n",
    "#    'tags' : 'kangaroo',\n",
    "#    'extras' : 'url_o',\n",
    "    'per_page' : '1',  # default is 100, maximum is 500. Use paging to retrieve more than 500.\n",
    "    'page' : '1',\n",
    "    'group_id' : user_id,\n",
    "    'oauth_consumer_key' : api_key,\n",
    "    'nojsoncallback' : '1', # this parameter causes the API to return actual JSON instead of its weird default string\n",
    "    'format' : 'json' # overrides the default XML serialization for the search results\n",
    "    }\n",
    "\n",
    "metadata_response = requests.get(endpoint_url, params = param_dict)\n",
    "\n",
    "# print(metadata_response.url) # uncomment this if testing is needed, again don't reveal key in notebook\n",
    "data = metadata_response.json()\n",
    "print(json.dumps(data, indent=4))\n",
    "print()\n",
    "\n",
    "number_photos = int(data['photos']['total']) # need to convert string to number\n",
    "print('Number of photos: ', number_photos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test to see what kinds of useful metadata we can get\n",
    "\n",
    "The instructions for the [method](https://www.flickr.com/services/api/flickr.people.getPhotos.html) says what kinds of \"extras\" you can request metadata about. Let's ask for everything that we care about and don't already know: \n",
    "\n",
    "`description,license,original_format,date_taken,original_format,geo,tags,machine_tags,media,url_t,url_o`\n",
    "\n",
    "`url_t` is the URL for a thumbnail of the image and `url_o` is the URL to retrieve the original photo. The dimensions of these images will be given automatically when we request the URLs, so we don't need `o_dims`. There isn't any place to request the title, since it's automatically returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"photos\": {\n",
      "        \"page\": 1,\n",
      "        \"pages\": 205,\n",
      "        \"perpage\": 1,\n",
      "        \"total\": 205,\n",
      "        \"photo\": [\n",
      "            {\n",
      "                \"id\": \"52938328347\",\n",
      "                \"owner\": \"30365320@N04\",\n",
      "                \"secret\": \"e3341fdd7a\",\n",
      "                \"server\": \"65535\",\n",
      "                \"farm\": 66,\n",
      "                \"title\": \"jiashan \\u5047\\u5c71\",\n",
      "                \"ispublic\": 1,\n",
      "                \"isfriend\": 0,\n",
      "                \"isfamily\": 0,\n",
      "                \"ownername\": \"tgmill\",\n",
      "                \"dateadded\": \"1685494158\",\n",
      "                \"license\": \"0\",\n",
      "                \"description\": {\n",
      "                    \"_content\": \"jiashan of yellow stone, &quot;Yungang,&quot;  Wangshiyuan (Master of the Nets Garden), Qing dynasty, Suzhou, Jiangsu Province, detail showing interior &quot;grotto&quot;  \\u7db2\\u5e2b\\u5712\\u96f2\\u5d17\\u5047\\u5c71\\uff0c \\u5c40\\u90e8\\uff0c\\u9ec3\\u77f3\\u4e0b\\u6709\\u6d1e\\uff0c\\u6c5f\\u8607\\u7701\\u8607\\u5dde\\uff08photo: Jin Yinuo \\u91d1\\u4e00\\u8afe, 2023\\uff09\"\n",
      "                },\n",
      "                \"datetaken\": \"2023-05-30 19:44:40\",\n",
      "                \"datetakengranularity\": 0,\n",
      "                \"datetakenunknown\": \"1\",\n",
      "                \"tags\": \"master fishing nets garden suzhou \\u7db2\\u5e2b\\u5712 \\u6c5f\\u8607\\u7701\\u8607\\u5dde\",\n",
      "                \"machine_tags\": \"\",\n",
      "                \"originalsecret\": \"b117da8b39\",\n",
      "                \"originalformat\": \"jpg\",\n",
      "                \"latitude\": 0,\n",
      "                \"longitude\": 0,\n",
      "                \"accuracy\": 0,\n",
      "                \"context\": 0,\n",
      "                \"media\": \"photo\",\n",
      "                \"media_status\": \"ready\",\n",
      "                \"url_t\": \"https://live.staticflickr.com/65535/52938328347_e3341fdd7a_t.jpg\",\n",
      "                \"height_t\": 75,\n",
      "                \"width_t\": 100,\n",
      "                \"url_o\": \"https://live.staticflickr.com/65535/52938328347_b117da8b39_o.jpg\",\n",
      "                \"height_o\": 3024,\n",
      "                \"width_o\": 4032\n",
      "            }\n",
      "        ]\n",
      "    },\n",
      "    \"stat\": \"ok\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    'method' : method,\n",
    "    'extras' : 'description,license,original_format,date_taken,original_format,geo,tags,machine_tags,media,url_t,url_o',\n",
    "    'per_page' : '1',  # default is 100, maximum is 500. Use paging to retrieve more than 500.\n",
    "    'page' : '1',\n",
    "    'group_id' : user_id,\n",
    "    'oauth_consumer_key' : api_key,\n",
    "    'nojsoncallback' : '1', # this parameter causes the API to return actual JSON instead of its weird default string\n",
    "    'format' : 'json' # overrides the default XML serialization for the search results\n",
    "    }\n",
    "\n",
    "metadata_response = requests.get(endpoint_url, params = param_dict)\n",
    "# print(metadata_response.url) # uncomment this if testing is needed, again don't reveal key in notebook\n",
    "\n",
    "data = metadata_response.json()\n",
    "print(json.dumps(data, indent=4))\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and test the function to extract the data we want\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def extract_data(photo_number, data):\n",
    "    dictionary = {} # create an empty dictionary\n",
    "\n",
    "    # load the response data into a dictionary\n",
    "    dictionary['id'] = data['photos']['photo'][photo_number]['id']\n",
    "    dictionary['title'] = data['photos']['photo'][photo_number]['title']\n",
    "    dictionary['license'] = data['photos']['photo'][photo_number]['license']\n",
    "    dictionary['description'] = data['photos']['photo'][photo_number]['description']['_content']\n",
    "\n",
    "    # convert the stupid date format to ISO 8601 dateTime; don't know the time zone - maybe add later?\n",
    "    temp_time = data['photos']['photo'][photo_number]['datetaken']\n",
    "    dictionary['date_taken'] = temp_time.replace(' ', 'T')\n",
    "\n",
    "    dictionary['tags'] = data['photos']['photo'][photo_number]['tags']\n",
    "    dictionary['machine_tags'] = data['photos']['photo'][photo_number]['machine_tags']\n",
    "    dictionary['original_format'] = data['photos']['photo'][photo_number]['originalformat']\n",
    "    dictionary['latitude'] = data['photos']['photo'][photo_number]['latitude']\n",
    "    dictionary['longitude'] = data['photos']['photo'][photo_number]['longitude']\n",
    "    dictionary['thumbnail_url'] = data['photos']['photo'][photo_number]['url_t']\n",
    "    dictionary['original_url'] = data['photos']['photo'][photo_number]['url_o']\n",
    "    dictionary['original_height'] = data['photos']['photo'][photo_number]['height_o']\n",
    "    dictionary['original_width'] = data['photos']['photo'][photo_number]['width_o']\n",
    "    \n",
    "    return dictionary\n",
    "\n",
    "# test the function with a single row\n",
    "table = []\n",
    "\n",
    "photo_number = 0\n",
    "photo_dictionary = extract_data(photo_number, data)\n",
    "table.append(photo_dictionary)\n",
    "\n",
    "# write the data to a file\n",
    "fieldnames = photo_dictionary.keys() # use the keys from the last dictionary for column headers; assume all are the same\n",
    "write_dicts_to_csv(table, filename, fieldnames)\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the loops to do the paging\n",
    "\n",
    "Flickr limits the number of photos that can be requested to 500. Since we have more than that, we need to request the data 500 photos at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "retrieving page  1\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "per_page = 500   # use 500 for full download, use smaller number like 5 for testing\n",
    "pages = number_photos // per_page   # the // operator returns the integer part of the division (\"floor\")\n",
    "table = []\n",
    "\n",
    "for page_number in range(0, pages + 1):  # need to add one to get the final partial page\n",
    "#for page_number in range(0, 1):  # use this to do only one page for testing\n",
    "    print('retrieving page ', page_number + 1)\n",
    "    page_string = str(page_number + 1)\n",
    "    param_dict = {\n",
    "        'method' : method,\n",
    "        'extras' : 'description,license,original_format,date_taken,original_format,geo,tags,machine_tags,media,url_t,url_o',\n",
    "        'per_page' : str(per_page),  # default is 100, maximum is 500.\n",
    "        'page' : page_string,\n",
    "        'group_id' : user_id,\n",
    "        'oauth_consumer_key' : api_key,\n",
    "        'nojsoncallback' : '1', # this parameter causes the API to return actual JSON instead of its weird default string\n",
    "        'format' : 'json' # overrides the default XML serialization for the search results\n",
    "        }\n",
    "    metadata_response = requests.get(endpoint_url, params = param_dict)\n",
    "    data = metadata_response.json()\n",
    "#    print(json.dumps(data, indent=4))  # uncomment this line for testing\n",
    "    \n",
    "    # data['photos']['photo'] is the number of photos for which data was returned\n",
    "    for image_number in range(0, len(data['photos']['photo'])):\n",
    "        photo_dictionary = extract_data(image_number, data)\n",
    "        table.append(photo_dictionary)\n",
    "\n",
    "    # write the data to a file\n",
    "    # We could just do this for all the data at the end.\n",
    "    # But if the search fails in the middle, we will at least get partial results\n",
    "    fieldnames = photo_dictionary.keys() # use the keys from the last dictionary for column headers; assume all are the same\n",
    "    write_dicts_to_csv(table, filename, fieldnames)\n",
    "\n",
    "    sleep(1) # wait a second to avoid getting blocked for hitting the API to rapidly\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download the images to a local directory.\n",
    "\n",
    "Before running this cell, add a column called image_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52938328347.jpg\n",
      "52909252587.jpg\n",
      "52908806535.jpg\n",
      "52905463137.jpg\n",
      "52763297178.jpg\n",
      "52762775821.jpg\n",
      "52762312780.jpg\n",
      "52762312795.jpg\n",
      "52762157619.jpg\n",
      "52761361362.jpg\n",
      "52761361382.jpg\n",
      "52760165567.jpg\n",
      "52749647023.jpg\n",
      "52739995797.jpg\n",
      "52740739089.jpg\n",
      "52737347115.jpg\n",
      "52736784674.jpg\n",
      "52736527921.jpg\n",
      "52736761524.jpg\n",
      "52735974457.jpg\n",
      "52729746371.jpg\n",
      "52729641751.jpg\n",
      "52726033636.jpg\n",
      "52711796280.jpg\n",
      "52705293679.jpg\n",
      "52699855437.jpg\n",
      "52698644285.jpg\n",
      "52270792221.jpg\n",
      "52247309391.jpg\n",
      "52244240167.jpg\n",
      "52245212591.jpg\n",
      "51734052350.jpg\n",
      "52243339731.jpg\n",
      "52243231095.jpg\n",
      "52225051557.jpg\n",
      "52225990416.jpg\n",
      "52220742225.jpg\n",
      "52185457598.jpg\n",
      "52184610640.jpg\n",
      "52183425289.jpg\n",
      "52182248038.jpg\n",
      "52182252156.png\n",
      "52182240896.jpg\n",
      "52182613380.jpg\n",
      "52182607685.png\n",
      "52182118053.png\n",
      "52180012568.jpg\n",
      "52168682808.png\n",
      "52166485361.png\n",
      "52165321614.jpg\n",
      "52164247456.jpg\n",
      "52164482254.jpg\n",
      "52163211072.jpg\n",
      "52164721640.png\n",
      "51312063806.jpg\n",
      "51277669993.jpg\n",
      "51273946031.jpg\n",
      "51273155892.jpg\n",
      "51273831066.jpg\n",
      "51274177544.jpg\n",
      "51272659777.jpg\n",
      "51272443557.jpg\n",
      "51273884904.jpg\n",
      "51273149921.jpg\n",
      "51273295433.jpg\n",
      "51272358062.jpg\n",
      "51273062306.jpg\n",
      "51274000060.jpg\n",
      "51273692164.jpg\n",
      "51271089077.jpg\n",
      "51272856055.jpg\n",
      "51270917657.jpg\n",
      "51271646746.jpg\n",
      "51271929049.jpg\n",
      "51271321083.jpg\n",
      "51271831469.jpg\n",
      "51271148768.jpg\n",
      "51271699104.jpg\n",
      "51270633319.jpg\n",
      "51270686055.jpg\n",
      "51270326294.jpg\n",
      "51268846272.jpg\n",
      "51265129277.jpg\n",
      "51261374915.jpg\n",
      "51260513028.jpg\n",
      "51257712641.jpg\n",
      "51244357465.jpg\n",
      "51240572631.jpg\n",
      "51168404377.jpg\n",
      "51151712553.jpg\n",
      "51151480841.jpg\n",
      "51151480821.jpg\n",
      "51161075871.jpg\n",
      "51160394722.png\n",
      "51161820955.jpg\n",
      "51159940752.jpg\n",
      "51161690595.jpg\n",
      "51159005643.jpg\n",
      "51159477774.jpg\n",
      "51158877903.jpg\n",
      "51158192711.jpg\n",
      "51151480861.jpg\n",
      "51150352112.jpg\n",
      "51147259732.jpg\n",
      "51149022330.jpg\n",
      "51149009405.jpg\n",
      "51147916251.jpg\n",
      "51148087718.gif\n",
      "51146567120.jpg\n",
      "51142535907.jpg\n",
      "51144314945.jpg\n",
      "51143056548.jpg\n",
      "51142587928.jpg\n",
      "51138340997.jpg\n",
      "51140114380.jpg\n",
      "51139226553.jpg\n",
      "51140114445.jpg\n",
      "51138333287.jpg\n",
      "51140114485.jpg\n",
      "51138333337.jpg\n",
      "51139009476.jpg\n",
      "51139226678.jpg\n",
      "51139009531.jpg\n",
      "51140114550.jpg\n",
      "51138333467.jpg\n",
      "51140114590.jpg\n",
      "51139009616.jpg\n",
      "51139784764.jpg\n",
      "51139226833.jpg\n",
      "51139784804.jpg\n",
      "51139226888.jpg\n",
      "51139784834.jpg\n",
      "51139784859.jpg\n",
      "51132246458.jpg\n",
      "51131142314.jpg\n",
      "51123193549.jpg\n",
      "51123429521.jpg\n",
      "51124228570.jpg\n",
      "51123429601.jpg\n",
      "51124228750.jpg\n",
      "51116426121.jpg\n",
      "51115862497.jpg\n",
      "51116426191.jpg\n",
      "51088823122.jpg\n",
      "51088737641.jpg\n",
      "51088823227.jpg\n",
      "51088737726.jpg\n",
      "51089344660.jpg\n",
      "51049169948.jpg\n",
      "51049894526.jpg\n",
      "50836687727.jpg\n",
      "50832220633.jpg\n",
      "4027106971.jpg\n",
      "4027088707.jpg\n",
      "4027090349.jpg\n",
      "4027848310.jpg\n",
      "4027106385.jpg\n",
      "4027860080.jpg\n",
      "4027098913.jpg\n",
      "4027850386.jpg\n",
      "50357087141.jpg\n",
      "49982166353.jpg\n",
      "49961867256.jpg\n",
      "49859722673.jpg\n",
      "49860362277.jpg\n",
      "49859702776.jpg\n",
      "49859143118.jpg\n",
      "49859652986.jpg\n",
      "49859491381.jpg\n",
      "49859799757.jpg\n",
      "49842750558.jpg\n",
      "49770079122.jpg\n",
      "49770078157.jpg\n",
      "49769217363.jpg\n",
      "49769751131.jpg\n",
      "49782525677.jpg\n",
      "49774706511.jpg\n",
      "49769217278.jpg\n",
      "49766148096.jpg\n",
      "49728849748.jpg\n",
      "49728732721.jpg\n",
      "49729022252.jpg\n",
      "49728697871.jpg\n",
      "49707088808.jpg\n",
      "49698489772.jpg\n",
      "5757834499.jpg\n",
      "3972914088.jpg\n",
      "49597750826.jpg\n",
      "49597166583.jpg\n",
      "49597662926.jpg\n",
      "49596730237.jpg\n",
      "49522180348.jpg\n",
      "49494518116.jpg\n",
      "49493824338.jpg\n",
      "49480387843.jpg\n",
      "49480405796.jpg\n",
      "49479025947.jpg\n",
      "49434252868.jpg\n",
      "47845934322.jpg\n",
      "47814614181.jpg\n",
      "46981817425.jpg\n",
      "40850661633.jpg\n",
      "40848169453.jpg\n",
      "33939906558.jpg\n",
      "4898999262.jpg\n"
     ]
    }
   ],
   "source": [
    "# Open the CSV file with the saved list of images with empty cells as empty strings\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "\n",
    "STORAGE_DIR = '/Users/baskausj/Downloads/miller/'\n",
    "\n",
    "# Read the CSV file\n",
    "images_df = pd.read_csv(filename, na_filter=False, dtype=str)\n",
    "\n",
    "# For testing, just use the first three rows\n",
    "#images_df = images_df.head(2)\n",
    "\n",
    "# Step through each row and download the image using the URL from the original_url column\n",
    "for index, row in images_df.iterrows():\n",
    "    # If the image_name column is not empty, skip this row\n",
    "    if row['image_name'] != '':\n",
    "        continue\n",
    "\n",
    "    # Get the original URL from the original_url column\n",
    "    url = row['original_url']\n",
    "    \n",
    "    # Get the filename from the id column\n",
    "    image_name = row['id'] + '.' + row['original_format']\n",
    "    print(image_name)\n",
    "    \n",
    "    # Download the image\n",
    "    r = requests.get(url)\n",
    "\n",
    "    # Save the image as a bytes file object\n",
    "    with open(STORAGE_DIR + image_name, 'wb') as f:\n",
    "        f.write(r.content)\n",
    "\n",
    "    # Add the image name to a column in the dataframe\n",
    "    images_df.loc[index, 'image_name'] = image_name\n",
    "\n",
    "    # Save the dataframe to the same CSV file after each image is downloaded in case the script crashes\n",
    "    images_df.to_csv(filename, index=False)\n",
    "\n",
    "    # Wait 1 second before downloading the next image\n",
    "    time.sleep(1)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
