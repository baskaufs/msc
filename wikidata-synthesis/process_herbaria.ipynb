{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VanderBot\n",
    "\n",
    "The scripts in this notebook are part of the development of VanderBot, a system to write information about Vanderbilt University researchers and their works to Wikidata.  \n",
    "\n",
    "This code is freely available under a CC0 license. Steve Baskauf 2019-12-16\n",
    "\n",
    "VanderBot 0.8 is under development and subject to continual change. At this point, it's too new to have any stable releases.  \n",
    "\n",
    "For more information, see [this page](https://github.com/HeardLibrary/linked-data/tree/master/publications).  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Code\n",
    "\n",
    "This code block includes import statements, function definitions, and declarations of variables that are common to the rest of the script. It needs to be run once before the other code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "#from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2010' # any date deaths before this date will be assumed to not be a match\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "\n",
    "# ***********************************\n",
    "# NOTE: the script fails if there is a current item in Wikidata that has the same values for both label and description. \n",
    "# A check needs to be run for this !!!\n",
    "# ***********************************\n",
    "\n",
    "# Here is some example JSON from a departmental configuration file (department-configuration.json):\n",
    "\n",
    "'''\n",
    "{\n",
    "  \"deptShortName\": \"anthropology\",\n",
    "  \"aads\": {\n",
    "    \"categories\": [\n",
    "      \"\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/aads/people/\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"African American and Diaspora Studies\",\n",
    "    \"departmentQId\": \"Q79117444\",\n",
    "    \"testAuthorAffiliation\": \"African American Diaspora Studies Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"African American and Diaspora Studies scholar\"\n",
    "    }\n",
    "  },\n",
    "  \"bsci\": {\n",
    "    \"categories\": [\n",
    "      \"primary-training-faculty\",\n",
    "      \"research-and-teaching-faculty\",\n",
    "      \"secondary-faculty\",\n",
    "      \"postdoc-fellows\",\n",
    "      \"emeriti\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/biosci/people/index.php?group=\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"Biological Sciences\",\n",
    "    \"departmentQId\": \"Q78041310\",\n",
    "    \"testAuthorAffiliation\": \"Biological Sciences Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"biology researcher\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "# Note that the first key: value pair sets the department to be processed.\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "\n",
    "# The nTables value is the number of HTML tables in the page to be searched.  Currently (2020-01-19) it isn't used\n",
    "# and the script just checks all of the tables, but it could be implemented if there are tables at the end that don't \n",
    "# include employee names.\n",
    "\n",
    "with open('department-configuration.json', 'rt', encoding='utf-8') as fileObject:\n",
    "    text = fileObject.read()\n",
    "deptSettings = json.loads(text)\n",
    "deptShortName = deptSettings['deptShortName']\n",
    "print('Department currently set for', deptShortName)\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId; P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of value Q IDs of the property propertyId for the item with Wikidata ID qId\n",
    "def searchWikidataSingleProperty(qId, propertyId, valueType):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?object where {\n",
    "        wd:'''+ qId + ''' wdt:''' + propertyId + ''' ?object.\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                if valueType == 'item':\n",
    "                    resultValue = extractQNumber(statement['object']['value'])\n",
    "                else:\n",
    "                    resultValue = statement['object']['value']\n",
    "                resultsList.append(resultValue)\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# if the value passed is '' then the value will be retrieved.  Otherwise, the value is used to screen.\n",
    "def searchStatementAtWikidata(qIds, prop, value, refPropList):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?statement '\n",
    "    # if no value was specified, find the value\n",
    "    if value == '':\n",
    "        query += '?statementValue '\n",
    "    if len(refPropList) != 0:\n",
    "        query += '?reference '\n",
    "    for refPropIndex in range(0, len(refPropList)):\n",
    "        query += '?refVal' + str(refPropIndex) + ' '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:'''+ prop + ''' ?statement.\n",
    "  ?statement ps:'''+ prop\n",
    "    \n",
    "    if value == '':\n",
    "        query += ' ?statementValue.'\n",
    "    else:\n",
    "        query += ' wd:' + value + '.'\n",
    "\n",
    "    if len(refPropList) != 0:\n",
    "        query += '''\n",
    "  optional {\n",
    "    ?statement prov:wasDerivedFrom ?reference.'''\n",
    "        for refPropIndex in range(0, len(refPropList)):\n",
    "            query +='''\n",
    "    ?reference pr:''' + refPropList[refPropIndex] + ''' ?refVal''' + str(refPropIndex) + '''.'''\n",
    "        query +='''\n",
    "        }'''\n",
    "    query +='''\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "    # This will result in several results with the same qNumeber, orcid, and referenceHash\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(result['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        \n",
    "        # NOTE: formerly used this:\n",
    "        #statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        # However, there was at least one case where the appended qNumber had a lower case Q and failed to match.\n",
    "        # So needed a different approach.\n",
    "        pieces = noDomain.split('-')\n",
    "        lastPieces = pieces[1:len(pieces)]\n",
    "        s = \"-\"\n",
    "        statementUuid = s.join(lastPieces)\n",
    "\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            statementValue = result['statementValue']['value']\n",
    "        if len(refPropList) != 0:\n",
    "            if 'reference' in result:\n",
    "                # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                referenceHash = extractFromIri(result['reference']['value'], 4)\n",
    "            else:\n",
    "                referenceHash = ''\n",
    "            referenceValues = []\n",
    "            for refPropIndex in range(0, len(refPropList)):\n",
    "                if 'refVal' + str(refPropIndex) in result:\n",
    "                    refVal = result['refVal' + str(refPropIndex)]['value']\n",
    "                    # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                    #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                    #    referenceValue = referenceValue.split('T')[0]\n",
    "                else:\n",
    "                    refVal = ''\n",
    "                referenceValues.append(refVal)\n",
    "        resultsDict = {'qId': qNumber, 'statementUuid': statementUuid}\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            resultsDict['statementValue'] = statementValue\n",
    "        if len(refPropList) != 0:\n",
    "            resultsDict['referenceHash'] = referenceHash\n",
    "            resultsDict['referenceValues'] = referenceValues\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions and classes\n",
    "\n",
    "Methods of the `Query()` class sends queries to Wikibase instances. It has the following methods:\n",
    "\n",
    "- `.generic_query(query)` Sends a specified query to the endpoint and returns a list of item Q IDs, item labels, or literal values. The variable to be returned must be `?object`.\n",
    "- `.single_property_values_for_item(qid)` Sends a subject Q ID to the endpoint and returns a list of item Q IDs, item labels, or literal values that are values of a specified property.\n",
    "- `.labels_descriptions(qids)` Sends a list of subject Q IDs to the endpoint and returns a list of dictionaries of the form `{'qid': qnumber, 'string': string}` where `string` is either a label, description, or alias.\n",
    "- `.search_statement(qids, reference_property_list)` Sends a list of Q IDs and a list of reference properties to the endpoint and returns information about statements using a specified property. If no value is specified, the information includes the values of the statements. For each statement, the reference UUID, reference property, and reference value is returned. If the statement has more than one reference, there will be multiple results per subject. Results are in the form `{'qId': qnumber, 'statementUuid': statement_uuid, 'statementValue': statement_value, 'referenceHash': reference_hash, 'referenceValue': reference_value}`\n",
    "\n",
    "It has the following attributes:\n",
    "\n",
    "| key | description | default value | applicable method |\n",
    "|----|----|----|\n",
    "| `endpoint` | endpoint URL of Wikabase | `https://query.wikidata.org/sparql` | all |\n",
    "| `mediatype` | Internet media type | `application/json` | all |\n",
    "| `useragent` | User-Agent string to send | `VanderBot/0.8` etc.| all |\n",
    "| `requestheader` | request headers to send |(generated dict) | all |\n",
    "| `sleep` | seconds to delay between queries | 0.25 | all |\n",
    "| `isitem` | `True` if value is item, `False` if value a literal | `True` | `generic_query`, `single_property_values_for_item` |\n",
    "| `uselabel` | `True` for label of item value , `False` for Q ID of item value | `True` | `generic_query`, `single_property_values_for_item` | \n",
    "| `lang` | language of label | `en` | `single_property_values_for_item`, `labels_descriptions`|\n",
    "| `labeltype` | returns `label`, `description`, or `alias` | `label` | `labels_descriptions` |\n",
    "| `labelscreen` | added triple pattern | empty string | `labels_descriptions` |\n",
    "| `pid` | property P ID | `P31` | `single_property_values_for_item`, `search_statement` |\n",
    "| `vid` | value Q ID | empty string | `search_statement` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extract_from_iri(iri, number_pieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[number_pieces]\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.lang = kwargs['lang']\n",
    "        except:\n",
    "            self.lang = 'en' # default to English\n",
    "        try:\n",
    "            self.mediatype = kwargs['mediatype']\n",
    "        except:\n",
    "            self.mediatype = 'application/json' # default to JSON formatted query results\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            self.useragent = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)' \n",
    "        self.requestheader = {\n",
    "        'Accept' : self.mediatype,\n",
    "        'User-Agent': self.useragent\n",
    "        }\n",
    "        try:\n",
    "            self.pid = kwargs['pid'] # property's P ID\n",
    "        except:\n",
    "            self.pid = 'P31' # default to \"instance of\"  \n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.25 # default throtting of 0.25 seconds\n",
    "            \n",
    "        # attributes for single property values method\n",
    "        try:\n",
    "            self.isitem = kwargs['isitem']\n",
    "        except:\n",
    "            self.isitem = True # default to values are items rather than literals   \n",
    "        try:\n",
    "            self.uselabel = kwargs['uselabel']\n",
    "        except:\n",
    "            self.uselabel = True # default is to show labels of items\n",
    "            \n",
    "        # attributes for labels and descriptions method\n",
    "        try:\n",
    "            self.labeltype = kwargs['labeltype']\n",
    "        except:\n",
    "            self.labeltype = 'label' # default to \"label\". Other options: \"description\", \"alias\"\n",
    "        try:\n",
    "            self.labelscreen = kwargs['labelscreen']\n",
    "        except:\n",
    "            self.labelscreen = '' # instead of using a list of subject items, add this line to screen for items\n",
    "            \n",
    "        # attributes for search_statement method\n",
    "        try:\n",
    "            self.vid = kwargs['vid'] # Q ID of the value of a statement. \n",
    "        except:\n",
    "            self.vid = '' # default to no value (the method returns the value of the statement)\n",
    "            \n",
    "    # send a generic query and return a list of Q IDs\n",
    "    def generic_query(self, query):\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['entity']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['entity']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['entity']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "            \n",
    "\n",
    "    # returns the value of a single property for an item by Q ID\n",
    "    def single_property_values_for_item(self, qid):\n",
    "        query = '''\n",
    "select distinct ?object where {\n",
    "    wd:'''+ qid + ''' wdt:''' + self.pid\n",
    "        if self.uselabel and self.isitem:\n",
    "            query += ''' ?objectItem.\n",
    "    ?objectItem rdfs:label ?object.\n",
    "    FILTER(lang(?object) = \"''' + self.lang +'\")'\n",
    "        else:\n",
    "            query += ''' ?object.'''            \n",
    "        query +=  '''\n",
    "    }'''\n",
    "        #print(query)\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['object']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['object']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['object']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "    \n",
    "    # search for any of the \"label\" types: label, alias, description. qids is a list of Q IDs without namespaces\n",
    "    def labels_descriptions(self, qids):\n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "            alternatives = ''\n",
    "            for qid in qids:\n",
    "                alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        if self.labeltype == 'label':\n",
    "            predicate = 'rdfs:label'\n",
    "        elif self.labeltype == 'alias':\n",
    "            predicate = 'skos:altLabel'\n",
    "        elif self.labeltype == 'description':\n",
    "            predicate = 'schema:description'\n",
    "        else:\n",
    "            predicate = 'rdfs:label'        \n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?string'''\n",
    "        \n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            query += '''\n",
    "    where {\n",
    "      VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }'''\n",
    "        # option to screen for Q IDs by triple pattern\n",
    "        if self.labelscreen != '':\n",
    "            query += '''\n",
    "    ''' + self.labelscreen\n",
    "            \n",
    "        query += '''\n",
    "    ?id '''+ predicate + ''' ?string.\n",
    "    filter(lang(?string)=\"''' + self.lang + '''\")\n",
    "    }'''\n",
    "        #print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            string = result['string']['value']\n",
    "            results_list.append({'qid': qnumber, 'string': string})\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "\n",
    "    # Searches for statements using a particular property. If no value is set, the value will be returned.\n",
    "    def search_statement(self, qids, reference_property_list):\n",
    "        # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "        alternatives = ''\n",
    "        for qid in qids:\n",
    "            alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?statement '''\n",
    "        # if no value was specified, find the value\n",
    "        if self.vid == '':\n",
    "            query += '?statementValue '\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '?reference '\n",
    "        for ref_prop_index in range(0, len(reference_property_list)):\n",
    "            query += '?refVal' + str(ref_prop_index) + ' '\n",
    "        query += '''\n",
    "    where {\n",
    "        VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }\n",
    "    ?id p:'''+ self.pid + ''' ?statement.\n",
    "    ?statement ps:'''+ self.pid\n",
    "\n",
    "        if self.vid == '': # return the value of the statement if no particular value is specified\n",
    "            query += ' ?statementValue.'\n",
    "        else:\n",
    "            query += ' wd:' + self.vid + '.' # specify the value to be searched for\n",
    "\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '''\n",
    "    optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.''' # search for references if there are any\n",
    "            for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                query +='''\n",
    "        ?reference pr:''' + reference_property_list[ref_prop_index] + ' ?refVal' + str(ref_prop_index) + '.'\n",
    "            query +='''\n",
    "            }'''\n",
    "        query +='''\n",
    "      }'''\n",
    "        print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "        # This will result in several results with the same subject qNumber\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "            no_domain = extract_from_iri(result['statement']['value'], 5)\n",
    "            # need to remove the qNumber that's appended in front of the UUID\n",
    "            pieces = no_domain.split('-')\n",
    "            last_pieces = pieces[1:len(pieces)]\n",
    "            s = \"-\"\n",
    "            statement_uuid = s.join(last_pieces)\n",
    "\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if value == '':\n",
    "                statement_value = result['statementValue']['value']\n",
    "            # extract the reference property data if any reference properties were specified\n",
    "            if len(reference_property_list) != 0:\n",
    "                if 'reference' in result:\n",
    "                    # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                    reference_hash = extract_qnumber(result['reference']['value'])\n",
    "                else:\n",
    "                    reference_hash = ''\n",
    "                reference_values = []\n",
    "                for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                    if 'refVal' + str(ref_prop_index) in result:\n",
    "                        reference_value = result['refVal' + str(ref_prop_index)]['value']\n",
    "                        # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                        #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                        #    referenceValue = referenceValue.split('T')[0]\n",
    "                    else:\n",
    "                        reference_value = ''\n",
    "                    reference_values.append(reference_value)\n",
    "            results_dict = {'qId': qnumber, 'statementUuid': statement_uuid}\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                results_dict['statementValue'] = statement_value\n",
    "            if len(reference_property_list) != 0:\n",
    "                results_dict['referenceHash'] = reference_hash\n",
    "                results_dict['referenceValues'] = reference_values\n",
    "            results_list.append(results_dict)\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "select distinct ?id ?statement ?statementValue ?reference ?refVal0 ?refVal1 \n",
      "    where {\n",
      "        VALUES ?id\n",
      "    {\n",
      "wd:Q40670042\n",
      "wd:Q57082956\n",
      "wd:Q75060085\n",
      "\n",
      "    }\n",
      "    ?id p:P108 ?statement.\n",
      "    ?statement ps:P108 ?statementValue.\n",
      "    optional {\n",
      "        ?statement prov:wasDerivedFrom ?reference.\n",
      "        ?reference pr:P854 ?refVal0.\n",
      "        ?reference pr:P813 ?refVal1.\n",
      "            }\n",
      "      }\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'statement_value' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-110-bacec0c03e43>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#print(get_aliases.labels_descriptions(qids))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mget_employer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'P108'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_employer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch_statement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreference_properties\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-109-b89c7bcae515>\u001b[0m in \u001b[0;36msearch_statement\u001b[0;34m(self, qids, reference_property_list)\u001b[0m\n\u001b[1;32m    286\u001b[0m             \u001b[0;31m# if no value was specified, get the value that was found in the search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvid\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 288\u001b[0;31m                 \u001b[0mresults_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'statementValue'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstatement_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreference_property_list\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0mresults_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'referenceHash'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreference_hash\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'statement_value' referenced before assignment"
     ]
    }
   ],
   "source": [
    "person = 'Q40670042'\n",
    "orcid = '0000-0003-4365-3135'\n",
    "qids = ['Q40670042', 'Q57082956', 'Q75060085']\n",
    "reference_properties = ['P854', 'P813']\n",
    "\n",
    "#get_orcid = Query(pid='P496', isitem=False)\n",
    "#print(get_orcid.single_property_values_for_item(person) )\n",
    "#print()\n",
    "\n",
    "#get_class = Query(pid='P31', uselabel=False)\n",
    "#print(get_class.single_property_values_for_item(person) )\n",
    "#print()\n",
    "\n",
    "#get_employer_label = Query(pid='P108')\n",
    "#print(get_employer_label.single_property_values_for_item(person) )\n",
    "#print()\n",
    "\n",
    "#get_labels = Query(labeltype='label')\n",
    "#print(get_labels.labels_descriptions(qids))\n",
    "#print()\n",
    "#get_descriptions = Query(labeltype='description')\n",
    "#print(get_descriptions.labels_descriptions(qids))\n",
    "#print()\n",
    "#get_aliases = Query(labeltype='alias')\n",
    "#print(get_aliases.labels_descriptions(qids))\n",
    "get_employer = Query(pid='P108')\n",
    "print(get_employer.search_statement(qids, reference_properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine CETAF classes\n",
    "\n",
    "Determine what classes are being used by CETAF institutions, then query to generate a list of items that are members of those classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionError",
     "evalue": "HTTPSConnectionPool(host='query.wikidata.org', port=443): Max retries exceeded with url: /sparql?query=%0Aselect+distinct+%3Fentity+where+%7B%0A++++%3Finstitution+wdt%3AP463+wd%3AQ5163385.%0A++++%3Finstitution+wdt%3AP31+%3Fentity.%0A%7D (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x106f2c1d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mgaierror\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    156\u001b[0m             conn = connection.create_connection(\n\u001b[0;32m--> 157\u001b[0;31m                 \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dns_host\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mextra_kw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m             )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/util/connection.py\u001b[0m in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOCK_STREAM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/socket.py\u001b[0m in \u001b[0;36mgetaddrinfo\u001b[0;34m(host, port, family, type, proto, flags)\u001b[0m\n\u001b[1;32m    747\u001b[0m     \u001b[0maddrlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 748\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_socket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetaddrinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    749\u001b[0m         \u001b[0maf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msocktype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanonname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msa\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mgaierror\u001b[0m: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNewConnectionError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    671\u001b[0m                 \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                 \u001b[0mchunked\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    673\u001b[0m             )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_make_request\u001b[0;34m(self, conn, method, url, timeout, chunked, **httplib_request_kw)\u001b[0m\n\u001b[1;32m    375\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mSocketTimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseSSLError\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36m_validate_conn\u001b[0;34m(self, conn)\u001b[0m\n\u001b[1;32m    993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sock\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# AppEngine might not have  `.sock`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 994\u001b[0;31m             \u001b[0mconn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    995\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# Add certificate verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 334\u001b[0;31m         \u001b[0mconn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_new_conn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    335\u001b[0m         \u001b[0mhostname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connection.py\u001b[0m in \u001b[0;36m_new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m             raise NewConnectionError(\n\u001b[0;32m--> 169\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Failed to establish a new connection: %s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m             )\n",
      "\u001b[0;31mNewConnectionError\u001b[0m: <urllib3.connection.VerifiedHTTPSConnection object at 0x106f2c1d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    448\u001b[0m                     \u001b[0mretries\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_retries\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m                 )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/connectionpool.py\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, **response_kw)\u001b[0m\n\u001b[1;32m    719\u001b[0m             retries = retries.increment(\n\u001b[0;32m--> 720\u001b[0;31m                 \u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_pool\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacktrace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m             )\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/urllib3/util/retry.py\u001b[0m in \u001b[0;36mincrement\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    435\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_retry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_exhausted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 436\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mMaxRetryError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mResponseError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcause\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    437\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPSConnectionPool(host='query.wikidata.org', port=443): Max retries exceeded with url: /sparql?query=%0Aselect+distinct+%3Fentity+where+%7B%0A++++%3Finstitution+wdt%3AP463+wd%3AQ5163385.%0A++++%3Finstitution+wdt%3AP31+%3Fentity.%0A%7D (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x106f2c1d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-107-f235fd6cae1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m }'''\n\u001b[1;32m      7\u001b[0m \u001b[0mgen_query_qids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQuery\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muselabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcetaf_classlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen_query_qids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcetaf_classlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-b89c7bcae515>\u001b[0m in \u001b[0;36mgeneric_query\u001b[0;34m(self, query)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# send a generic query and return a list of Q IDs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgeneric_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'query'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequestheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mresults_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(url, params, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'allow_redirects'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/api.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(method, url, **kwargs)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;31m# cases, and look like a memory leak in others.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msessions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[0m\n\u001b[1;32m    531\u001b[0m         }\n\u001b[1;32m    532\u001b[0m         \u001b[0msend_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 533\u001b[0;31m         \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msend_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    534\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    535\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/sessions.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, **kwargs)\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m         \u001b[0;31m# Send the request\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m         \u001b[0;31m# Total elapsed time of the request (approximately)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/adapters.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, timeout, verify, cert, proxies)\u001b[0m\n\u001b[1;32m    514\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mSSLError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mClosedPoolError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mConnectionError\u001b[0m: HTTPSConnectionPool(host='query.wikidata.org', port=443): Max retries exceeded with url: /sparql?query=%0Aselect+distinct+%3Fentity+where+%7B%0A++++%3Finstitution+wdt%3AP463+wd%3AQ5163385.%0A++++%3Finstitution+wdt%3AP31+%3Fentity.%0A%7D (Caused by NewConnectionError('<urllib3.connection.VerifiedHTTPSConnection object at 0x106f2c1d0>: Failed to establish a new connection: [Errno 8] nodename nor servname provided, or not known'))"
     ]
    }
   ],
   "source": [
    "# Find out all of the classes of which CETAF institutions are instances\n",
    "query = '''\n",
    "select distinct ?entity where {\n",
    "    ?institution wdt:P463 wd:Q5163385.\n",
    "    ?institution wdt:P31 ?entity.\n",
    "}'''\n",
    "gen_query_qids = Query(uselabel=False)\n",
    "cetaf_classlist = gen_query_qids.generic_query(query)\n",
    "print(cetaf_classlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.wikidata.org/entity/Q167346\n",
      "\n"
     ]
    },
    {
     "ename": "JSONDecodeError",
     "evalue": "Expecting value: line 1 column 1 (char 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-108-77c80e0a196a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;31m# Get the labels for all of the hits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabels_descriptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m     \u001b[0mtest_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mwikidata_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'labels'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'done'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-106-b89c7bcae515>\u001b[0m in \u001b[0;36mlabels_descriptions\u001b[0;34m(self, qids)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0mresults_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'query'\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequestheader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bindings'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/requests/models.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;31m# used.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m                     \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 897\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcomplexjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    898\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/simplejson/__init__.py\u001b[0m in \u001b[0;36mloads\u001b[0;34m(s, encoding, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, use_decimal, **kw)\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0mparse_constant\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mobject_pairs_hook\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             and not use_decimal and not kw):\n\u001b[0;32m--> 525\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_default_decoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mJSONDecoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mdecode\u001b[0;34m(self, s, _w, _PY3)\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_PY3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m         \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mend\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/simplejson/decoder.py\u001b[0m in \u001b[0;36mraw_decode\u001b[0;34m(self, s, idx, _w, _PY3)\u001b[0m\n\u001b[1;32m    398\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mord0\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0xef\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0midx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'\\xef\\xbb\\xbf'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m                 \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscan_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_w\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mJSONDecodeError\u001b[0m: Expecting value: line 1 column 1 (char 0)"
     ]
    }
   ],
   "source": [
    "# send a query for each class to retrieve items that are members\n",
    "test_results = []\n",
    "for wikidata_class in cetaf_classlist:\n",
    "    print(wikidata_class)\n",
    "    print()\n",
    "    \n",
    "    graph_pattern = '''\n",
    "    ?id wdt:P31 wd:''' + wikidata_class + '.'\n",
    "    get_labels = Query(labeltype='label', labelscreen=graph_pattern)\n",
    "    \n",
    "    # Get the labels for all of the hits\n",
    "    test_labels = get_labels.labels_descriptions([])\n",
    "    test_results.append({'class': wikidata_class, 'labels': test_labels})\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Q94923', 'Q153256', 'Q153541', 'Q153748', 'Q153829', 'Q163255', 'Q174823', 'Q188617', 'Q195786', 'Q200711', 'Q202120', 'Q220064', 'Q225403', 'Q256160', 'Q262120', 'Q280791', 'Q289277', 'Q296170', 'Q317714', 'Q318081', 'Q319334', 'Q320082', 'Q321565', 'Q321575', 'Q322634', 'Q323124', 'Q323954', 'Q323980', 'Q328081', 'Q328729', 'Q328848', 'Q373040', 'Q378480', 'Q384815', 'Q385057', 'Q428258', 'Q429533', 'Q431309', 'Q437790', 'Q437788', 'Q437794', 'Q437802', 'Q446505', 'Q458588', 'Q474085', 'Q475580', 'Q478245', 'Q478839', 'Q486315', 'Q503498', 'Q509884', 'Q510585', 'Q517377', 'Q527767', 'Q528505', 'Q536819', 'Q538819', 'Q543379', 'Q558491', 'Q559261', 'Q570148', 'Q574117', 'Q574773', 'Q577644', 'Q582317', 'Q589884', 'Q596642', 'Q596723', 'Q598892', 'Q600792', 'Q600863', 'Q604550', 'Q604699', 'Q606827', 'Q607584', 'Q609012', 'Q609992', 'Q613250', 'Q615695', 'Q618248', 'Q618523', 'Q621956', 'Q621967', 'Q623333', 'Q628828', 'Q629949', 'Q630118', 'Q630395', 'Q630410', 'Q630433', 'Q630459', 'Q632716', 'Q4998', 'Q8668', 'Q24185', 'Q24433', 'Q45242', 'Q54489', 'Q56046', 'Q63203', 'Q1907213', 'Q1923740', 'Q1937407', 'Q1946237', 'Q1957821', 'Q1970380', 'Q1970601', 'Q1971774', 'Q1971949', 'Q1980263', 'Q1990013', 'Q1995650', 'Q1996071', 'Q2011703', 'Q2028223', 'Q2035053', 'Q2066407', 'Q2066962', 'Q2095464', 'Q2101460', 'Q2115433', 'Q2115661', 'Q2116241', 'Q2148221', 'Q2148228', 'Q2155115', 'Q2169130', 'Q2171140', 'Q2178787', 'Q2197914', 'Q2249320', 'Q2278413', 'Q2295836', 'Q2307982', 'Q2313437', 'Q2329546', 'Q2330591', 'Q2364115', 'Q2366770', 'Q2369971', 'Q2371438', 'Q2371650', 'Q2376002', 'Q2400780', 'Q2407553', 'Q2426340', 'Q2435716', 'Q2439271', 'Q2444945', 'Q2447822', 'Q2455793', 'Q2455990', 'Q2461130', 'Q2466506', 'Q2466783', 'Q2468128', 'Q2483537', 'Q2485977', 'Q2494098', 'Q2498387', 'Q2511206', 'Q2533994', 'Q2539422', 'Q2563728', 'Q2566136', 'Q2572243', 'Q2585168', 'Q2605209', 'Q2606130', 'Q2610973', 'Q2616933', 'Q2619332', 'Q2634943', 'Q2645392', 'Q2656932', 'Q2684662', 'Q2684773', 'Q2689422', 'Q2689996', 'Q2692902', 'Q2721720', 'Q2738912', 'Q2739057', 'Q2739331', 'Q2743795', 'Q2744187', 'Q2745210', 'Q2747212', 'Q2788697', 'Q2794125', 'Q2834322', 'Q2835200', 'Q2836812', 'Q2836840', 'Q2840327', 'Q2842754', 'Q2843476', 'Q2843537', 'Q2859691', 'Q2859688', 'Q894670', 'Q895589', 'Q895891', 'Q910282', 'Q917672', 'Q917681', 'Q923683', 'Q927658', 'Q930638', 'Q931430', 'Q931457', 'Q943407', 'Q943874', 'Q944730', 'Q946082', 'Q946301', 'Q949061', 'Q956772', 'Q963825', 'Q964899', 'Q967219', 'Q967401', 'Q971049', 'Q971355', 'Q973233', 'Q974244', 'Q974285', 'Q976405', 'Q977761', 'Q979670', 'Q991258', 'Q996541', 'Q998418', 'Q1006690', 'Q1010308', 'Q1026407', 'Q1054097', 'Q1055404', 'Q1064892', 'Q1069148', 'Q1071076', 'Q1072277', 'Q1082578', 'Q1099139', 'Q1119013', 'Q1120504', 'Q1123665', 'Q1129875', 'Q1129930', 'Q1145112', 'Q1151078', 'Q1152424', 'Q1162245', 'Q1186086', 'Q1189287', 'Q1264942', 'Q1296160', 'Q1303386', 'Q1325788', 'Q1360424', 'Q1368384', 'Q1378297', 'Q1384208', 'Q1393011', 'Q1415481', 'Q1425474', 'Q1439933', 'Q1442657', 'Q1472957', 'Q1515798', 'Q1539251', 'Q1576596', 'Q1576733', 'Q1588999', 'Q1635335', 'Q1644473', 'Q1666801', 'Q1671713', 'Q1671734', 'Q1683445', 'Q1683458', 'Q1683460', 'Q1690700', 'Q1765144', 'Q1770187', 'Q1771324', 'Q1779489', 'Q1794620', 'Q1794858', 'Q1795487', 'Q1799639', 'Q1807521', 'Q1817736', 'Q1848855', 'Q1852803', 'Q1867975', 'Q1872824', 'Q1875558', 'Q1890086', 'Q1890779', 'Q2859694', 'Q2859692', 'Q2859693', 'Q2859697', 'Q2859703', 'Q2859700', 'Q2859701', 'Q2859706', 'Q2859707', 'Q2859705', 'Q2859711', 'Q2859708', 'Q2859709', 'Q2859714', 'Q2859715', 'Q2859712', 'Q2859713', 'Q2859718', 'Q2859719', 'Q2859716', 'Q2859717', 'Q2859723', 'Q2876503', 'Q2878518', 'Q2879127', 'Q2879615', 'Q2881810', 'Q2882456', 'Q2883914', 'Q2886410', 'Q2886576', 'Q2887063', 'Q2889004', 'Q2892452', 'Q2893067', 'Q2893656', 'Q2894419', 'Q2896269', 'Q2915575', 'Q2916832', 'Q2917011', 'Q2917341', 'Q2919730', 'Q2919885', 'Q2943261', 'Q2944918', 'Q2969934', 'Q2994486', 'Q2994484', 'Q2994490', 'Q2994530', 'Q2994542', 'Q3003810', 'Q3026688', 'Q3028259', 'Q3034611', 'Q3034795', 'Q3046409', 'Q3048736', 'Q3052500', 'Q3064716', 'Q3065485', 'Q3068815', 'Q3075676', 'Q3076985', 'Q3079714', 'Q3089144', 'Q3091616', 'Q3098339', 'Q3100546', 'Q3106834', 'Q3115229', 'Q3116396', 'Q3119056', 'Q3126659', 'Q3127505', 'Q3139233', 'Q3162283', 'Q3162322', 'Q3162356', 'Q3162373', 'Q3162378', 'Q3162376', 'Q3162383', 'Q3162380', 'Q3162381', 'Q3162386', 'Q3162387', 'Q3162384', 'Q3162390', 'Q3162391', 'Q3162394', 'Q3162393', 'Q3162398', 'Q3162399', 'Q3162397', 'Q3162402', 'Q3162403', 'Q3162401', 'Q3162404', 'Q3162406', 'Q3162407', 'Q3162405', 'Q3162410', 'Q3162411', 'Q3162408', 'Q3162414', 'Q3162415', 'Q3162418', 'Q3162419', 'Q3162422', 'Q3162423', 'Q3162421', 'Q3162426', 'Q3162427', 'Q3162424', 'Q3162430', 'Q3162431', 'Q3162440', 'Q3162454', 'Q3162479', 'Q3162482', 'Q3162547', 'Q3162563', 'Q3162566', 'Q3162564', 'Q3162565', 'Q3162571', 'Q3162568', 'Q3162572', 'Q3162606', 'Q3162611', 'Q3162650', 'Q3162653', 'Q3162657', 'Q3162666', 'Q3162686', 'Q3198468', 'Q3200597', 'Q3206389', 'Q3231880', 'Q3232192', 'Q3241308', 'Q3276615', 'Q3295344', 'Q3327712', 'Q3334165', 'Q3346386', 'Q3348108', 'Q3363773', 'Q3363814', 'Q3363812', 'Q3363873', 'Q3363886', 'Q3363889', 'Q3363918', 'Q3364003', 'Q3364008', 'Q3364123', 'Q3364222', 'Q3364279', 'Q3364335', 'Q3364353', 'Q3364653', 'Q3364803', 'Q3364808', 'Q3369639', 'Q3398986', 'Q3431727', 'Q3442569', 'Q3442578', 'Q3443536', 'Q3456566', 'Q3456644', 'Q3456702', 'Q3456814', 'Q3456929', 'Q3457218', 'Q3457245', 'Q3457250', 'Q3484004', 'Q3486457', 'Q3518517', 'Q3533091', 'Q3614087', 'Q3621362', 'Q3694552', 'Q3695266', 'Q3755226', 'Q3763858', 'Q3763875', 'Q3763878', 'Q3763876', 'Q3763882', 'Q3763883', 'Q3763880', 'Q3763886', 'Q3763887', 'Q3763889', 'Q3763892', 'Q632833', 'Q633365', 'Q636275', 'Q654341', 'Q663149', 'Q673757', 'Q673954', 'Q677516', 'Q677602', 'Q679067', 'Q686324', 'Q697970', 'Q707660', 'Q730948', 'Q739334', 'Q765568', 'Q767907', 'Q768376', 'Q769851', 'Q770229', 'Q776863', 'Q796229', 'Q810774', 'Q819663', 'Q819811', 'Q824180', 'Q824193', 'Q827373', 'Q827398', 'Q827406', 'Q827435', 'Q827442', 'Q829338', 'Q830674', 'Q831148', 'Q833862', 'Q834198', 'Q836254', 'Q838392', 'Q841037', 'Q841650', 'Q843141', 'Q848416', 'Q848422', 'Q848820', 'Q870156', 'Q870486', 'Q870492', 'Q871292', 'Q873682', 'Q873964', 'Q873982', 'Q874042', 'Q875233', 'Q875332', 'Q875673', 'Q877692', 'Q877915', 'Q877942', 'Q878970', 'Q879081', 'Q880814', 'Q882377', 'Q883976', 'Q890543', 'Q890625', 'Q891066', 'Q891197', 'Q892711', 'Q894579', 'Q894629', 'Q894634', 'Q894635', 'Q894633', 'Q894638', 'Q894639', 'Q894636', 'Q894637', 'Q894642', 'Q894643', 'Q894646', 'Q894647', 'Q894645', 'Q894650', 'Q894651', 'Q894648', 'Q894649', 'Q894654', 'Q894652', 'Q894653', 'Q894658', 'Q894659', 'Q894656', 'Q894657', 'Q894663', 'Q894660', 'Q894661', 'Q894664', 'Q894665', 'Q894668', 'Q3763910', 'Q3763908', 'Q3763918', 'Q3763924', 'Q3780715', 'Q3786842', 'Q3815348', 'Q3816996', 'Q3822703', 'Q3886267', 'Q3886270', 'Q3886271', 'Q3886269', 'Q3886274', 'Q3886275', 'Q3886273', 'Q3886279', 'Q3886280', 'Q3886281', 'Q3886286', 'Q3886284', 'Q3886285', 'Q3890919', 'Q3892794', 'Q3895518', 'Q3905190', 'Q3920848', 'Q3936706', 'Q3983829', 'Q4012424', 'Q4012680', 'Q4055198', 'Q4062924', 'Q4068273', 'Q4095105', 'Q4095110', 'Q4095111', 'Q4095108', 'Q4095109', 'Q4095114', 'Q4095115', 'Q4095113', 'Q4095118', 'Q4095119', 'Q4095116', 'Q4095117', 'Q4095120', 'Q4095121', 'Q4118460', 'Q4120758', 'Q4131008', 'Q4144549', 'Q4158699', 'Q4166218', 'Q4175880', 'Q4240824', 'Q4257680', 'Q4259805', 'Q4275602', 'Q4333775', 'Q4348479', 'Q4353880', 'Q4385625', 'Q4388023', 'Q4404906', 'Q4405768', 'Q4469636', 'Q4499803', 'Q4504376', 'Q4508039', 'Q4533987', 'Q4540471', 'Q4543284', 'Q4575039', 'Q4664282', 'Q4680461', 'Q4681683', 'Q4683242', 'Q4693837', 'Q4698736', 'Q4700365', 'Q4712812', 'Q4713999', 'Q4725706', 'Q4726558', 'Q4731647', 'Q4736012', 'Q4736267', 'Q4739454', 'Q4743435', 'Q4754109', 'Q4759058', 'Q4778784', 'Q4781336', 'Q4783906', 'Q4784764', 'Q4784786', 'Q4784784', 'Q4784785', 'Q4784790', 'Q4784794', 'Q4784792', 'Q4784793', 'Q4784799', 'Q4784796', 'Q4784797', 'Q4784802', 'Q4784800', 'Q4784801', 'Q4784806', 'Q4784807', 'Q4784804', 'Q4784805', 'Q4784810', 'Q4784811', 'Q4784808', 'Q4784814', 'Q4784815', 'Q4784812', 'Q4784813', 'Q4784818', 'Q4784819', 'Q4784816', 'Q4784817', 'Q4784822', 'Q4784823', 'Q4784820', 'Q4784821', 'Q4784826', 'Q4784827', 'Q4784824', 'Q4784825', 'Q4784830', 'Q4784831', 'Q4784828', 'Q4784829', 'Q4784834', 'Q4784835', 'Q4784832', 'Q4784833', 'Q4784838', 'Q4784839', 'Q4784836', 'Q4784842', 'Q4784843', 'Q4784840', 'Q4784841', 'Q4784846', 'Q4784847', 'Q4784844', 'Q4784850', 'Q4784851', 'Q4784848', 'Q4784849', 'Q4784854', 'Q4784855', 'Q4784852', 'Q4784853', 'Q4784858', 'Q4784859', 'Q4784856', 'Q4784857', 'Q4784862', 'Q4784863', 'Q4784860', 'Q4784861', 'Q4784866', 'Q4784864', 'Q4784865', 'Q4784870', 'Q4784871', 'Q4784868', 'Q4784869', 'Q4784875', 'Q4784872', 'Q4784873', 'Q4784878', 'Q4784877', 'Q4784882', 'Q4784880', 'Q4784881', 'Q4784886', 'Q4784884', 'Q4784885', 'Q4784890', 'Q4784891', 'Q4784888', 'Q4784889', 'Q4784894', 'Q4784895', 'Q4784892', 'Q4784897', 'Q4784900', 'Q4788932', 'Q4790352', 'Q4791118', 'Q4791256', 'Q4791262', 'Q4791739', 'Q4792056', 'Q4796651', 'Q4812638', 'Q4818944', 'Q4819256', 'Q4819495', 'Q4820892', 'Q4823826', 'Q4824296', 'Q4829988', 'Q4848332', 'Q4850208', 'Q4850425', 'Q4851467', 'Q4853406', 'Q4863296', 'Q4865386', 'Q4865387', 'Q4867821', 'Q4869639', 'Q4877545', 'Q4883646', 'Q4883720', 'Q4883922', 'Q4884044', 'Q4886835', 'Q4889783', 'Q4892246', 'Q4892608', 'Q4894262', 'Q4895276', 'Q4895577', 'Q4898817', 'Q4903645', 'Q4904330', 'Q4916622', 'Q4924392', 'Q4926930', 'Q4927320', 'Q4929903', 'Q4937253', 'Q4942509', 'Q4943609', 'Q4944451', 'Q4948416', 'Q4948422', 'Q4948424', 'Q4948429', 'Q4948438', 'Q4948437', 'Q4948440', 'Q4948441', 'Q4948445', 'Q4948448', 'Q4948449', 'Q4948466', 'Q4948467', 'Q4948464', 'Q4948465', 'Q4948470', 'Q4948471', 'Q4948468', 'Q4948469', 'Q4948474', 'Q4948475', 'Q4948472', 'Q4948473', 'Q4948478', 'Q4948477', 'Q4948482', 'Q4948481', 'Q4948484', 'Q4948485', 'Q4948490', 'Q4948488', 'Q4948492', 'Q4951275', 'Q4957297', 'Q4961701', 'Q4967317', 'Q4975032', 'Q4980293', 'Q4980894', 'Q4985906', 'Q4998210', 'Q5000425', 'Q5001352', 'Q5003274', 'Q5016165', 'Q5016195', 'Q5021110', 'Q5021796', 'Q5028700', 'Q5032531', 'Q5040751', 'Q5041406', 'Q5048300', 'Q5055941', 'Q5056934', 'Q5057002', 'Q5059121', 'Q5059135', 'Q5059184', 'Q5059198', 'Q5072666', 'Q5073277', 'Q5087704', 'Q5092123', 'Q5092126', 'Q5092220', 'Q5092826', 'Q5093642', 'Q5094572', 'Q5097312', 'Q5113693', 'Q5124375', 'Q5124376', 'Q5127155', 'Q5127686', 'Q5129634', 'Q5132073', 'Q5138303', 'Q5141058', 'Q5141722', 'Q5141866', 'Q5148353', 'Q5153206', 'Q5155139', 'Q5158067', 'Q5158903', 'Q5159485', 'Q5161496', 'Q5162921', 'Q5162929', 'Q5163119', 'Q5163120', 'Q5163125', 'Q5163145', 'Q5169399', 'Q5181789', 'Q5188024', 'Q5189729', 'Q5189930', 'Q5191378', 'Q5196873', 'Q5200329', 'Q5203463', 'Q5203477', 'Q5211266', 'Q5214910', 'Q5216608', 'Q5216623', 'Q5218854', 'Q5241640', 'Q5250897', 'Q5250991', 'Q5253243', 'Q5256610', 'Q5263507', 'Q5263676', 'Q5263937', 'Q5267815', 'Q5267813', 'Q5286496', 'Q5291030', 'Q5293251', 'Q5302612', 'Q5304337', 'Q5311168', 'Q5312784', 'Q5314964', 'Q5315426', 'Q5315692', 'Q5316059', 'Q5316277', 'Q5316580', 'Q5318478', 'Q5325962', 'Q5329510', 'Q5333888', 'Q5338443', 'Q5338579', 'Q5359319', 'Q5361885', 'Q5364342', 'Q5366186', 'Q5375398', 'Q5388359', 'Q5391541', 'Q5393922', 'Q5395347', 'Q5395528', 'Q5397629', 'Q5397998', 'Q5398689', 'Q5398695', 'Q5403157', 'Q5404697', 'Q5413888', 'Q5421146', 'Q5430801', 'Q5434096', 'Q5438314', 'Q5438922', 'Q5442495', 'Q5445158', 'Q5452183', 'Q5458809', 'Q5464017', 'Q5469066', 'Q5469438', 'Q5470627', 'Q5470640', 'Q5472380', 'Q5473778', 'Q5474987', 'Q5477143', 'Q5477354', 'Q5477670', 'Q5480434', 'Q5481608', 'Q5486082', 'Q5489812', 'Q5491740', 'Q5495358', 'Q5499240', 'Q5500098', 'Q5501450', 'Q5506031', 'Q5506389', 'Q5507342', 'Q5507695', 'Q5508289', 'Q5509590', 'Q5510444', 'Q5522300', 'Q5522493', 'Q5523093', 'Q5523798', 'Q5524513', 'Q5529837', 'Q5530790', 'Q5537331', 'Q5541469', 'Q5541520', 'Q5546931', 'Q5547658', 'Q5547722', 'Q5551958', 'Q5554951', 'Q5555522', 'Q5558578', 'Q5558577', 'Q5558582', 'Q5558585', 'Q5558590', 'Q5558588', 'Q5558594', 'Q5558592', 'Q5558597', 'Q5558603', 'Q5558600', 'Q5558607', 'Q5558605', 'Q5558608', 'Q5558614', 'Q5558615', 'Q5558612', 'Q5558613', 'Q5558616', 'Q5558620', 'Q5558621', 'Q5558624', 'Q5558625', 'Q5558630', 'Q5558631', 'Q5558628', 'Q5558633', 'Q5559047', 'Q5560477', 'Q5562476', 'Q5564873', 'Q5568202', 'Q5576152', 'Q5578654', 'Q5583496', 'Q5589082', 'Q5589544', 'Q5602279', 'Q5603225', 'Q5604440', 'Q5607276', 'Q5617335', 'Q5620069', 'Q5638108', 'Q5638880', 'Q5639412', 'Q5640444', 'Q5640598', 'Q5644261', 'Q5644961', 'Q5647261', 'Q5652238', 'Q5655000', 'Q5658220', 'Q5660965', 'Q5662054', 'Q5664914', 'Q5671434', 'Q5674349', 'Q5675001', 'Q5676476', 'Q5676841', 'Q5677077', 'Q5678574', 'Q5679170', 'Q5681074', 'Q5682337', 'Q5683743', 'Q5685095', 'Q5686571', 'Q5687806', 'Q5690719', 'Q5693628', 'Q5695205', 'Q5696656', 'Q5699399', 'Q5702471', 'Q5702499', 'Q5728107', 'Q5732438', 'Q5733324', 'Q5736258', 'Q5738956', 'Q5741796', 'Q5743575', 'Q5744224', 'Q5751878', 'Q5758940', 'Q5759514', 'Q5760668', 'Q5761958', 'Q5762178', 'Q5764331', 'Q5765252', 'Q5788556', 'Q5792262', 'Q5796845', 'Q5799270', 'Q5801444', 'Q5805605', 'Q5806554', 'Q5810092', 'Q5815109', 'Q5815281', 'Q5817623', 'Q5837199', 'Q5842566', 'Q5843471', 'Q5848778', 'Q5849100', 'Q5873822', 'Q5876906', 'Q5880077', 'Q5883298', 'Q5904241', 'Q5913996', 'Q5920617', 'Q5921142', 'Q5925182', 'Q5926366', 'Q5926371', 'Q5926368', 'Q5926378', 'Q5926386', 'Q5926384', 'Q5926390', 'Q5926388', 'Q5926394', 'Q5926397', 'Q5926402', 'Q5926400', 'Q5926407', 'Q5926405', 'Q5926410', 'Q5926417', 'Q5926423', 'Q5926425', 'Q5926431', 'Q5926428', 'Q5926434', 'Q5926438', 'Q5926442', 'Q5926440', 'Q5926445', 'Q5926451', 'Q5926448', 'Q5926454', 'Q5926458', 'Q5926456', 'Q5926460', 'Q5926463', 'Q5926466', 'Q5926469', 'Q5926475', 'Q5926479', 'Q5926477', 'Q5926482', 'Q5926485', 'Q5926489', 'Q5926495', 'Q5926498', 'Q5926505', 'Q5926511', 'Q5926508', 'Q5926514', 'Q5926517', 'Q5926521', 'Q5926530', 'Q5926528', 'Q5926534', 'Q5926532', 'Q5926538', 'Q5926542', 'Q5926540', 'Q5926545', 'Q5926549', 'Q5926555', 'Q5926552', 'Q5926558', 'Q5926562', 'Q5926560', 'Q5926566', 'Q5926574', 'Q5926572', 'Q5926577', 'Q5926583', 'Q5926580', 'Q5926588', 'Q5926594', 'Q5926600', 'Q5926607', 'Q5926610', 'Q5926614', 'Q5926612', 'Q5926619', 'Q5926617', 'Q5926623', 'Q5926630', 'Q5926636', 'Q5926640', 'Q5926644', 'Q5926649', 'Q5926654', 'Q5926662', 'Q5926660', 'Q5926665', 'Q5926669', 'Q5926672', 'Q5926678', 'Q5926676', 'Q5926682', 'Q5926687', 'Q5926685', 'Q5926720', 'Q5926739', 'Q5926737', 'Q5926743', 'Q5926746', 'Q5926748', 'Q5926755', 'Q5926752', 'Q5926758', 'Q5926763', 'Q5926767', 'Q5926770', 'Q5926775', 'Q5926773', 'Q5926779', 'Q5926777', 'Q5926782', 'Q5926787', 'Q5926785', 'Q5926790', 'Q5926788', 'Q5926793', 'Q5926799', 'Q5926802', 'Q5926807', 'Q5926805', 'Q5926811', 'Q5926814', 'Q5926817', 'Q5926823', 'Q5926820', 'Q5926829', 'Q5926834', 'Q5926832', 'Q5926838', 'Q5926841', 'Q5926844', 'Q5926850', 'Q5926853', 'Q5926857', 'Q5926862', 'Q5926866', 'Q5926869', 'Q5926875', 'Q5926872', 'Q5926879', 'Q5926882', 'Q5926887', 'Q5926909', 'Q5926914', 'Q5926916', 'Q5926921', 'Q5926931', 'Q5926928', 'Q5926948', 'Q5926984', 'Q5926994', 'Q5927000', 'Q5927023', 'Q5928661', 'Q5935387', 'Q5935607', 'Q5937864', 'Q5945847', 'Q5946290', 'Q5954531', 'Q5954731', 'Q5963016', 'Q5987322', 'Q5987437', 'Q6023097', 'Q6034535', 'Q6035874', 'Q6050639', 'Q6051707', 'Q6052362', 'Q6062615', 'Q6062796', 'Q6080270', 'Q6083215', 'Q6094461', 'Q6101950', 'Q6105048', 'Q6106188', 'Q6113013', 'Q6117389', 'Q6120820', 'Q6149771', 'Q6159633', 'Q6159675', 'Q6159677', 'Q6159680', 'Q6159686', 'Q6159684', 'Q6159690', 'Q6159688', 'Q6159698', 'Q6159696', 'Q6159700', 'Q6159706', 'Q6159707', 'Q6159704', 'Q6159714', 'Q6159712', 'Q6159718', 'Q6159716', 'Q6159722', 'Q6159726', 'Q6159724', 'Q6159729', 'Q6159735', 'Q6159739', 'Q6159736', 'Q6159740', 'Q6159746', 'Q6159747', 'Q6159744', 'Q6159750', 'Q6159752', 'Q6159758', 'Q6159757', 'Q6159760', 'Q6159764', 'Q6159765', 'Q6159771', 'Q6159769', 'Q6159774', 'Q6159779', 'Q6159783', 'Q6159786', 'Q6159789', 'Q6159798', 'Q6159796', 'Q6159801', 'Q6159807', 'Q6159804', 'Q6159860', 'Q6159863', 'Q6159866', 'Q6159869', 'Q6159875', 'Q6161770', 'Q6171795', 'Q6177578', 'Q6183529', 'Q6186409', 'Q6192061', 'Q6204096', 'Q6217139', 'Q6217664', 'Q6224422', 'Q6241319', 'Q6251603', 'Q6253997', 'Q6275125', 'Q6312823', 'Q6313620', 'Q6346643', 'Q6347531', 'Q6354084', 'Q6354264', 'Q6360650', 'Q6365017', 'Q6365086', 'Q6366680', 'Q6373245', 'Q6379501', 'Q6382032', 'Q6383740', 'Q6392277', 'Q6394310', 'Q6398129', 'Q6405286', 'Q6413811', 'Q6417925', 'Q6420109', 'Q6424313', 'Q6424318', 'Q6433318', 'Q6438783', 'Q6439185', 'Q6442170', 'Q6442701', 'Q6446462', 'Q6449893', 'Q6451242', 'Q6451246', 'Q6451261', 'Q6455748', 'Q6460012', 'Q6465257', 'Q6475769', 'Q6478493', 'Q6481248', 'Q6492906', 'Q6500981', 'Q6507400', 'Q6509400', 'Q6512645', 'Q6515686', 'Q6519839', 'Q6525416', 'Q6530208', 'Q6541724', 'Q6547502', 'Q6554570', 'Q6631744', 'Q6659072', 'Q6665455', 'Q6674383', 'Q6674460', 'Q6682068', 'Q6688622', 'Q6689283', 'Q6689344', 'Q6694210', 'Q6705512', 'Q6709180', 'Q6713283', 'Q6749503', 'Q6751088', 'Q6753137', 'Q6754821', 'Q6763018', 'Q6763683', 'Q6771028', 'Q6774572', 'Q6779555', 'Q6784570', 'Q6784868', 'Q6788091', 'Q6789786', 'Q6792382', 'Q6796076', 'Q6799881', 'Q6799907', 'Q6801292', 'Q6801726', 'Q6807694', 'Q6815551', 'Q6816681', 'Q6817823', 'Q6818260', 'Q6819486', 'Q6827273', 'Q6837731', 'Q6837897', 'Q6839679', 'Q6850966', 'Q6868351', 'Q6879712', 'Q6881835', 'Q6884272', 'Q6884925', 'Q6903024', 'Q6904227', 'Q6906467', 'Q6907207', 'Q6908082', 'Q6908498', 'Q6914006', 'Q6919307', 'Q6919414', 'Q6921241', 'Q6921866', 'Q6925261', 'Q6930263', 'Q6940574', 'Q6941117', 'Q6944087', 'Q6948090', 'Q6958624', 'Q6963591', 'Q6963822', 'Q6963849', 'Q6971101', 'Q6971107', 'Q6971105', 'Q6978112', 'Q6980969', 'Q6984790', 'Q7010296', 'Q7010698', 'Q7026551', 'Q7035157', 'Q7046304', 'Q7053479', 'Q7053571', 'Q7054752', 'Q7056220', 'Q7059390', 'Q7069905', 'Q7073668', 'Q7074096', 'Q7076776', 'Q7082118', 'Q7082160', 'Q7084007', 'Q7084202', 'Q7088789', 'Q7090650', 'Q7103359', 'Q7105047', 'Q7105045', 'Q7105051', 'Q7105048', 'Q7105049', 'Q7105055', 'Q7105052', 'Q7105053', 'Q7105059', 'Q7105056', 'Q7105057', 'Q7105063', 'Q7105060', 'Q7105061', 'Q7105067', 'Q7105064', 'Q7105065', 'Q7105071', 'Q7105068', 'Q7105069', 'Q7105075', 'Q7105072', 'Q7105076', 'Q7105097', 'Q7105597', 'Q7108457', 'Q7112217', 'Q7124860', 'Q7128121', 'Q7132513', 'Q7136364', 'Q7136390', 'Q7136391', 'Q7136388', 'Q7136394', 'Q7136395', 'Q7136398', 'Q7136396', 'Q7136406', 'Q7136404', 'Q7136485', 'Q7136498', 'Q7138156', 'Q7140038', 'Q7156907', 'Q7162208', 'Q7169030', 'Q7178201', 'Q7179815', 'Q7186407', 'Q7195466', 'Q7199235', 'Q7225800', 'Q7240712', 'Q7242177', 'Q7257221', 'Q7259109', 'Q7259118', 'Q7260229', 'Q7260997', 'Q7268267', 'Q7269234', 'Q7270062', 'Q7270732', 'Q7270766', 'Q7271100', 'Q7273447', 'Q7276735', 'Q7290178', 'Q7290313', 'Q7292958', 'Q7307052', 'Q7307243', 'Q7308467', 'Q7309119', 'Q7310174', 'Q7310399', 'Q7313619', 'Q7319614', 'Q7321123', 'Q7334402', 'Q7337128', 'Q7337414', 'Q7338072', 'Q7338554', 'Q7346641', 'Q7355385', 'Q7356515', 'Q7359133', 'Q7359930', 'Q7360060', 'Q7368490', 'Q7373834', 'Q7373835', 'Q7373836', 'Q7374979', 'Q7382790', 'Q7397937', 'Q7400442', 'Q7403230', 'Q7404826', 'Q7413313', 'Q7413603', 'Q7414566', 'Q7414743', 'Q7414901', 'Q7416259', 'Q7422667', 'Q7423241', 'Q7425054', 'Q7429204', 'Q7431032', 'Q7431779', 'Q7431989', 'Q7443926', 'Q7445268', 'Q7446538', 'Q7451248', 'Q7455417', 'Q7487463', 'Q7488599', 'Q7492594', 'Q7496970', 'Q7517208', 'Q7527037', 'Q7539562', 'Q7566002', 'Q7566305', 'Q7566560', 'Q7566849', 'Q7567837', 'Q7568404', 'Q7568633', 'Q7569315', 'Q7570953', 'Q8035422', 'Q8038234', 'Q8038333', 'Q8044781', 'Q8047084', 'Q8048094', 'Q8054635', 'Q8071897', 'Q8078344', 'Q8203213', 'Q8203264', 'Q8251228', 'Q8343597', 'Q8343771', 'Q8561722', 'Q8563621', 'Q9002880', 'Q9003738', 'Q9005444', 'Q9010895', 'Q9010892', 'Q9010898', 'Q9010897', 'Q9010902', 'Q9010907', 'Q9010911', 'Q9010908', 'Q9010914', 'Q9010917', 'Q9010921', 'Q9010925', 'Q9010931', 'Q9010928', 'Q9010943', 'Q9010947', 'Q9010949', 'Q9010954', 'Q9010957', 'Q9010962', 'Q9010963', 'Q9010970', 'Q9010975', 'Q9010973', 'Q9010977', 'Q9010982', 'Q9010986', 'Q9010985', 'Q9010989', 'Q9011001', 'Q9017951', 'Q9034856', 'Q9054473', 'Q9055770', 'Q9055783', 'Q9055804', 'Q9055827', 'Q9096880', 'Q9158893', 'Q10305530', 'Q10305531', 'Q10305535', 'Q10305532', 'Q10305533', 'Q10305539', 'Q10305536', 'Q10305542', 'Q10305543', 'Q10305540', 'Q10305541', 'Q10305544', 'Q10314485', 'Q10344937', 'Q10540701', 'Q10749928', 'Q10752072', 'Q10874031', 'Q10874028', 'Q10914190', 'Q10924128', 'Q10934500', 'Q11156851', 'Q11224475', 'Q11235207', 'Q11235239', 'Q11235293', 'Q11235318', 'Q11235342', 'Q11235374', 'Q11235414', 'Q11260140', 'Q11260219', 'Q11263854', 'Q11265644', 'Q11265896', 'Q11271747', 'Q11273098', 'Q11273221', 'Q11305067', 'Q11306622', 'Q11318399', 'Q7593903', 'Q7599852', 'Q7603023', 'Q7603982', 'Q7606757', 'Q7607242', 'Q7619314', 'Q7619506', 'Q7623187', 'Q7636914', 'Q7640428', 'Q7641480', 'Q7658896', 'Q7680346', 'Q7683047', 'Q7710591', 'Q7719017', 'Q7719467', 'Q7734512', 'Q7740370', 'Q7753799', 'Q7757538', 'Q7777576', 'Q7795755', 'Q7796667', 'Q7800874', 'Q7813303', 'Q7814010', 'Q7814154', 'Q7827382', 'Q7829690', 'Q7837547', 'Q7845745', 'Q7845798', 'Q7849674', 'Q7851146', 'Q7865354', 'Q7882375', 'Q7894514', 'Q7895133', 'Q7895164', 'Q7895350', 'Q7895445', 'Q7895482', 'Q7895517', 'Q7895521', 'Q7895528', 'Q7895589', 'Q7895612', 'Q7895895', 'Q7895912', 'Q7895950', 'Q7895949', 'Q7896210', 'Q7896216', 'Q7896414', 'Q7896412', 'Q7902320', 'Q7911832', 'Q7912071', 'Q7913733', 'Q7921716', 'Q7930160', 'Q7932464', 'Q7934482', 'Q7941038', 'Q7945596', 'Q7945707', 'Q7958966', 'Q7959533', 'Q7959606', 'Q7960871', 'Q7961105', 'Q7966148', 'Q7968923', 'Q7971999', 'Q7972107', 'Q7975512', 'Q7981039', 'Q7981239', 'Q7987735', 'Q7992239', 'Q7995285', 'Q8000170', 'Q8000328', 'Q8001101', 'Q8001583', 'Q8005159', 'Q8008773', 'Q8013779', 'Q8014317', 'Q8021157', 'Q8022455', 'Q8022986', 'Q8023946', 'Q8025508', 'Q8028642', 'Q8030366', 'Q8032819', 'Q8033579', 'Q12874443', 'Q12874706', 'Q13091131', 'Q13442808', 'Q13442809', 'Q13534604', 'Q13534608', 'Q13563536', 'Q13636396', 'Q13643615', 'Q14324727', 'Q14468502', 'Q14544954', 'Q14685978', 'Q14689260', 'Q14689264', 'Q14689872', 'Q14692903', 'Q14704991', 'Q14705032', 'Q14705638', 'Q14708086', 'Q14708290', 'Q14708448', 'Q14708565', 'Q14708625', 'Q14715663', 'Q14716398', 'Q14716828', 'Q15068096', 'Q15068097', 'Q15068112', 'Q15068138', 'Q15068143', 'Q15068141', 'Q15068182', 'Q15068194', 'Q15068195', 'Q15068201', 'Q15068204', 'Q15068219', 'Q15107882', 'Q15207803', 'Q15262331', 'Q15295535', 'Q15304757', 'Q15712913', 'Q15730699', 'Q15820827', 'Q15873460', 'Q15972919', 'Q16132546', 'Q16216945', 'Q16303803', 'Q16508372', 'Q16508380', 'Q16510029', 'Q16580816', 'Q16580825', 'Q16580851', 'Q16580884', 'Q16580893', 'Q16580929', 'Q16580941', 'Q16615423', 'Q16643901', 'Q16645064', 'Q16654688', 'Q16735524', 'Q16802179', 'Q16875434', 'Q16880106', 'Q16902084', 'Q16964582', 'Q16971124', 'Q17002506', 'Q17028819', 'Q17052559', 'Q17069159', 'Q17212514', 'Q17212523', 'Q17213039', 'Q17213377', 'Q17222373', 'Q17224197', 'Q17226252', 'Q17229517', 'Q17279814', 'Q17279815', 'Q17279812', 'Q17279813', 'Q17279816', 'Q17485550', 'Q17495732', 'Q17623038', 'Q17630747', 'Q17990227', 'Q18043463', 'Q18060537', 'Q18289109', 'Q18337303', 'Q18337868', 'Q18356982', 'Q18389907', 'Q18418712', 'Q18418721', 'Q18418724', 'Q18418728', 'Q18432909', 'Q18458536', 'Q18463502', 'Q18466636', 'Q18574984', 'Q18578011', 'Q18578016', 'Q18745818', 'Q18748726', 'Q18771183', 'Q18771243', 'Q18775796', 'Q18785665', 'Q18785758', 'Q18817140', 'Q19601767', 'Q19601768', 'Q19601769', 'Q19873756', 'Q19908972', 'Q20022388', 'Q20024801', 'Q20073887', 'Q20128291', 'Q20453578', 'Q20522889', 'Q20667967', 'Q20708737', 'Q20892053', 'Q21020990', 'Q21031176', 'Q21074538', 'Q21074551', 'Q21074552', 'Q21162430', 'Q21406869', 'Q21707417', 'Q21925410', 'Q21928223', 'Q22691420', 'Q23034758', 'Q23198440', 'Q24190463', 'Q24191415', 'Q24206915', 'Q24228302', 'Q24236081', 'Q24777248', 'Q24871474', 'Q25042337', 'Q25100363', 'Q25104341', 'Q25390520', 'Q25468966', 'Q25841894', 'Q26202489', 'Q26258355', 'Q27488965', 'Q27919299', 'Q27963526', 'Q28009161', 'Q28058155', 'Q28406481', 'Q28711988', 'Q28725139', 'Q28923817', 'Q28945699', 'Q29025197', 'Q29467638', 'Q29584002', 'Q29882974', 'Q30672433', 'Q31184737', 'Q34828220', 'Q37778025', 'Q38232987', 'Q41015695', 'Q41792201', 'Q43008193', 'Q43179137', 'Q47524353', 'Q48866139', 'Q48941460', 'Q50272464', 'Q50304597', 'Q53000060', 'Q53149834', 'Q53524829', 'Q54612084', 'Q54681212', 'Q54681445', 'Q54681934', 'Q54683271', 'Q54824183', 'Q54853226', 'Q54869772', 'Q54932368', 'Q55424230', 'Q55424278', 'Q55438181', 'Q55594541', 'Q55643058', 'Q55657898', 'Q55759670', 'Q55839855', 'Q56241286', 'Q56292661', 'Q56477357', 'Q56636346', 'Q56636347', 'Q56636348', 'Q56640903', 'Q56640901', 'Q56850397', 'Q56876734', 'Q56876735', 'Q56876733', 'Q58233652', 'Q58356333', 'Q58931389', 'Q59200593', 'Q60770490', 'Q61598900', 'Q61804909', 'Q61896925', 'Q62024020', 'Q62602451', 'Q63079063', 'Q63087961', 'Q63116460', 'Q63191876', 'Q63246853', 'Q63246945', 'Q63247113', 'Q63248537', 'Q63249301', 'Q63251296', 'Q63252691', 'Q63252689', 'Q63252692', 'Q63252698', 'Q63252700', 'Q63252701', 'Q63252821', 'Q63253119', 'Q63253373', 'Q63254307', 'Q63423783', 'Q64128791', 'Q65007833', 'Q65152941', 'Q65329582', 'Q65485192', 'Q65552653', 'Q65933898', 'Q66521786', 'Q66765752', 'Q66814304', 'Q66942450', 'Q67009068', 'Q68191059', 'Q68654324', 'Q68654764', 'Q68655029', 'Q68656621', 'Q68657274', 'Q68658419', 'Q68658800', 'Q68661377', 'Q68661966', 'Q68664567', 'Q68773451', 'Q68773878', 'Q68774231', 'Q68776348', 'Q68777459', 'Q68777896', 'Q68778220', 'Q68778647', 'Q68779947', 'Q68869594', 'Q68870148', 'Q68881461', 'Q68882889', 'Q68883961', 'Q68884392', 'Q68885137', 'Q68885551', 'Q68915480', 'Q68915891', 'Q68916828', 'Q69289379', 'Q69290351', 'Q69290437', 'Q69290509', 'Q69290568', 'Q69291041', 'Q69291563', 'Q69293705', 'Q69294377', 'Q69295853', 'Q69297013', 'Q69298005', 'Q69299557', 'Q69300075', 'Q69301804', 'Q69303111', 'Q69303993', 'Q69305602', 'Q69388013', 'Q69388869', 'Q69480120', 'Q69480494', 'Q69481419', 'Q69482290', 'Q69482638', 'Q69524290', 'Q69524375', 'Q69542158', 'Q69546874', 'Q69549381', 'Q71823292', 'Q72926931', 'Q74703619', 'Q75042415', 'Q80356185', 'Q81720226', 'Q83968838', 'Q84131683', 'Q84435783', 'Q85216414', 'Q85281604', 'Q11325259', 'Q11333595', 'Q11333601', 'Q11335164', 'Q11353780', 'Q11360263', 'Q11362519', 'Q11374609', 'Q11377996', 'Q11380531', 'Q11380529', 'Q11381641', 'Q11383107', 'Q11385048', 'Q11391725', 'Q11397147', 'Q11397314', 'Q11402467', 'Q11404152', 'Q11406052', 'Q11415127', 'Q11415201', 'Q11417470', 'Q11427321', 'Q11444056', 'Q11449473', 'Q11450483', 'Q11465466', 'Q11469688', 'Q11479135', 'Q11480258', 'Q11482157', 'Q11486557', 'Q11487751', 'Q11489818', 'Q11494133', 'Q11496999', 'Q11498634', 'Q11504396', 'Q11521829', 'Q11527327', 'Q11527576', 'Q11528762', 'Q11532109', 'Q11543497', 'Q11557781', 'Q11568009', 'Q11580390', 'Q11581471', 'Q11589167', 'Q11589823', 'Q11594341', 'Q11609397', 'Q11612037', 'Q11614982', 'Q11615227', 'Q11615228', 'Q11622553', 'Q11626052', 'Q11633027', 'Q11633858', 'Q11641183', 'Q11648257', 'Q11651440', 'Q11663873', 'Q11664683', 'Q11678534', 'Q11679872', 'Q11684456', 'Q11684461', 'Q11684471', 'Q11684480', 'Q11684488', 'Q11684493', 'Q11684506', 'Q11688583', 'Q11695430', 'Q11796742', 'Q11796743', 'Q11796745', 'Q11854549', 'Q11854739', 'Q11981731', 'Q11990287', 'Q12059502', 'Q12060327', 'Q12061907', 'Q12135229', 'Q12176716', 'Q12176720', 'Q12186211', 'Q12273822', 'Q12273823', 'Q12297237', 'Q12302850', 'Q12308250', 'Q12313655', 'Q12404972', 'Q12537486', 'Q12628189']\n"
     ]
    }
   ],
   "source": [
    "print(trimmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data about herbaria from Index Herbariorum\n",
    "\n",
    "Create a table with the useful information about each herbariumlisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve JSON data from the API\n",
    "acceptMediaType = 'application/json'\n",
    "endpointUrl = 'http://sweetgum.nybg.org/science/api/v1/institutions'\n",
    "r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "dataFull = r.json()\n",
    "data = dataFull['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(data[0:10], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pull out the data from the JSON and put in list of lists (table) in a file\n",
    "output_table = []\n",
    "fieldnames = ['wikidata_id', 'orgid', 'organization', 'code', 'division', 'department', 'email']\n",
    "\n",
    "for herb in data:\n",
    "    results_dict = {}\n",
    "    results_dict['wikidata_id'] = ''\n",
    "    results_dict['orgid'] = herb['something']\n",
    "    results_dict['organization'] = herb['organization']\n",
    "    results_dict['code'] = herb['code']\n",
    "    results_dict['division'] = herb['division']\n",
    "    results_dict['department'] = herb['department']\n",
    "    results_dict['email'] = herb['contact']['email']\n",
    "    output_table.append(results_dict)\n",
    "\n",
    "filename = 'herb-basic.csv'\n",
    "writeDictsToCsv(output_table, filename, fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match candidate items with IH collections\n",
    "\n",
    "For each of the retrieved items, do fuzzy matching with IH collection names and see which is most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_results = []\n",
    "for result in test_result[0:1]:\n",
    "    herbarium_matches = []\n",
    "    print('Class:', result['class'])\n",
    "    for item in result['labels']:\n",
    "        for row in output_table:\n",
    "            nameTestRatio = fuzz.token_set_ratio(item['string'], row['organization'])\n",
    "            if nameTestRatio >= 90:\n",
    "                herbarium_matches.append({'qid': item['qid'], 'orgid': row['orgid'], 'name': row['organization']})\n",
    "    match_results.append({'class': result['class'], 'matches': herbarium_matches)\n",
    "    print('Matches: ', len(herbarium_matches))\n",
    "    print('Fraction:' , len(herbarium_matches)/len(result['labels']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByIhCode(herb_code):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P5858 \"''' + herb_code + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# Look up the institution using the IH code\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "people = readDict(filename)\n",
    "\n",
    "#for personIndex in range(0, len(people)):\n",
    "for personIndex in range(1, 100): \n",
    "    ihCodes = searchWikidataForQIdByIhCode(people[personIndex]['herb_code'])\n",
    "    print(people[personIndex]['herb_code'], ihCodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match employees to Wikidata\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/match_bsci_wikidata.ipynb\n",
    "\n",
    "Attempts to match records of people Wikidata knows to work at Vanderbilt with departmental people by matching their ORCIDs, then name strings. If there isn't a match with the downloaded Wikidata records, for employees with ORCIDs, the script attempts to find them in Wikidata by directly doing a SPARQL search for their ORCID.\n",
    "\n",
    "As people are matched (or determined to not have a match), a code is recorded with information about how the match was made.  Here are the values:\n",
    "\n",
    "```\n",
    "0=unmatched\n",
    "1=matched with ORCID in both sources\n",
    "2=ORCID from match to ORCID records but name match to Wikidata (no ORCID)\n",
    "3=no ORCID from match to ORCID records but name match to Wikidata (with ORCID); could happen if affiliation isn't matched in ORCID\n",
    "4=no ORCID from match to ORCID records but name match to Wikidata (no ORCID)\n",
    "5=ORCID from match to ORCID records and found via SPARQL ORCID search (likely non-VU affiliated in Wikidata)\n",
    "6=ORCID from match to ORCID records and found via SPARQL name search (non-VU affiliated without ORCID)\n",
    "7=no name match\n",
    "8=ORCID from match to ORCID records, error in SPARQL ORCID search\n",
    "9=no ORCID from match to ORCID records, error in SPARQL name search\n",
    "10=affiliation match in article\n",
    "11=match by human choice after looking at entity data\n",
    "12=no matching entities were possible matches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscheck people against publications\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/crosscheck-publications.ipynb\n",
    "\n",
    "Checks possible Wikidata records against publications in CrossRef and PubMed to see if the author metadata will disambiguate the Wikidata record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptMediaType = 'application/json'\n",
    "requestHeaderDictionary = generateHeaderDictionary(acceptMediaType)\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        initials.append(piece[0:1])\n",
    "    \n",
    "    # NOTE: currently doesn't handle \", Jr.\", \"III\", etc.\n",
    "    \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsDict\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qId\n",
    "def searchWikidataArticle(qId):\n",
    "    resultsList = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article; P356 is the DOI of the article\n",
    "    query = '''select distinct ?title ?doi ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qId + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = 'en')\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      optional {?article wdt:P356 ?doi.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                #print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            if 'doi' in statement:\n",
    "                doi = statement['doi']['value']\n",
    "            else:\n",
    "                doi = ''\n",
    "            resultsList.append({'title': title, 'pmid': pmid, 'doi': doi})\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def retrievePubMedData(pmid):\n",
    "    fetchUrl = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    paramDict = {\n",
    "        'tool': toolName, \n",
    "        'email': emailAddress,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetchUrl, params=paramDict)    \n",
    "    #print(response.url)\n",
    "    if response.status_code == 404:\n",
    "        affiliations = [] # return an empty list if the constructed URL won't dereference\n",
    "    else:\n",
    "        pubData = response.text  # the response text is XML\n",
    "        #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "        # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "        root = et.fromstring(pubData)\n",
    "        try:\n",
    "            title = root.findall('.//ArticleTitle')[0].text\n",
    "        except:\n",
    "            title = ''\n",
    "        names = root.findall('.//Author')\n",
    "        affiliations = []\n",
    "        for name in names:\n",
    "            try:\n",
    "                affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "            except:\n",
    "                affiliation = ''\n",
    "            try:\n",
    "                lastName = name.find('./LastName').text\n",
    "            except:\n",
    "                lastName = ''\n",
    "            try:\n",
    "                foreName = name.find('./ForeName').text\n",
    "            except:\n",
    "                foreName = ''\n",
    "            try:\n",
    "                idField = name.find('./Identifier')\n",
    "                if idField.get('Source') == 'ORCID':\n",
    "                    orcid = idField.text\n",
    "                else:\n",
    "                    orcid = ''\n",
    "            except:\n",
    "                orcid = ''\n",
    "\n",
    "            #print(lastName)\n",
    "            #print(affiliation)\n",
    "            affiliations.append({'affiliation': affiliation, 'surname': lastName, 'forename': foreName, 'orcid': orcid})\n",
    "        #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.5) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return affiliations\n",
    "\n",
    "def retrieveCrossRefDoi(doi):\n",
    "    authorList = []\n",
    "    crossRefEndpointUrl = 'https://api.crossref.org/works/'\n",
    "    encodedDoi = urllib.parse.quote(doi)\n",
    "    searchUrl = crossRefEndpointUrl + encodedDoi\n",
    "    acceptMediaType = 'application/json'\n",
    "    response = requests.get(searchUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    if response.status_code == 404:\n",
    "        authorList = [] # return an empty list if the DOI won't dereference at CrossRef\n",
    "    else:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            #print(json.dumps(data, indent = 2))\n",
    "            if 'author' in data['message']:\n",
    "                authors = data['message']['author']\n",
    "                for author in authors:\n",
    "                    authorDict = {}\n",
    "                    if 'ORCID' in author:\n",
    "                        authorDict['orcid'] = author['ORCID']\n",
    "                    else:\n",
    "                        authorDict['orcid'] = ''\n",
    "                    if 'given' in author:\n",
    "                        authorDict['givenName'] = author['given']\n",
    "                    else:\n",
    "                        authorDict['givenName'] = ''\n",
    "                    if 'family' in author:\n",
    "                        authorDict['familyName'] = author['family']\n",
    "                    else:\n",
    "                        authorDict['familyName'] = ''\n",
    "                    affiliationList = []\n",
    "                    if 'affiliation' in author:\n",
    "                        for affiliation in author['affiliation']:\n",
    "                            affiliationList.append(affiliation['name'])\n",
    "                    # if there aren't any affiliations, the list will remain empty\n",
    "                    authorDict['affiliation'] = affiliationList\n",
    "                    authorList.append(authorDict)\n",
    "        except:\n",
    "            authorList = [data]\n",
    "    return authorList\n",
    "\n",
    "# ***** BODY OF SEARCH\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "#for employeeIndex in range(0, len(employees)):\n",
    "for employeeIndex in range(11, 50): # just do one person for testing\n",
    "    # perform search only for people who weren't already matched\n",
    "    if employees[employeeIndex]['wikidataStatus'] == '0':\n",
    "        matchStatus = 0\n",
    "        print('--------------------------')\n",
    "        results = searchNameAtWikidata(employees[employeeIndex]['name'])\n",
    "        print('Position: ', employees[employeeIndex]['position'], ', Specialities: ', employees[employeeIndex]['specialities'])\n",
    "        print('Born: ', employees[employeeIndex]['birth_date'], ', Herb code: ', employees[employeeIndex]['herb_code'], ', Place: ', employees[employeeIndex]['city'], ', ', employees[employeeIndex]['state'])\n",
    "        if len(results) == 0:\n",
    "            print('No Wikidata name match: ', employees[employeeIndex]['name'])\n",
    "            matchStatus = 7\n",
    "            print()\n",
    "        else:\n",
    "            print('SPARQL name search: ', employees[employeeIndex]['name'])\n",
    "            if len(results) == 1:\n",
    "                if 'error' in results[0]:\n",
    "                    matchStatus = 9\n",
    "                    print('Error message in\n",
    "                          name search:', results[0]['error'])\n",
    "                    break # discontinue processing this person\n",
    "            qIds = []\n",
    "            nameVariants = []\n",
    "            potentialOrcid = []\n",
    "            for result in results:\n",
    "                qIds.append(result['qId'])\n",
    "                nameVariants.append(result['name'])\n",
    "            \n",
    "            testAuthor = employees[employeeIndex]['name']\n",
    "            testOrcid = employees[employeeIndex]['orcid']\n",
    "\n",
    "            if testOrcid == '':\n",
    "                print('(no ORCID)')\n",
    "            else:\n",
    "                print('ORCID: ', testOrcid)\n",
    "            print()\n",
    "            \n",
    "            foundMatch = False # start the flag with the person not being matched\n",
    "            possibleMatch = False # start the flag with there not being a possibility that the person could match\n",
    "            for qIdIndex in range(0, len(qIds)):\n",
    "                potentialOrcid.append('') # default to no ORCID found for that person\n",
    "                print()\n",
    "                print(qIdIndex, 'Wikidata ID: ', qIds[qIdIndex], ' Name variant: ', nameVariants[qIdIndex], ' ', 'https://www.wikidata.org/wiki/' + qIds[qIdIndex])\n",
    "                wdClassList = searchWikidataSingleProperty(qIds[qIdIndex], 'P31', 'item')\n",
    "                # if there is a class property, check if it's a human\n",
    "                if len(wdClassList) != 0:\n",
    "                    # if it's not a human\n",
    "                    if wdClassList[0] != 'Q5':\n",
    "                        print('This item is not a human!')\n",
    "                        break\n",
    "                        \n",
    "                # check for a death date\n",
    "                deathDateList = searchWikidataSingleProperty(qIds[qIdIndex], 'P570', 'string')\n",
    "                if len(deathDateList) == 0:\n",
    "                    print('No death date given.')\n",
    "                else:\n",
    "                    deathDate = deathDateList[0][0:10] # all dates are converted to xsd:dateTime and will have a y-m-d date\n",
    "                    if deathDate < deathDateLimit:\n",
    "                        # if the person died a long time ago, don't retrieve other stuff\n",
    "                        print('This person died in ', deathDate)\n",
    "                        break\n",
    "                    else:\n",
    "                        # if the person died recently, we still might be interested in them so keep going\n",
    "                        print('This person died in ', deathDate)\n",
    "\n",
    "                # check for a birth date\n",
    "                if employees[employeeIndex]['birth_date'] != '': # only check Wikidata if the person has a birthdate\n",
    "                    birthDateList = searchWikidataSingleProperty(qIds[qIdIndex], 'P569', 'string')\n",
    "                    if len(birthDateList) == 0: # do nothing if there are no birthdates retrieved from Wikidata\n",
    "                        print('No birth date given.')\n",
    "                    else:\n",
    "                        birthDate = birthDateList[0][0:4] # get only the first four digits since only years are given\n",
    "                        if birthDate != employees[employeeIndex]['birth_date']:\n",
    "                            print('Wikidata birthdate ', birthDate, ' does not match ', employees[employeeIndex]['birth_date'])\n",
    "                            break\n",
    "\n",
    "                descriptors = searchWikidataDescription(qIds[qIdIndex])\n",
    "                employers = searchWikidataEmployer(qIds[qIdIndex])\n",
    "                #print(descriptors)\n",
    "                if descriptors != {}:\n",
    "                    if descriptors['description'] != '':\n",
    "                        print('description: ', descriptors['description'])\n",
    "                    for occupation in descriptors['occupation']:\n",
    "                        print('occupation: ', occupation)\n",
    "                    for employer in employers:\n",
    "                        print('employer: ', employer)\n",
    "                    if descriptors['orcid'] != '':\n",
    "                        if testOrcid == '':\n",
    "                            # **** NOTE: if the person has an ORCID, it may be possible to find articles via ORCID\n",
    "                            # that aren't linked in Wikidata. Not sure if this happens often enough to handle it\n",
    "                            print('ORCID: ', descriptors['orcid'])\n",
    "                            potentialOrcid[qIdIndex] = descriptors['orcid']\n",
    "                        else:\n",
    "                            # This should always be true if the SPARQL query for ORCID was already done\n",
    "                            if testOrcid != descriptors['orcid']:\n",
    "                                print('*** NOT the same person; ORCID ' + descriptors['orcid'] + ' does not match.')\n",
    "                                break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                            else:\n",
    "                                print('*** An ORCID match! How did it get missed in the earlier SPARQL query?')\n",
    "                                break\n",
    "                else:\n",
    "                    print('No description or occupation given.')\n",
    "\n",
    "                result = searchWikidataArticle(qIds[qIdIndex])\n",
    "                if len(result) == 0:\n",
    "                    print('No articles authored by that person')\n",
    "                else:\n",
    "                    articleCount = 0\n",
    "                    for article in result:\n",
    "                        print()\n",
    "                        print('Checking article: ', article['title'])\n",
    "                        if article['pmid'] == '':\n",
    "                            print('No PubMed ID')\n",
    "                        else:\n",
    "                            print('Checking authors in PubMed article: ', article['pmid'])\n",
    "                            pubMedAuthors = retrievePubMedData(article['pmid'])\n",
    "                            if pubMedAuthors == []:\n",
    "                                print('PubMed ID does not seem to be valid.')\n",
    "                            #print(pubMedAuthors)\n",
    "                            for author in pubMedAuthors:\n",
    "                                nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "                                #print(nameTestRatio, author['surname'])\n",
    "                                if nameTestRatio >= 90:\n",
    "                                    # if the PubMed metadata gives an ORCID for the matched person, record it unless \n",
    "                                    # the ORCID has already been gotten from the Wikidata record\n",
    "                                    if author['orcid'] != '':\n",
    "                                        if testOrcid == '':\n",
    "                                            print('ORCID from article: ', author['orcid'])\n",
    "                                            if potentialOrcid[qIdIndex] == '':\n",
    "                                                potentialOrcid[qIdIndex] = author['orcid']\n",
    "                                        else:\n",
    "                                            if testOrcid != author['orcid']:\n",
    "                                                print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                            else:\n",
    "                                                print('*** An ORCID match!')\n",
    "                                                foundMatch = True\n",
    "                                                matchStatus = 6\n",
    "                                                break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "                                    if author['affiliation'] != '': \n",
    "                                        setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], author['affiliation'])\n",
    "                                        print('Affiliation test: ', setRatio, author['affiliation'])\n",
    "                                        if setRatio >= 90:\n",
    "                                            foundMatch = True\n",
    "                                            matchStatus = 10\n",
    "                                            break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                    else:\n",
    "                                        break # give up on this article because no affiliation string\n",
    "                        # Don't look up the DOI if it's already found a match with PubMed\n",
    "                        if foundMatch:\n",
    "                            break # stop checking articles after a PubMed one has matched\n",
    "                        else:\n",
    "                            if article['doi'] == '':\n",
    "                                print('No DOI')\n",
    "                            else:\n",
    "                                print('Checking authors in DOI article: ', article['doi'])\n",
    "                                doiAuthors = retrieveCrossRefDoi(article['doi'])\n",
    "                                if doiAuthors == []:\n",
    "                                    print('DOI does not dereference at CrossRef')\n",
    "                                for author in doiAuthors:\n",
    "                                    nameTestRatio = fuzz.token_set_ratio(author['familyName'], testAuthor)\n",
    "                                    #print(nameTestRatio, author['familyName'])\n",
    "                                    if nameTestRatio >= 90:\n",
    "                                        if author['orcid'] != '':\n",
    "                                            if testOrcid == '':\n",
    "                                                # DOI records the entire ORCID URI, not just the ID number\n",
    "                                                # so pull the last 19 characters from the string\n",
    "                                                print('ORCID from article: ', author['orcid'][-19:])\n",
    "                                                # only add the ORCID from article if there isn't already one,\n",
    "                                                # for example, one gotten from the Wikidata record itself\n",
    "                                                if potentialOrcid[qIdIndex] == '':\n",
    "                                                    potentialOrcid[qIdIndex] = author['orcid'][-19:]\n",
    "                                            else:\n",
    "                                                if testOrcid != author['orcid']:\n",
    "                                                    print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                    break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                                else:\n",
    "                                                    print('*** An ORCID match!')\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 6\n",
    "                                                    break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "\n",
    "                                        if len(author['affiliation']) > 0:\n",
    "                                            for affiliation in author['affiliation']:\n",
    "                                                setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], affiliation)\n",
    "                                                print('Affiliation test: ', setRatio, affiliation)\n",
    "                                                if setRatio >= 90:\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 10\n",
    "                                                    break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                        else:\n",
    "                                            break # give up on this article because no affiliation string\n",
    "                            if foundMatch:\n",
    "                                break # stop checking articles after a DOI one has matched\n",
    "                        articleCount += 1\n",
    "                        if articleCount > 10:\n",
    "                            checkMore = input('There are more than 10 articles. Press Enter to skip the rest or enter anything to get the rest.')\n",
    "                            if checkMore == '':\n",
    "                                break\n",
    "                    if foundMatch:\n",
    "                        print('***', qIds[qIdIndex], ' is a match.')\n",
    "                        print()\n",
    "                        employees[employeeIndex]['wikidataId'] = qIds[qIdIndex]\n",
    "                        employees[employeeIndex]['orcid'] = potentialOrcid[qIdIndex]\n",
    "                        break # quit checking Q IDs since the person was matched\n",
    "                    else:\n",
    "                        print('No match found.')\n",
    "                print('Employee: ', employees[employeeIndex]['name'], ' vs. name variant: ', nameVariants[qIdIndex])\n",
    "                possibleMatch = True # made it all the way through the loop without hitting a break, so a match is possible\n",
    "                print()\n",
    "            if not foundMatch:\n",
    "                if not possibleMatch:\n",
    "                    matchStatus = 12\n",
    "                else:\n",
    "                    choiceString = input('Enter the number of the matched entity, or press Enter/return if none match: ')\n",
    "                    if choiceString == '':\n",
    "                        matchStatus = 7\n",
    "                    else:\n",
    "                        # NOTE: there is no error trapping here for mis-entry !!!\n",
    "                        choice = int(choiceString)\n",
    "                        matchStatus = 11\n",
    "                        employees[employeeIndex]['wikidataId'] = qIds[choice]\n",
    "                        # write a discovered ORCID only if the person didn't already have one\n",
    "                        if (potentialOrcid[choice] != '') and (employees[employeeIndex]['orcid'] == ''):\n",
    "                            employees[employeeIndex]['orcid'] = potentialOrcid[choice]\n",
    "                    print()\n",
    "                \n",
    "        # record the final match status\n",
    "        employees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "    \n",
    "    # write the file after each person is checked in case the user crashes the script\n",
    "    filename = deptShortName + '-employees-curated.csv'\n",
    "    fieldnames = ['wikidataId', 'name', 'irn', 'herb_code', 'birth_date', 'position', 'specialities', 'city', 'state', 'wikidataStatus', 'orcid']\n",
    "    writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download various statements and references, then generate write file\n",
    "\n",
    "NOTE: between the previous step and this one, one can add a gender/sex column to the table that will be processed if it exists.  Column header: 'gender'.  Allowed values (from Wikidata): m=male, f=female, i=intersex, tf=transgender female, tm=transgender male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees-curated.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "# create a list of the employees who have Wikidata qIDs\n",
    "qIds = []\n",
    "for employee in employees:\n",
    "    if employee['wikidataId'] != '':\n",
    "        qIds.append(employee['wikidataId'])\n",
    "\n",
    "# get all of the ORCID data that is already in Wikidata\n",
    "prop = 'P496' # ORCID iD\n",
    "value = '' # since no value is passed, the search will retrieve the value\n",
    "refProps = ['P813'] # retrieved\n",
    "wikidataOrcidData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "#print(json.dumps(wikidataOrcidData, indent=2))\n",
    "\n",
    "# match people who have ORCIDs with ORCID data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataOrcidDataIndex in range(0, len(wikidataOrcidData)):\n",
    "        if wikidataOrcidData[wikidataOrcidDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            if employees[employeeIndex]['orcid'] != wikidataOrcidData[wikidataOrcidDataIndex]['statementValue']:\n",
    "                print('Non-matching ORCID for ', employees[employeeIndex]['name'])\n",
    "            # if there is a match, record whatever data was retrieved\n",
    "            else:\n",
    "                employees[employeeIndex]['orcidStatementUuid'] = wikidataOrcidData[wikidataOrcidDataIndex]['statementUuid']\n",
    "                employees[employeeIndex]['orcidReferenceHash'] = wikidataOrcidData[wikidataOrcidDataIndex]['referenceHash']\n",
    "                # if there is no referenceHash then try to dereference the ORCID\n",
    "                if employees[employeeIndex]['orcidReferenceHash']== '':\n",
    "                    # if there is a match, check whether the ORCID record can be retrieved\n",
    "                    print('Checking ORCID for Wikidata matched: ', employees[employeeIndex]['name'])\n",
    "                    # returned value is the current date if successful; empty string if not\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "                # if there is an existing reference, record the value for the first reference property (only one ref property)\n",
    "                else:\n",
    "                    print('Already an ORCID reference for: ', employees[employeeIndex]['name'])\n",
    "                    # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = '+' + wikidataOrcidData[wikidataOrcidDataIndex]['referenceValues'][0]\n",
    "            # stop checking at the first match.\n",
    "            break\n",
    "    # if the person doesn't match with those whose ORCIDs came back from the query...\n",
    "    if not matched:\n",
    "        # check for access if they have an ORCID (not present in Wikidata)\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            print('Checking ORCID for unmatched: ', employees[employeeIndex]['name'])\n",
    "            # the function returns the current date (to use as the retrieved date) if the ORCID is found, otherwise empty string\n",
    "            employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "\n",
    "# get data already in Wikidata about people employed at Vanderbilt\n",
    "prop = 'P108' # employer\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, employerQId, refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with employment data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['employerStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['employerReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['employerReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['employerReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['employerReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrite any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "    # everyone is assigned the employerQId as a value because either they showed up in the SPARQL search for employerQId\n",
    "    # or we are making a statement that they work for employerQId.\n",
    "    employees[employeeIndex]['employer'] = employerQId\n",
    "    if not matchedReference:  # generate the reference metadata if the reference URL wasn't found\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['employerReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['employerReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# *** This is a copy and paste of the employer section above, modified for affiliation\n",
    "\n",
    "# get data already in Wikidata about people affiliated with the department\n",
    "prop = 'P1416' # affiliation\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, deptSettings[deptShortName]['departmentQId'], refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with affiliation data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['affiliationStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['affiliationReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['affiliationReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['affiliationReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['affiliationReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrited any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "        \n",
    "    # everyone is assigned the department as a value because either they showed up in the SPARQL search\n",
    "    # or we are making a statement that they are affiliated with the department.\n",
    "    employees[employeeIndex]['affiliation'] = deptSettings[deptShortName]['departmentQId']\n",
    "    if not matchedReference:  # generate the reference metadata if the reference URL wasn't found\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['affiliationReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['affiliationReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# get all of the data that is already in Wikidata about who are humans\n",
    "prop = 'P31' # instance of\n",
    "value = 'Q5' # human\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions that they are humans and record their statement IDs.\n",
    "# Assign the properties to all others.\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            employees[employeeIndex]['instanceOfUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "    # everybody is assigned a value of 'human'\n",
    "    employees[employeeIndex]['instanceOf'] = 'Q5'\n",
    "\n",
    "# hack of human code immediately above\n",
    "\n",
    "# get all of the data that is already in Wikidata about the sex or gender of the researchers\n",
    "prop = 'P21' # sex or gender\n",
    "value = '' # don't provide a value so that it will return whatever value it finds\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions of sex/gender and record their statement IDs.\n",
    "# Assign the value for the property to all others.\n",
    "# NOTE: Wikidata doesn't seem to care a lot about references for this property and we don't really have one anyway\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['sexOrGenderUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "            # use the value in Wikidata and ignore the value in the 'gender' column of the table.\n",
    "            # extractFromIri() function strips the namespace from the qId\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = extractFromIri(wikidataHumanData[wikidataHumanIndex]['statementValue'], 4)\n",
    "    if not matched:\n",
    "        # assign the value from the 'gender' column in the table if not already in Wikidata\n",
    "        if 'gender' in employees[employeeIndex]:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = decodeSexOrGender(employees[employeeIndex]['gender'])\n",
    "        else:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = ''\n",
    "\n",
    "# get all of the English language labels for the employees that are already in Wikidata\n",
    "labelType = 'label'\n",
    "language = 'en'\n",
    "wikidataLabels = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their labels\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataLabelIndex in range(0, len(wikidataLabels)):\n",
    "        if wikidataLabels[wikidataLabelIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['labelEn'] = wikidataLabels[wikidataLabelIndex]['string']\n",
    "    if not matched:\n",
    "        # assign the value from the 'name' column in the table if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['labels']['source'] == 'column':\n",
    "            # then use the value from the default label column.\n",
    "            defaultLabelColumn = deptSettings[deptShortName]['labels']['value']\n",
    "            employees[employeeIndex]['labelEn'] = employees[employeeIndex][defaultLabelColumn]\n",
    "        else:\n",
    "            # or use the default label value.\n",
    "            employees[employeeIndex]['labelEn'] = deptSettings[deptShortName]['labels']['value']\n",
    "\n",
    "# get all of the English language descriptions for the employees that are already in Wikidata\n",
    "labelType = 'description'\n",
    "language = 'en'\n",
    "wikidataDescriptions = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their descriptions\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataDescriptionIndex in range(0, len(wikidataDescriptions)):\n",
    "        if wikidataDescriptions[wikidataDescriptionIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['description'] = wikidataDescriptions[wikidataDescriptionIndex]['string']\n",
    "    if not matched:\n",
    "        # assign a default value if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['descriptions']['source'] == 'column':\n",
    "            # then use the value from the default description column.\n",
    "            defaultDescriptionColumn = deptSettings[deptShortName]['descriptions']['value']\n",
    "            employees[employeeIndex]['description'] = employees[employeeIndex][defaultDescriptionColumn]\n",
    "        else:\n",
    "            # or use the default description value.\n",
    "            employees[employeeIndex]['description'] = deptSettings[deptShortName]['descriptions']['value']\n",
    "\n",
    "# Get all of the aliases already at Wikidata for employees.  \n",
    "# Since there can be multiple aliases, they are stored as a list structure.\n",
    "# The writing script can handle multiple languages, but here we are only dealing with English ones.\n",
    "\n",
    "# retrieve the aliases in that language that already exist in Wikidata and match them with table rows\n",
    "labelType = 'alias'\n",
    "language = 'en'\n",
    "aliasesAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "for entityIndex in range(0, len(employees)):\n",
    "    personAliasList = []\n",
    "    if employees[entityIndex]['wikidataId'] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "        for wikiLabel in aliasesAtWikidata:\n",
    "            if employees[entityIndex]['wikidataId'] == wikiLabel['qId']:\n",
    "                personAliasList.append(wikiLabel['string'])\n",
    "    # if not found, the personAliasList list will remain empty\n",
    "    employees[entityIndex]['alias'] = json.dumps(personAliasList)\n",
    "\n",
    "# set the departmental short name for all entities\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    employees[employeeIndex]['department'] = deptShortName\n",
    "\n",
    "# write the file\n",
    "filename = deptShortName + '-employees-to-write.csv'\n",
    "fieldnames = ['department', 'wikidataId', 'name', 'labelEn', 'alias', 'description', 'orcidStatementUuid', 'orcid', 'orcidReferenceHash', 'orcidReferenceValue', 'employerStatementUuid', 'employer', 'employerReferenceHash', 'employerReferenceSourceUrl', 'employerReferenceRetrieved', 'affiliationStatementUuid', 'affiliation', 'affiliationReferenceHash', 'affiliationReferenceSourceUrl', 'affiliationReferenceRetrieved', 'instanceOfUuid', 'instanceOf', 'sexOrGenderUuid', 'sexOrGenderQId', 'gender', 'degree', 'category', 'wikidataStatus', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set file name in CSV metadata file\n",
    "\n",
    "Prior to writing the data to Wikidata using the `process_csv_metadata_full.py` script, the input file name needs to be changed in the `csv-metadata.json` file to have the correct `deptShortName` for the department. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csv-metadata.json', 'rt', encoding='utf-8') as inFileObject:\n",
    "    text = inFileObject.read()\n",
    "schema = json.loads(text)\n",
    "schema['tables'][0]['url'] = deptShortName + '-employees-to-write.csv'\n",
    "outText = json.dumps(schema, indent = 2)\n",
    "with open('csv-metadata.json', 'wt', encoding='utf-8') as outFileObject:\n",
    "    outFileObject.write(outText)\n",
    "print('Department to be written:', deptShortName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
