{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define functions and classes\n",
    "\n",
    "Methods of the `Query()` class sends queries to Wikibase instances. It has the following methods:\n",
    "\n",
    "`.generic_query(query)` Sends a specified query to the endpoint and returns a list of item Q IDs, item labels, or literal values. The variable to be returned must be `?object`.\n",
    "\n",
    "`.single_property_values_for_item(qid)` Sends a subject Q ID to the endpoint and returns a list of item Q IDs, item labels, or literal values that are values of a specified property.\n",
    "\n",
    "`.labels_descriptions(qids)` Sends a list of subject Q IDs to the endpoint and returns a list of dictionaries of the form `{'qid': qnumber, 'string': string}` where `string` is either a label, description, or alias.\n",
    "\n",
    "`.search_statement(qids, reference_property_list)` Sends a list of Q IDs and a list of reference properties to the endpoint and returns information about statements using a specified property. If no value is specified, the information includes the values of the statements. For each statement, the reference UUID, reference property, and reference value is returned. If the statement has more than one reference, there will be multiple results per subject. Results are in the form `{'qId': qnumber, 'statementUuid': statement_uuid, 'statementValue': statement_value, 'referenceHash': reference_hash, 'referenceValue': reference_value}`\n",
    "\n",
    "It has the following attributes:\n",
    "\n",
    "| key | description | default value | applicable method |\n",
    "|:-----|:-----|:-----|:-----|\n",
    "| `endpoint` | endpoint URL of Wikabase | `https://query.wikidata.org/sparql` | all |\n",
    "| `mediatype` | Internet media type | `application/json` | all |\n",
    "| `useragent` | User-Agent string to send | `VanderBot/0.8` etc.| all |\n",
    "| `requestheader` | request headers to send |(generated dict) | all |\n",
    "| `sleep` | seconds to delay between queries | 0.25 | all |\n",
    "| `isitem` | `True` if value is item, `False` if value a literal | `True` | `generic_query`, `single_property_values_for_item` |\n",
    "| `uselabel` | `True` for label of item value , `False` for Q ID of item value | `True` | `generic_query`, `single_property_values_for_item` | \n",
    "| `lang` | language of label | `en` | `single_property_values_for_item`, `labels_descriptions`|\n",
    "| `labeltype` | returns `label`, `description`, or `alias` | `label` | `labels_descriptions` |\n",
    "| `labelscreen` | added triple pattern | empty string | `labels_descriptions` |\n",
    "| `pid` | property P ID | `P31` | `single_property_values_for_item`, `search_statement` |\n",
    "| `vid` | value Q ID | empty string | `search_statement` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "import json\n",
    "import csv\n",
    "from time import sleep\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extract_qnumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extract_from_iri(iri, number_pieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[number_pieces]\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csv_file_object:\n",
    "        writer = csv.DictWriter(csv_file_object, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    with open(filename, 'r', newline='', encoding='utf-8') as file_object:\n",
    "        dict_object = csv.DictReader(file_object)\n",
    "        array = []\n",
    "        for row in dict_object:\n",
    "            array.append(row)\n",
    "    return array\n",
    "\n",
    "class Query:\n",
    "    def __init__(self, **kwargs):\n",
    "        # attributes for all methods\n",
    "        try:\n",
    "            self.lang = kwargs['lang']\n",
    "        except:\n",
    "            self.lang = 'en' # default to English\n",
    "        try:\n",
    "            self.mediatype = kwargs['mediatype']\n",
    "        except:\n",
    "            self.mediatype = 'application/json' # default to JSON formatted query results\n",
    "        try:\n",
    "            self.endpoint = kwargs['endpoint']\n",
    "        except:\n",
    "            self.endpoint = 'https://query.wikidata.org/sparql' # default to Wikidata endpoint\n",
    "        try:\n",
    "            self.useragent = kwargs['useragent']\n",
    "        except:\n",
    "            self.useragent = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)' \n",
    "        self.requestheader = {\n",
    "        'Accept' : self.mediatype,\n",
    "        'User-Agent': self.useragent\n",
    "        }\n",
    "        try:\n",
    "            self.pid = kwargs['pid'] # property's P ID\n",
    "        except:\n",
    "            self.pid = 'P31' # default to \"instance of\"  \n",
    "        try:\n",
    "            self.sleep = kwargs['sleep']\n",
    "        except:\n",
    "            self.sleep = 0.25 # default throtting of 0.25 seconds\n",
    "            \n",
    "        # attributes for single property values method\n",
    "        try:\n",
    "            self.isitem = kwargs['isitem']\n",
    "        except:\n",
    "            self.isitem = True # default to values are items rather than literals   \n",
    "        try:\n",
    "            self.uselabel = kwargs['uselabel']\n",
    "        except:\n",
    "            self.uselabel = True # default is to show labels of items\n",
    "            \n",
    "        # attributes for labels and descriptions method\n",
    "        try:\n",
    "            self.labeltype = kwargs['labeltype']\n",
    "        except:\n",
    "            self.labeltype = 'label' # default to \"label\". Other options: \"description\", \"alias\"\n",
    "        try:\n",
    "            self.labelscreen = kwargs['labelscreen']\n",
    "        except:\n",
    "            self.labelscreen = '' # instead of using a list of subject items, add this line to screen for items\n",
    "            \n",
    "        # attributes for search_statement method\n",
    "        try:\n",
    "            self.vid = kwargs['vid'] # Q ID of the value of a statement. \n",
    "        except:\n",
    "            self.vid = '' # default to no value (the method returns the value of the statement)\n",
    "            \n",
    "    # send a generic query and return a list of Q IDs\n",
    "    def generic_query(self, query):\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['entity']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['entity']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['entity']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "            \n",
    "\n",
    "    # returns the value of a single property for an item by Q ID\n",
    "    def single_property_values_for_item(self, qid):\n",
    "        query = '''\n",
    "select distinct ?object where {\n",
    "    wd:'''+ qid + ''' wdt:''' + self.pid\n",
    "        if self.uselabel and self.isitem:\n",
    "            query += ''' ?objectItem.\n",
    "    ?objectItem rdfs:label ?object.\n",
    "    FILTER(lang(?object) = \"''' + self.lang +'\")'\n",
    "        else:\n",
    "            query += ''' ?object.'''            \n",
    "        query +=  '''\n",
    "    }'''\n",
    "        #print(query)\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        results_list = []\n",
    "        try:\n",
    "        #if 1==1: # replace try: to let errors occur, also comment out the except: clause\n",
    "            data = r.json()\n",
    "            #print(data)\n",
    "            statements = data['results']['bindings']\n",
    "            if len(statements) > 0: # if no results, the list remains empty\n",
    "                for statement in statements:\n",
    "                    if self.isitem:\n",
    "                        if self.uselabel:\n",
    "                            result_value = statement['object']['value']\n",
    "                        else:\n",
    "                            result_value = extract_qnumber(statement['object']['value'])\n",
    "                    else:\n",
    "                        result_value = statement['object']['value']\n",
    "                    results_list.append(result_value)\n",
    "        except:\n",
    "            results_list = [r.text]\n",
    "        \n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "    \n",
    "    # search for any of the \"label\" types: label, alias, description. qids is a list of Q IDs without namespaces\n",
    "    def labels_descriptions(self, qids):\n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "            alternatives = ''\n",
    "            for qid in qids:\n",
    "                alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        if self.labeltype == 'label':\n",
    "            predicate = 'rdfs:label'\n",
    "        elif self.labeltype == 'alias':\n",
    "            predicate = 'skos:altLabel'\n",
    "        elif self.labeltype == 'description':\n",
    "            predicate = 'schema:description'\n",
    "        else:\n",
    "            predicate = 'rdfs:label'        \n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?string where {'''\n",
    "        \n",
    "        # option to explicitly list subject Q IDs\n",
    "        if self.labelscreen == '':\n",
    "            query += '''\n",
    "      VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }'''\n",
    "        # option to screen for Q IDs by triple pattern\n",
    "        if self.labelscreen != '':\n",
    "            query += '''\n",
    "    ''' + self.labelscreen\n",
    "            \n",
    "        query += '''\n",
    "    ?id '''+ predicate + ''' ?string.\n",
    "    filter(lang(?string)=\"''' + self.lang + '''\")\n",
    "    }'''\n",
    "        #print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            string = result['string']['value']\n",
    "            results_list.append({'qid': qnumber, 'string': string})\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list\n",
    "\n",
    "    # Searches for statements using a particular property. If no value is set, the value will be returned.\n",
    "    def search_statement(self, qids, reference_property_list):\n",
    "        # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "        alternatives = ''\n",
    "        for qid in qids:\n",
    "            alternatives += 'wd:' + qid + '\\n'\n",
    "\n",
    "        # create a string for the query\n",
    "        query = '''\n",
    "select distinct ?id ?statement '''\n",
    "        # if no value was specified, find the value\n",
    "        if self.vid == '':\n",
    "            query += '?statementValue '\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '?reference '\n",
    "        for ref_prop_index in range(0, len(reference_property_list)):\n",
    "            query += '?refVal' + str(ref_prop_index) + ' '\n",
    "        query += '''\n",
    "    where {\n",
    "        VALUES ?id\n",
    "    {\n",
    "''' + alternatives + '''\n",
    "    }\n",
    "    ?id p:'''+ self.pid + ''' ?statement.\n",
    "    ?statement ps:'''+ self.pid\n",
    "\n",
    "        if self.vid == '': # return the value of the statement if no particular value is specified\n",
    "            query += ' ?statementValue.'\n",
    "        else:\n",
    "            query += ' wd:' + self.vid + '.' # specify the value to be searched for\n",
    "\n",
    "        if len(reference_property_list) != 0:\n",
    "            query += '''\n",
    "    optional {\n",
    "        ?statement prov:wasDerivedFrom ?reference.''' # search for references if there are any\n",
    "            for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                query +='''\n",
    "        ?reference pr:''' + reference_property_list[ref_prop_index] + ' ?refVal' + str(ref_prop_index) + '.'\n",
    "            query +='''\n",
    "            }'''\n",
    "        query +='''\n",
    "      }'''\n",
    "        print(query)\n",
    "\n",
    "        results_list = []\n",
    "        r = requests.get(self.endpoint, params={'query' : query}, headers=self.requestheader)\n",
    "        data = r.json()\n",
    "        results = data['results']['bindings']\n",
    "        # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "        # This will result in several results with the same subject qNumber\n",
    "        for result in results:\n",
    "            # remove wd: 'http://www.wikidata.org/entity/'\n",
    "            qnumber = extract_qnumber(result['id']['value'])\n",
    "            # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "            no_domain = extract_from_iri(result['statement']['value'], 5)\n",
    "            # need to remove the qNumber that's appended in front of the UUID\n",
    "            pieces = no_domain.split('-')\n",
    "            last_pieces = pieces[1:len(pieces)]\n",
    "            s = \"-\"\n",
    "            statement_uuid = s.join(last_pieces)\n",
    "\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                statement_value = result['statementValue']['value']\n",
    "            # extract the reference property data if any reference properties were specified\n",
    "            if len(reference_property_list) != 0:\n",
    "                if 'reference' in result:\n",
    "                    # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                    reference_hash = extract_qnumber(result['reference']['value'])\n",
    "                else:\n",
    "                    reference_hash = ''\n",
    "                reference_values = []\n",
    "                for ref_prop_index in range(0, len(reference_property_list)):\n",
    "                    if 'refVal' + str(ref_prop_index) in result:\n",
    "                        reference_value = result['refVal' + str(ref_prop_index)]['value']\n",
    "                        # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                        #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                        #    referenceValue = referenceValue.split('T')[0]\n",
    "                    else:\n",
    "                        reference_value = ''\n",
    "                    reference_values.append(reference_value)\n",
    "            results_dict = {'qId': qnumber, 'statementUuid': statement_uuid}\n",
    "            # if no value was specified, get the value that was found in the search\n",
    "            if self.vid == '':\n",
    "                results_dict['statementValue'] = statement_value\n",
    "            if len(reference_property_list) != 0:\n",
    "                results_dict['referenceHash'] = reference_hash\n",
    "                results_dict['referenceValues'] = reference_values\n",
    "            results_list.append(results_dict)\n",
    "\n",
    "        # delay by some amount (quarter second default) to avoid hitting the SPARQL endpoint too rapidly\n",
    "        sleep(self.sleep)\n",
    "        return results_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "person = 'Q40670042'\n",
    "orcid = '0000-0003-4365-3135'\n",
    "qids = ['Q40670042', 'Q57082956', 'Q75060085']\n",
    "reference_properties = ['P854', 'P813']\n",
    "\n",
    "#get_orcid = Query(pid='P496', isitem=False)\n",
    "#print(get_orcid.single_property_values_for_item(person) )\n",
    "#print()\n",
    "\n",
    "#get_class = Query(pid='P31', uselabel=False)\n",
    "#print(get_class.single_property_values_for_item(person) )\n",
    "#print()\n",
    "\n",
    "#get_employer_label = Query(pid='P108')\n",
    "#print(get_employer_label.single_property_values_for_item(person) )\n",
    "#print()\n",
    "\n",
    "#get_labels = Query(labeltype='label')\n",
    "#print(get_labels.labels_descriptions(qids))\n",
    "#print()\n",
    "#get_descriptions = Query(labeltype='description')\n",
    "#print(get_descriptions.labels_descriptions(qids))\n",
    "#print()\n",
    "#get_aliases = Query(labeltype='alias')\n",
    "#print(get_aliases.labels_descriptions(qids))\n",
    "get_employer = Query(pid='P108')\n",
    "print(get_employer.search_statement(qids, reference_properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine CETAF classes\n",
    "\n",
    "Determine what classes are being used by CETAF institutions, then query to generate a list of items that are members of those classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"qid\": \"Q31855\",\n",
      "    \"string\": \"research institute\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q3918\",\n",
      "    \"string\": \"university\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q33506\",\n",
      "    \"string\": \"museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q43229\",\n",
      "    \"string\": \"organization\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q167346\",\n",
      "    \"string\": \"botanical garden\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q414147\",\n",
      "    \"string\": \"academy of sciences\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q875538\",\n",
      "    \"string\": \"public university\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q866133\",\n",
      "    \"string\": \"university museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q588140\",\n",
      "    \"string\": \"science museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q570116\",\n",
      "    \"string\": \"tourist attraction\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q327333\",\n",
      "    \"string\": \"government agency\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q181916\",\n",
      "    \"string\": \"herbarium\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q1497375\",\n",
      "    \"string\": \"ensemble\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q1664720\",\n",
      "    \"string\": \"institute\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q1497649\",\n",
      "    \"string\": \"memory institution\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q1970365\",\n",
      "    \"string\": \"natural history museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q1899015\",\n",
      "    \"string\": \"conservation organization\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q1966910\",\n",
      "    \"string\": \"national academy\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q2065736\",\n",
      "    \"string\": \"cultural property\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q2385804\",\n",
      "    \"string\": \"educational institution\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q2668072\",\n",
      "    \"string\": \"collection\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q2288140\",\n",
      "    \"string\": \"Federal Scientific Institute\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q3343298\",\n",
      "    \"string\": \"non-departmental public body\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q5341295\",\n",
      "    \"string\": \"educational organization\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q5193377\",\n",
      "    \"string\": \"cultural institution\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q7315155\",\n",
      "    \"string\": \"research center\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q13226383\",\n",
      "    \"string\": \"facility\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q17431399\",\n",
      "    \"string\": \"national museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q16735822\",\n",
      "    \"string\": \"history museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q18233199\",\n",
      "    \"string\": \"country museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q24699794\",\n",
      "    \"string\": \"museum building\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q26959050\",\n",
      "    \"string\": \"botanical museum\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q45400320\",\n",
      "    \"string\": \"open-access publisher\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q68295960\",\n",
      "    \"string\": \"Swedish government agency\"\n",
      "  },\n",
      "  {\n",
      "    \"qid\": \"Q62091513\",\n",
      "    \"string\": \"faculty of science\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Find out all of the classes of which CETAF institutions are instances\n",
    "query = '''\n",
    "select distinct ?entity where {\n",
    "    ?institution wdt:P463 wd:Q5163385.\n",
    "    ?institution wdt:P31 ?entity.\n",
    "}'''\n",
    "gen_query_qids = Query(uselabel=False)\n",
    "cetaf_classlist = gen_query_qids.generic_query(query)\n",
    "\n",
    "cetaf_labels = Query(labeltype='label')\n",
    "cetaf_class_dict = cetaf_labels.labels_descriptions(cetaf_classlist)\n",
    "print(json.dumps(cetaf_class_dict, indent = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q31855   research institute\n",
      "7589\n",
      "\n",
      "Q3918   university\n",
      "12659\n",
      "\n",
      "Q33506   museum\n",
      "16538\n",
      "\n",
      "Q43229   organization\n",
      "47557\n",
      "\n",
      "Q167346   botanical garden\n",
      "1959\n",
      "\n",
      "Q414147   academy of sciences\n",
      "227\n",
      "\n",
      "Q875538   public university\n",
      "835\n",
      "\n",
      "Q866133   university museum\n",
      "92\n",
      "\n",
      "Q588140   science museum\n",
      "247\n",
      "\n",
      "Q570116   tourist attraction\n",
      "1570\n",
      "\n",
      "Q327333   government agency\n",
      "10663\n",
      "\n",
      "Q181916   herbarium\n",
      "284\n",
      "\n",
      "Q1497375   ensemble\n",
      "2432\n",
      "\n",
      "Q1664720   institute\n",
      "1279\n",
      "\n",
      "Q1497649   memory institution\n",
      "769\n",
      "\n",
      "Q1970365   natural history museum\n",
      "235\n",
      "\n",
      "Q1899015   conservation organization\n",
      "80\n",
      "\n",
      "Q1966910   national academy\n",
      "97\n",
      "\n",
      "Q2065736   cultural property\n",
      "8478\n",
      "\n",
      "Q2385804   educational institution\n",
      "5298\n",
      "\n",
      "Q2668072   collection\n",
      "285701\n",
      "\n",
      "Q2288140   Federal Scientific Institute\n",
      "13\n",
      "\n",
      "Q3343298   non-departmental public body\n",
      "103\n",
      "\n",
      "Q5341295   educational organization\n",
      "129\n",
      "\n",
      "Q5193377   cultural institution\n",
      "1220\n",
      "\n",
      "Q7315155   research center\n",
      "797\n",
      "\n",
      "Q13226383   facility\n",
      "5017\n",
      "\n",
      "Q17431399   national museum\n",
      "544\n",
      "\n",
      "Q16735822   history museum\n",
      "320\n",
      "\n",
      "Q18233199   country museum\n",
      "7\n",
      "\n",
      "Q24699794   museum building\n",
      "531\n",
      "\n",
      "Q26959050   botanical museum\n",
      "7\n",
      "\n",
      "Q45400320   open-access publisher\n",
      "6117\n",
      "\n",
      "Q68295960   Swedish government agency\n",
      "197\n",
      "\n",
      "Q62091513   faculty of science\n",
      "25\n",
      "\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# send a query for each class to retrieve items that are members\n",
    "test_results = []\n",
    "for wikidata_class in cetaf_class_dict:\n",
    "    print(wikidata_class['qid'], ' ', wikidata_class['string'])\n",
    "    \n",
    "    graph_pattern = '''\n",
    "    ?id wdt:P31 wd:''' + wikidata_class['qid'] + '.'\n",
    "    get_labels = Query(labeltype='label', labelscreen=graph_pattern)\n",
    "    \n",
    "    # Get the labels for all of the hits\n",
    "    test_labels = get_labels.labels_descriptions([])\n",
    "    print(len(test_labels))\n",
    "    print()\n",
    "    test_results.append({'class_id': wikidata_class['qid'], 'class_name': wikidata_class['string'], 'labels': test_labels})\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_string = json.dumps(test_results, indent = 2)\n",
    "with open('test-output.json', 'wt', encoding='utf-8') as fileObject:\n",
    "    fileObject.write(json_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve data about herbaria from Index Herbariorum\n",
    "\n",
    "Create a table with the useful information about each herbariumlisted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# retrieve JSON data from the API\n",
    "acceptMediaType = 'application/json'\n",
    "endpointUrl = 'http://sweetgum.nybg.org/science/api/v1/institutions'\n",
    "r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "dataFull = r.json()\n",
    "data = dataFull['data']\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"irn\": 124564,\n",
      "    \"organization\": \"Forest Service, USDA\",\n",
      "    \"code\": \"POFS\",\n",
      "    \"division\": \"Division of Range Management\",\n",
      "    \"department\": \"\",\n",
      "    \"specimenTotal\": 0,\n",
      "    \"currentStatus\": \"Inactive\",\n",
      "    \"dateFounded\": \"\",\n",
      "    \"taxonomicCoverage\": \"\",\n",
      "    \"geography\": \"\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"\",\n",
      "      \"physicalCity\": \"Portland\",\n",
      "      \"physicalState\": \"Oregon\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"U.S.A.\",\n",
      "      \"postalStreet\": \"\",\n",
      "      \"postalCity\": \"Portland\",\n",
      "      \"postalState\": \"Oregon\",\n",
      "      \"postalZipCode\": \"97208\",\n",
      "      \"postalCountry\": \"U.S.A.\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"\",\n",
      "      \"email\": \"\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 0,\n",
      "      \"lon\": 0\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [],\n",
      "    \"notes\": \"Updated Mar 2007. POFS (2000 specimens) transferred to LAGO in late 1970s. LAGO dispersed in 1990s. See LAGO.\",\n",
      "    \"dateModified\": \"2019-05-08\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 125776,\n",
      "    \"organization\": \"Research Institute of Plant Protection\",\n",
      "    \"code\": \"TBIP\",\n",
      "    \"division\": \"\",\n",
      "    \"department\": \"Agriculture and Life Sciences\",\n",
      "    \"specimenTotal\": 0,\n",
      "    \"currentStatus\": \"Permanently closed\",\n",
      "    \"dateFounded\": \"1931\",\n",
      "    \"taxonomicCoverage\": \"Microfungi, mainly parasitic\",\n",
      "    \"geography\": \"Georgia\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"\",\n",
      "      \"physicalCity\": \"Tbilisi\",\n",
      "      \"physicalState\": \"\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"Georgia\",\n",
      "      \"postalStreet\": \"\",\n",
      "      \"postalCity\": \"\",\n",
      "      \"postalState\": \"\",\n",
      "      \"postalZipCode\": \"\",\n",
      "      \"postalCountry\": \"\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[995] 032 2 200901\",\n",
      "      \"email\": \"\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 0,\n",
      "      \"lon\": 0\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [\n",
      "      \"Horti Botanici Tiflisiensis.\"\n",
      "    ],\n",
      "    \"importantCollectors\": [],\n",
      "    \"notes\": \"As of May 2018, TBIP is permanently closed.  All collections have moved to AUMH.\",\n",
      "    \"dateModified\": \"2018-05-23\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 257635,\n",
      "    \"organization\": \"Universidad Nacional Pedro Henr\\u00edquez Ure\\u00f1a\",\n",
      "    \"code\": \"HUNPHU\",\n",
      "    \"division\": \"Facultad de Ciencias y Tecnolog\\u00eda\",\n",
      "    \"department\": \"Herbario\",\n",
      "    \"specimenTotal\": 13366,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1970\",\n",
      "    \"taxonomicCoverage\": \"Seed plants, Pteridophytes  and some Bryophytes\",\n",
      "    \"geography\": \"Dominican Republic\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"Av. John F. Kennedy, Km 7 1/2\",\n",
      "      \"physicalCity\": \"Santo Domingo\",\n",
      "      \"physicalState\": \"National District\",\n",
      "      \"physicalZipCode\": \"10601\",\n",
      "      \"physicalCountry\": \"Dominican Repubic\",\n",
      "      \"postalStreet\": \"Av.John F. Kennedy Km 7 \\u00bd.\",\n",
      "      \"postalCity\": \"Santo Domingo\",\n",
      "      \"postalState\": \"National District\",\n",
      "      \"postalZipCode\": \"10601\",\n",
      "      \"postalCountry\": \"Dominican Repubic\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"Universidad Nacional Pedro Henr\\u00edquez Ure\\u00f1a\",\n",
      "      \"email\": \"ccruz@unphu.edu.do\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 18.735693,\n",
      "      \"lon\": -70.1626511\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 300,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 426,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 12940,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [\n",
      "      \"L. A. Julia\",\n",
      "      \"D. Dod\",\n",
      "      \"J. J. Jim\\u00e9nez\",\n",
      "      \"H.A. Liogier\",\n",
      "      \"E. J. Marcano\"\n",
      "    ],\n",
      "    \"notes\": \"Entered December 2019. The UNPHU Herbarium has the oldest plant collection of Dr. Henri Alain Liogier in the Dominican Republic. We have approximately 12,000 specimens among which there are 8,615 collected by Liogier and 19 Types.\\nOur herbarium is emerging as a research center with interests in Ethnobotany, Ethnopharmacology, Conservation, Traditional Environmental Knowledge, among other topics related to the Flora of our country.\",\n",
      "    \"dateModified\": \"2020-01-27\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 126115,\n",
      "    \"organization\": \"Tanta University\",\n",
      "    \"code\": \"TANE\",\n",
      "    \"division\": \"Faculty of Science\",\n",
      "    \"department\": \"Botany Department\",\n",
      "    \"specimenTotal\": 20000,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1969\",\n",
      "    \"taxonomicCoverage\": \"\",\n",
      "    \"geography\": \"Egypt, especially Nile Delta region.\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"Al-Gaesh Street.\",\n",
      "      \"physicalCity\": \"Tanta\",\n",
      "      \"physicalState\": \"\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"Egypt\",\n",
      "      \"postalStreet\": \"\",\n",
      "      \"postalCity\": \"Tanta\",\n",
      "      \"postalState\": \"Gharbyya\",\n",
      "      \"postalZipCode\": \"31527\",\n",
      "      \"postalCountry\": \"Egypt\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[20] 40 3344352 ext. 302\",\n",
      "      \"email\": \"\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 30.791109,\n",
      "      \"lon\": 30.99806\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [\n",
      "      \"K.H. Shaltout\",\n",
      "      \"H.F. El Kady\",\n",
      "      \"A.A. Sharaf El-Din\",\n",
      "      \"M.A. El-Beheiry\",\n",
      "      \"Y.A. Al-Sodany\",\n",
      "      \"M.A. El-Sheikh\",\n",
      "      \"M.T. Mousa\",\n",
      "      \"R.A. El-Fahar\",\n",
      "      \"Th.M. El-Komi\",\n",
      "      \"D.A. Ahmed\",\n",
      "      \"S.A. El-Masry\",\n",
      "      \"M.Z. Hatim\",\n",
      "      \"E.E. Ammar\"\n",
      "    ],\n",
      "    \"notes\": \"Botanical library is available to all researchers. Library includes books of different floras and monographs. In addition to different botanical MSC and PHD thesis. Collections of archives and manuscripts, seed catalogs and photographs are available.\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 126332,\n",
      "    \"organization\": \"Universidade do Rio Grande (FURG)\",\n",
      "    \"code\": \"HURG\",\n",
      "    \"division\": \"\",\n",
      "    \"department\": \"Instituto de Ci\\u00eancias Biol\\u00f3gicas\",\n",
      "    \"specimenTotal\": 6536,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1982\",\n",
      "    \"taxonomicCoverage\": \"Especially aquatic, medicinal plants, and fungi\",\n",
      "    \"geography\": \"Southern Rio Grande do Sul\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"Avenida It\\u00e1lia, Km 8, Campus Carreiros.\",\n",
      "      \"physicalCity\": \"Rio Grande\",\n",
      "      \"physicalState\": \"Rio Grande do Sul\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"Brazil\",\n",
      "      \"postalStreet\": \"Caixa Postal 474\",\n",
      "      \"postalCity\": \"Rio Grande\",\n",
      "      \"postalState\": \"Rio Grande do Sul\",\n",
      "      \"postalZipCode\": \"96201-900\",\n",
      "      \"postalCountry\": \"Brazil\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[55] 3293 5169\\n[55] 3233 6633\",\n",
      "      \"email\": \"soniahefler@furg.br\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": -32.044605,\n",
      "      \"lon\": -52.078587\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [\n",
      "      \"C. Cordazzo\",\n",
      "      \"P. Costa\",\n",
      "      \"B. Irgang\",\n",
      "      \"G. Pedralli\",\n",
      "      \"F. Pinheiro\",\n",
      "      \"V. Susin\"\n",
      "    ],\n",
      "    \"notes\": \"\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 125120,\n",
      "    \"organization\": \"La Sierra University\",\n",
      "    \"code\": \"LOMA\",\n",
      "    \"division\": \"\",\n",
      "    \"department\": \"Biology Department\",\n",
      "    \"specimenTotal\": 3000,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1964\",\n",
      "    \"taxonomicCoverage\": \"\",\n",
      "    \"geography\": \"Local, especially Santa Ana Mountains.\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"\",\n",
      "      \"physicalCity\": \"Riverside\",\n",
      "      \"physicalState\": \"California\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"U.S.A.\",\n",
      "      \"postalStreet\": \"\",\n",
      "      \"postalCity\": \"Riverside\",\n",
      "      \"postalState\": \"California\",\n",
      "      \"postalZipCode\": \"92515-8247\",\n",
      "      \"postalCountry\": \"U.S.A.\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[1] 909 785 2105\",\n",
      "      \"email\": \"bmartin@lasierra.edu\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 33.925591,\n",
      "      \"lon\": -117.453743\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [],\n",
      "    \"notes\": \"LOMA moved from Loma Linda campus to Riverside campus in 1986. Riverside campus became La Sierra University in 1991.\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 246354,\n",
      "    \"organization\": \"Finger Lakes Community College\",\n",
      "    \"code\": \"FLH\",\n",
      "    \"division\": \"\",\n",
      "    \"department\": \"\",\n",
      "    \"specimenTotal\": 17000,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1976\",\n",
      "    \"taxonomicCoverage\": \"Vascular plants, bryophytes and lichens; aquatic and terrestrial invasive species\",\n",
      "    \"geography\": \"Finger Lakes region; northern New York\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"3325 Marvin Sands Dr.\",\n",
      "      \"physicalCity\": \"Canandaigua\",\n",
      "      \"physicalState\": \"New York\",\n",
      "      \"physicalZipCode\": \"14424\",\n",
      "      \"physicalCountry\": \"U.S.A.\",\n",
      "      \"postalStreet\": \"3325 Marvin Sands Dr.\",\n",
      "      \"postalCity\": \"Canandaigua\",\n",
      "      \"postalState\": \"New York\",\n",
      "      \"postalZipCode\": \"14424\",\n",
      "      \"postalCountry\": \"U.S.A.\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[1] 585 785 1255\",\n",
      "      \"email\": \"Bruce.Gilman@flcc.edu\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 42.867179,\n",
      "      \"lon\": -77.241067\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [],\n",
      "    \"notes\": \"\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 123883,\n",
      "    \"organization\": \"Azerbaijan National Academy of Sciences (ANAS)\",\n",
      "    \"code\": \"BAK\",\n",
      "    \"division\": \"Institute of Botany\",\n",
      "    \"department\": \"\",\n",
      "    \"specimenTotal\": 600000,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1827\",\n",
      "    \"taxonomicCoverage\": \"\",\n",
      "    \"geography\": \"Azerbaijan; Caucasus; Iran; Turkey; Palestine; Iraq; western Europe; northern Africa; North America; South America.\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"\",\n",
      "      \"physicalCity\": \"Baku\",\n",
      "      \"physicalState\": \"\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"Azerbaijan\",\n",
      "      \"postalStreet\": \"Badamdar highway 40\",\n",
      "      \"postalCity\": \"Baku\",\n",
      "      \"postalState\": \"\",\n",
      "      \"postalZipCode\": \"AZ1004\",\n",
      "      \"postalCountry\": \"Azerbaijan\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[994] 12 4970994\",\n",
      "      \"email\": \"p.garakhani@mail.ru\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 40.37767,\n",
      "      \"lon\": 49.89201\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [],\n",
      "    \"notes\": \"Upated April 2017 (Institution name, specimen total, contact information updated).\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 126881,\n",
      "    \"organization\": \"University of Victoria\",\n",
      "    \"code\": \"UVIC\",\n",
      "    \"division\": \"\",\n",
      "    \"department\": \"Biology Department\",\n",
      "    \"specimenTotal\": 48000,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1962\",\n",
      "    \"taxonomicCoverage\": \"Western North American <Aster> and <Erythronium>\",\n",
      "    \"geography\": \"British Columbia and adjacent regions\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"\",\n",
      "      \"physicalCity\": \"Victoria\",\n",
      "      \"physicalState\": \"British Columbia\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"Canada\",\n",
      "      \"postalStreet\": \"P.O. Box 3020 STN CSC\",\n",
      "      \"postalCity\": \"Victoria\",\n",
      "      \"postalState\": \"British Columbia\",\n",
      "      \"postalZipCode\": \"V8W 3N5\",\n",
      "      \"postalCountry\": \"Canada\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[1] 250 721 7097\",\n",
      "      \"email\": \"\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 48.4634067,\n",
      "      \"lon\": -123.3116935\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [\n",
      "      \"SCCBC (8000 specimens) in 1994\",\n",
      "      \"NLSN (5000 specimens) in 1994\"\n",
      "    ],\n",
      "    \"importantCollectors\": [],\n",
      "    \"notes\": \"Updated  Aug 2016 (geocoordinates corrected).   LEA mistletoes, which were on permanent loan to UVIC from 1994-2009, have now been transferred to UC.\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  },\n",
      "  {\n",
      "    \"irn\": 174257,\n",
      "    \"organization\": \"Jasper Ridge Biological Preserve, Stanford University\",\n",
      "    \"code\": \"JROH\",\n",
      "    \"division\": \"\",\n",
      "    \"department\": \"\",\n",
      "    \"specimenTotal\": 5400,\n",
      "    \"currentStatus\": \"Active\",\n",
      "    \"dateFounded\": \"1996\",\n",
      "    \"taxonomicCoverage\": \"\",\n",
      "    \"geography\": \"The 1189 acres now comprising the Jasper Ridge Biological Preserve of Stanford University, San Mateo County, California USA\",\n",
      "    \"address\": {\n",
      "      \"physicalStreet\": \"\",\n",
      "      \"physicalCity\": \"Woodside\",\n",
      "      \"physicalState\": \"California\",\n",
      "      \"physicalZipCode\": \"\",\n",
      "      \"physicalCountry\": \"U.S.A.\",\n",
      "      \"postalStreet\": \"4001 Sand Hill Rd.\",\n",
      "      \"postalCity\": \"Woodside\",\n",
      "      \"postalState\": \"California\",\n",
      "      \"postalZipCode\": \"94062\",\n",
      "      \"postalCountry\": \"U.S.A.\"\n",
      "    },\n",
      "    \"contact\": {\n",
      "      \"phone\": \"[1] 650 224 3778\\n[1] 650 851 6813 (admin. assist.)\",\n",
      "      \"email\": \"nonajrbp@stanford.edu\\nalambrec@stanford.edu\",\n",
      "      \"webUrl\": \"\"\n",
      "    },\n",
      "    \"location\": {\n",
      "      \"lat\": 37.403439,\n",
      "      \"lon\": -122.244953\n",
      "    },\n",
      "    \"collectionsSummary\": {\n",
      "      \"numAlgae\": 0,\n",
      "      \"numAlgaeDatabased\": 0,\n",
      "      \"numAlgaeImaged\": 0,\n",
      "      \"numBryos\": 0,\n",
      "      \"numBryosDatabased\": 0,\n",
      "      \"numBryosImaged\": 0,\n",
      "      \"numFungi\": 0,\n",
      "      \"numFungiDatabased\": 0,\n",
      "      \"numFungiImaged\": 0,\n",
      "      \"numPteridos\": 0,\n",
      "      \"numPteridosDatabased\": 0,\n",
      "      \"numPteridosImaged\": 0,\n",
      "      \"numSeedPl\": 0,\n",
      "      \"numSeedPlDatabased\": 0,\n",
      "      \"numSeedPlImaged\": 0\n",
      "    },\n",
      "    \"incorporatedHerbaria\": [],\n",
      "    \"importantCollectors\": [\n",
      "      \"L. Abrams\",\n",
      "      \"E. I. Applegate\",\n",
      "      \"D. E. Breedlove\",\n",
      "      \"D. D. Davis\",\n",
      "      \"W. R. Dudley\",\n",
      "      \"A. Elmer\",\n",
      "      \"R. Ferris\",\n",
      "      \"D. D. Keck\",\n",
      "      \"H. Mason\",\n",
      "      \"V. Rattan\",\n",
      "      \"L. S. Rose\",\n",
      "      \"J. H. Thomas\",\n",
      "      \"A. G. Vestal\",\n",
      "      \"C. B. Wolf\"\n",
      "    ],\n",
      "    \"notes\": \"Updated  Feb 2013 (staff additions).  3,879 specimens (approximately 70% of the JROH herbarium) are from the Dudley Herbarium of Stanford University (DS), the majority of which is housed in the California Academy of Sciences (CAS).\\n\\nNew accessions primarily fill gaps in the herbarium's representation of the Jasper Ridge vascular plant flora.\",\n",
      "    \"dateModified\": \"2018-01-12\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(data[0:10], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# pull out the data from the JSON and put in list of lists (table) in a file\n",
    "output_table = []\n",
    "fieldnames = ['wikidata_id', 'orgid', 'organization', 'code', 'division', 'department', 'email']\n",
    "\n",
    "for herb in data:\n",
    "    results_dict = {}\n",
    "    results_dict['wikidata_id'] = ''\n",
    "    results_dict['orgid'] = herb['irn']\n",
    "    results_dict['organization'] = herb['organization']\n",
    "    results_dict['code'] = herb['code']\n",
    "    results_dict['division'] = herb['division']\n",
    "    results_dict['department'] = herb['department']\n",
    "    results_dict['email'] = herb['contact']['email']\n",
    "    output_table.append(results_dict)\n",
    "\n",
    "filename = 'herb-basic.csv'\n",
    "writeDictsToCsv(output_table, filename, fieldnames)\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match candidate items with IH collections\n",
    "\n",
    "For each of the retrieved items, do fuzzy matching with IH collection names and see which is most effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'herb-basic.csv'\n",
    "output_table = readDict(filename)\n",
    "\n",
    "filename = 'test-output.json'\n",
    "json_string = readDict(filename)\n",
    "test_results = json.loads(json_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_results = []\n",
    "for result in test_results[0:1]:\n",
    "    herbarium_matches = []\n",
    "    print('Class:', result['class_name'])\n",
    "    for item in result['labels']:\n",
    "        for row in output_table:\n",
    "            nameTestRatio = fuzz.token_set_ratio(item['string'], row['organization'])\n",
    "            if nameTestRatio >= 50:\n",
    "                herbarium_matches.append({'qid': item['qid'], 'orgid': row['orgid'], 'name': row['organization']})\n",
    "    match_results.append({'class': result['class'], 'matches': herbarium_matches})\n",
    "    print('Matches: ', len(herbarium_matches))\n",
    "    print('Fraction:' , len(herbarium_matches)/len(result['labels']))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByIhCode(herb_code):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P5858 \"''' + herb_code + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# Look up the institution using the IH code\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "people = readDict(filename)\n",
    "\n",
    "#for personIndex in range(0, len(people)):\n",
    "for personIndex in range(1, 100): \n",
    "    ihCodes = searchWikidataForQIdByIhCode(people[personIndex]['herb_code'])\n",
    "    print(people[personIndex]['herb_code'], ihCodes)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests   # best library to manage HTTP transactions\n",
    "#from bs4 import BeautifulSoup # web-scraping library\n",
    "import json\n",
    "from time import sleep\n",
    "import csv\n",
    "import math\n",
    "from fuzzywuzzy import fuzz # fuzzy logic matching\n",
    "from fuzzywuzzy import process\n",
    "import xml.etree.ElementTree as et # library to traverse XML tree\n",
    "import urllib\n",
    "import datetime\n",
    "\n",
    "# For a particular processing round, set a short name for the department here.\n",
    "# This name is used to generate a set of unique processing files for that department.\n",
    "testEmployer = 'Vanderbilt University' # to test against Wikidata employer property\n",
    "employerQId = 'Q29052' # Vanderbilt University\n",
    "deathDateLimit = '2010' # any date deaths before this date will be assumed to not be a match\n",
    "\n",
    "# NOTE: eventually need to test against all affiliations in cases of faculty with multiple appointments\n",
    "\n",
    "# ***********************************\n",
    "# NOTE: the script fails if there is a current item in Wikidata that has the same values for both label and description. \n",
    "# A check needs to be run for this !!!\n",
    "# ***********************************\n",
    "\n",
    "# Here is some example JSON from a departmental configuration file (department-configuration.json):\n",
    "\n",
    "'''\n",
    "{\n",
    "  \"deptShortName\": \"anthropology\",\n",
    "  \"aads\": {\n",
    "    \"categories\": [\n",
    "      \"\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/aads/people/\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"African American and Diaspora Studies\",\n",
    "    \"departmentQId\": \"Q79117444\",\n",
    "    \"testAuthorAffiliation\": \"African American Diaspora Studies Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"African American and Diaspora Studies scholar\"\n",
    "    }\n",
    "  },\n",
    "  \"bsci\": {\n",
    "    \"categories\": [\n",
    "      \"primary-training-faculty\",\n",
    "      \"research-and-teaching-faculty\",\n",
    "      \"secondary-faculty\",\n",
    "      \"postdoc-fellows\",\n",
    "      \"emeriti\"\n",
    "    ],\n",
    "    \"baseUrl\": \"https://as.vanderbilt.edu/biosci/people/index.php?group=\",\n",
    "    \"nTables\": 1,\n",
    "    \"departmentSearchString\": \"Biological Sciences\",\n",
    "    \"departmentQId\": \"Q78041310\",\n",
    "    \"testAuthorAffiliation\": \"Biological Sciences Vanderbilt\",\n",
    "    \"labels\": {\n",
    "      \"source\": \"column\",\n",
    "      \"value\": \"name\"\n",
    "    },\n",
    "    \"descriptions\": {\n",
    "      \"source\": \"constant\",\n",
    "      \"value\": \"biology researcher\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "'''\n",
    "# Note that the first key: value pair sets the department to be processed.\n",
    "\n",
    "# The default labels and descriptions can either be a column in the table or set as a constant. \n",
    "# If it's a column, the value is the column header.  If it's a constant, the value is the string to assign as the value.\n",
    "\n",
    "# The nTables value is the number of HTML tables in the page to be searched.  Currently (2020-01-19) it isn't used\n",
    "# and the script just checks all of the tables, but it could be implemented if there are tables at the end that don't \n",
    "# include employee names.\n",
    "\n",
    "with open('department-configuration.json', 'rt', encoding='utf-8') as fileObject:\n",
    "    text = fileObject.read()\n",
    "deptSettings = json.loads(text)\n",
    "deptShortName = deptSettings['deptShortName']\n",
    "print('Department currently set for', deptShortName)\n",
    "\n",
    "wikidataEndpointUrl = 'https://query.wikidata.org/sparql'\n",
    "degreeList = [\n",
    "    {'string': 'Ph.D.', 'value': 'Ph.D.'},\n",
    "    {'string': 'PhD', 'value': 'Ph.D.'},\n",
    "    {'string': 'D.Phil.', 'value': 'D.Phil.'},\n",
    "    {'string': 'J.D.', 'value': 'J.D.'}\n",
    "     ]\n",
    "\n",
    "# NCBI identification requirements:\n",
    "# tool name and email address should be sent with all requests\n",
    "# see https://www.ncbi.nlm.nih.gov/books/NBK25499/#chapter4.ESearch\n",
    "emailAddress = 'steve.baskauf@vanderbilt.edu' # put your email address here\n",
    "toolName = 'VanderBot' # give your application a name here\n",
    "\n",
    "# generates a dictionary to be passed in a requests GET method to generate the request header\n",
    "def generateHeaderDictionary(acceptMediaType):\n",
    "    userAgentHeader = 'VanderBot/0.8 (https://github.com/HeardLibrary/linked-data/tree/master/publications; mailto:steve.baskauf@vanderbilt.edu)'\n",
    "    requestHeaderDictionary = {\n",
    "        'Accept' : acceptMediaType,\n",
    "        'User-Agent': userAgentHeader\n",
    "    }\n",
    "    return requestHeaderDictionary\n",
    "\n",
    "# write a list of lists to a CSV file\n",
    "def writeCsv(fileName, array):\n",
    "    fileObject = open(fileName, 'w', newline='', encoding='utf-8')\n",
    "    writerObject = csv.writer(fileObject)\n",
    "    for row in array:\n",
    "        writerObject.writerow(row)\n",
    "    fileObject.close()\n",
    "\n",
    "# write a list of dictionaries to a CSV file\n",
    "def writeDictsToCsv(table, filename, fieldnames):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvFileObject:\n",
    "        writer = csv.DictWriter(csvFileObject, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "        for row in table:\n",
    "            writer.writerow(row)\n",
    "\n",
    "# read from a CSV file into a list of dictionaries\n",
    "def readDict(filename):\n",
    "    fileObject = open(filename, 'r', newline='', encoding='utf-8')\n",
    "    dictObject = csv.DictReader(fileObject)\n",
    "    array = []\n",
    "    for row in dictObject:\n",
    "        array.append(row)\n",
    "    fileObject.close()\n",
    "    return array\n",
    "\n",
    "# extracts the qNumber from a Wikidata IRI\n",
    "def extractQNumber(iri):\n",
    "    # pattern is http://www.wikidata.org/entity/Q6386232\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[4]\n",
    "\n",
    "# extracts a local name from an IRI, specify the list item number for the last piece separated by slash\n",
    "def extractFromIri(iri, numberPieces):\n",
    "    # with pattern like http://www.wikidata.org/entity/Q6386232 there are 5 pieces with qId as number 4\n",
    "    pieces = iri.split('/')\n",
    "    return pieces[numberPieces]\n",
    "\n",
    "# see https://www.wikidata.org/wiki/Property:P21 for values\n",
    "def decodeSexOrGender(code):\n",
    "    code = code.lower()\n",
    "    if code == 'm':\n",
    "        qId = 'Q6581097'\n",
    "    elif code == 'f':\n",
    "        qId = 'Q6581072'\n",
    "    elif code == 'i':\n",
    "        qId = 'Q1097630'\n",
    "    elif code == 'tf':\n",
    "        qId = 'Q1052281'\n",
    "    elif code == 'tm':\n",
    "        qId = 'Q2449503'\n",
    "    else:\n",
    "        qId = ''\n",
    "    return qId\n",
    "\n",
    "# query for a single variable that's an item named 'item'\n",
    "# returns a list of results\n",
    "def searchWikidataForQIdByOrcid(orcid):\n",
    "    query = '''\n",
    "select distinct ?item where {\n",
    "  ?item wdt:P496 \"''' + employees[employeeIndex]['orcid'] + '''\".\n",
    "  }\n",
    "'''\n",
    "    results = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers = generateHeaderDictionary(acceptMediaType))\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append(qNumber)\n",
    "    except:\n",
    "        results = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint to rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a list of employer strings for the item with Wikidata ID qId; P108 is employer\n",
    "def searchWikidataEmployer(qId):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?employer where {\n",
    "        wd:'''+ qId + ''' wdt:P108 ?employerId.\n",
    "        ?employerId rdfs:label ?employer.\n",
    "        FILTER(lang(?employer) = 'en')\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                resultsList.append(statement['employer']['value'])\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "# returns a list of value Q IDs of the property propertyId for the item with Wikidata ID qId\n",
    "def searchWikidataSingleProperty(qId, propertyId, valueType):\n",
    "    resultsList = []\n",
    "    query = '''select distinct ?object where {\n",
    "        wd:'''+ qId + ''' wdt:''' + propertyId + ''' ?object.\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the list remains empty\n",
    "            for statement in statements:\n",
    "                if valueType == 'item':\n",
    "                    resultValue = extractQNumber(statement['object']['value'])\n",
    "                else:\n",
    "                    resultValue = statement['object']['value']\n",
    "                resultsList.append(resultValue)\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def checkOrcid(orcid):\n",
    "    namespace = 'https://orcid.org/'\n",
    "    endpointUrl = namespace + orcid\n",
    "    acceptMediaType = 'application/ld+json'\n",
    "    r = requests.get(endpointUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    code = r.status_code\n",
    "    #print(r.text)\n",
    "    data = r.json()\n",
    "    response = {'code': code, 'data': data}\n",
    "    if response['code'] != 200:\n",
    "        print('Attempt to dereference ORCID resulted in HTTP response code ', response['code'])\n",
    "        data['orcidReferenceValue'] = ''\n",
    "    else:\n",
    "        print('Successfully retrieved')\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "    # delay a quarter second to avoid hitting the API too rapidly\n",
    "    sleep(0.25)\n",
    "    return(wholeDateZ)\n",
    "\n",
    "# if the value passed is '' then the value will be retrieved.  Otherwise, the value is used to screen.\n",
    "def searchStatementAtWikidata(qIds, prop, value, refPropList):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?statement '\n",
    "    # if no value was specified, find the value\n",
    "    if value == '':\n",
    "        query += '?statementValue '\n",
    "    if len(refPropList) != 0:\n",
    "        query += '?reference '\n",
    "    for refPropIndex in range(0, len(refPropList)):\n",
    "        query += '?refVal' + str(refPropIndex) + ' '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id p:'''+ prop + ''' ?statement.\n",
    "  ?statement ps:'''+ prop\n",
    "    \n",
    "    if value == '':\n",
    "        query += ' ?statementValue.'\n",
    "    else:\n",
    "        query += ' wd:' + value + '.'\n",
    "\n",
    "    if len(refPropList) != 0:\n",
    "        query += '''\n",
    "  optional {\n",
    "    ?statement prov:wasDerivedFrom ?reference.'''\n",
    "        for refPropIndex in range(0, len(refPropList)):\n",
    "            query +='''\n",
    "    ?reference pr:''' + refPropList[refPropIndex] + ''' ?refVal''' + str(refPropIndex) + '''.'''\n",
    "        query +='''\n",
    "        }'''\n",
    "    query +='''\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    # ********** NOTE: need to deal with case where there are more than one reference per result\n",
    "    # This will result in several results with the same qNumeber, orcid, and referenceHash\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        # remove wds: 'http://www.wikidata.org/entity/statement/'\n",
    "        noDomain = extractFromIri(result['statement']['value'], 5)\n",
    "        # need to remove the qNumber that's appended in front of the UUID\n",
    "        \n",
    "        # NOTE: formerly used this:\n",
    "        #statementUuid = noDomain.partition(qNumber + '-')[2]\n",
    "        # However, there was at least one case where the appended qNumber had a lower case Q and failed to match.\n",
    "        # So needed a different approach.\n",
    "        pieces = noDomain.split('-')\n",
    "        lastPieces = pieces[1:len(pieces)]\n",
    "        s = \"-\"\n",
    "        statementUuid = s.join(lastPieces)\n",
    "\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            statementValue = result['statementValue']['value']\n",
    "        if len(refPropList) != 0:\n",
    "            if 'reference' in result:\n",
    "                # remove wdref: 'http://www.wikidata.org/reference/'\n",
    "                referenceHash = extractFromIri(result['reference']['value'], 4)\n",
    "            else:\n",
    "                referenceHash = ''\n",
    "            referenceValues = []\n",
    "            for refPropIndex in range(0, len(refPropList)):\n",
    "                if 'refVal' + str(refPropIndex) in result:\n",
    "                    refVal = result['refVal' + str(refPropIndex)]['value']\n",
    "                    # if it's a date, it comes down as 2019-12-05T00:00:00Z, but the API wants just the date: 2019-12-05\n",
    "                    #if referenceProperty == 'P813': # the likely property is \"retrieved\"; just leave it if it's another property\n",
    "                    #    referenceValue = referenceValue.split('T')[0]\n",
    "                else:\n",
    "                    refVal = ''\n",
    "                referenceValues.append(refVal)\n",
    "        resultsDict = {'qId': qNumber, 'statementUuid': statementUuid}\n",
    "        # if no value was specified, get the value that was found in the search\n",
    "        if value == '':\n",
    "            resultsDict['statementValue'] = statementValue\n",
    "        if len(refPropList) != 0:\n",
    "            resultsDict['referenceHash'] = referenceHash\n",
    "            resultsDict['referenceValues'] = referenceValues\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue\n",
    "\n",
    "# search for any of the \"label\" types: label, alias, description\n",
    "def searchLabelsDescriptionsAtWikidata(qIds, labelType, language):\n",
    "    # create a string for all of the Wikidata item IDs to be used as subjects in the query\n",
    "    alternatives = ''\n",
    "    for qId in qIds:\n",
    "        alternatives += 'wd:' + qId + '\\n'\n",
    "        \n",
    "    if labelType == 'label':\n",
    "        predicate = 'rdfs:label'\n",
    "    elif labelType == 'alias':\n",
    "        predicate = 'skos:altLabel'\n",
    "    elif labelType == 'description':\n",
    "        predicate = 'schema:description'\n",
    "    else:\n",
    "        predicate = 'rdfs:label'        \n",
    "        \n",
    "    # create a string for the query\n",
    "    query = 'select distinct ?id ?string '\n",
    "    query += '''where {\n",
    "  VALUES ?id\n",
    "{\n",
    "''' + alternatives + '''}\n",
    "  ?id '''+ predicate + ''' ?string.\n",
    "  filter(lang(?string)=\"''' + language + '''\")\n",
    "  }'''\n",
    "    #print(query)\n",
    "\n",
    "    returnValue = []\n",
    "    acceptMediaType = 'application/json'\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    data = r.json()\n",
    "    results = data['results']['bindings']\n",
    "    for result in results:\n",
    "        # remove wd: 'http://www.wikidata.org/entity/'\n",
    "        qNumber = extractFromIri(result['id']['value'], 4)\n",
    "        string = result['string']['value']\n",
    "        resultsDict = {'qId': qNumber, 'string': string}\n",
    "        returnValue.append(resultsDict)\n",
    "\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    \n",
    "    return returnValue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match employees to Wikidata\n",
    "\n",
    "Script developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/wikidata/match_bsci_wikidata.ipynb\n",
    "\n",
    "Attempts to match records of people Wikidata knows to work at Vanderbilt with departmental people by matching their ORCIDs, then name strings. If there isn't a match with the downloaded Wikidata records, for employees with ORCIDs, the script attempts to find them in Wikidata by directly doing a SPARQL search for their ORCID.\n",
    "\n",
    "As people are matched (or determined to not have a match), a code is recorded with information about how the match was made.  Here are the values:\n",
    "\n",
    "```\n",
    "0=unmatched\n",
    "1=matched with ORCID in both sources\n",
    "2=ORCID from match to ORCID records but name match to Wikidata (no ORCID)\n",
    "3=no ORCID from match to ORCID records but name match to Wikidata (with ORCID); could happen if affiliation isn't matched in ORCID\n",
    "4=no ORCID from match to ORCID records but name match to Wikidata (no ORCID)\n",
    "5=ORCID from match to ORCID records and found via SPARQL ORCID search (likely non-VU affiliated in Wikidata)\n",
    "6=ORCID from match to ORCID records and found via SPARQL name search (non-VU affiliated without ORCID)\n",
    "7=no name match\n",
    "8=ORCID from match to ORCID records, error in SPARQL ORCID search\n",
    "9=no ORCID from match to ORCID records, error in SPARQL name search\n",
    "10=affiliation match in article\n",
    "11=match by human choice after looking at entity data\n",
    "12=no matching entities were possible matches\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crosscheck people against publications\n",
    "\n",
    "Developed at https://github.com/HeardLibrary/linked-data/blob/master/publications/crosscheck-publications.ipynb\n",
    "\n",
    "Checks possible Wikidata records against publications in CrossRef and PubMed to see if the author metadata will disambiguate the Wikidata record.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acceptMediaType = 'application/json'\n",
    "requestHeaderDictionary = generateHeaderDictionary(acceptMediaType)\n",
    "\n",
    "def generateNameAlternatives(name):\n",
    "    # get rid of periods\n",
    "    name = name.replace('.', '')\n",
    "    pieces = name.split(' ')\n",
    "    \n",
    "    # generate initials for all names\n",
    "    initials = []\n",
    "    for piece in pieces:\n",
    "        initials.append(piece[0:1])\n",
    "    \n",
    "    # NOTE: currently doesn't handle \", Jr.\", \"III\", etc.\n",
    "    \n",
    "    alternatives = []\n",
    "    # full name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += pieces[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # first and last name with initials and periods\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first and last name only\n",
    "    nameVersion = pieces[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial and last name only\n",
    "    nameVersion = initials[0] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # first initial with period and last name only\n",
    "    nameVersion = initials[0] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with last name\n",
    "    nameVersion = initials[0] + ' '\n",
    "    for pieceNumber in range(1, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + ' '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials with periods with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber] + '. '\n",
    "    nameVersion += pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "\n",
    "    # all name initials concatenated with last name\n",
    "    nameVersion = ''\n",
    "    for pieceNumber in range(0, len(pieces)-1):\n",
    "        nameVersion += initials[pieceNumber]\n",
    "    nameVersion += ' ' + pieces[len(pieces)-1]\n",
    "    alternatives.append(nameVersion)\n",
    "    \n",
    "    # remove duplicates\n",
    "    dedupe = list(set(alternatives))\n",
    "\n",
    "    return dedupe\n",
    "\n",
    "def searchNameAtWikidata(name):\n",
    "    nameList = generateNameAlternatives(name)\n",
    "    alternatives = ''\n",
    "    for alternative in nameList:\n",
    "        alternatives += '\"' + alternative + '\"@en\\n'\n",
    "    query = '''\n",
    "select distinct ?item ?label where {\n",
    "  VALUES ?value\n",
    "  {\n",
    "  ''' + alternatives + '''}\n",
    "?item rdfs:label|skos:altLabel ?value.\n",
    "?item rdfs:label ?label.\n",
    "FILTER(lang(?label)='en')\n",
    "  }\n",
    "'''\n",
    "    #print(query)\n",
    "    #print('searching for ', name)\n",
    "    results = []\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            wikidataIri = statement['item']['value']\n",
    "            if 'label' in statement:\n",
    "                name = statement['label']['value']\n",
    "            else:\n",
    "                name = ''\n",
    "            qNumber = extractQNumber(wikidataIri)\n",
    "            results.append({'qId': qNumber, 'name': name})\n",
    "    except:\n",
    "        results = [{'error': r.text}]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return results\n",
    "\n",
    "# returns a dictionary of various descriptors of the item with Wikidata ID qId\n",
    "# P106 is occupation, schema:description is filtered to be the English description\n",
    "def searchWikidataDescription(qId):\n",
    "    resultsDict = {}\n",
    "    query = '''select distinct ?description ?orcid ?occupation where {\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' schema:description ?description.\n",
    "            FILTER(lang(?description) = 'en')\n",
    "            }\n",
    "        optional {\n",
    "            wd:'''+ qId + ''' wdt:P106 ?occupationId.\n",
    "            ?occupationId rdfs:label ?occupation.\n",
    "            FILTER(lang(?occupation) = 'en')            \n",
    "            }\n",
    "        optional {wd:'''+ qId + ''' wdt:P496 ?orcid.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        if len(statements) > 0: # if no results, the dictionary remains empty\n",
    "            # Only a single description per language is allowed, so there should only be one description\n",
    "            if 'description' in statements[0]:\n",
    "                description = statements[0]['description']['value']\n",
    "            else:\n",
    "                description = ''\n",
    "            resultsDict['description'] = description\n",
    "            \n",
    "            # Only a single ORCID is allowed, so there should only be one orcid value\n",
    "            if 'orcid' in statements[0]:\n",
    "                orcid = statements[0]['orcid']['value']\n",
    "            else:\n",
    "                orcid = ''\n",
    "            resultsDict['orcid'] = orcid\n",
    "            \n",
    "            # if there are multiple statements, that's because there are more than one occupation\n",
    "            occupationList = []\n",
    "            for statement in statements:\n",
    "                if 'occupation' in statement:\n",
    "                    occupationList.append(statement['occupation']['value'])\n",
    "            resultsDict['occupation'] = occupationList\n",
    "    except:\n",
    "        resultsDict = {'error': r.text}\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsDict\n",
    "\n",
    "# returns a list of results of articles by person with Wikidata ID qId\n",
    "def searchWikidataArticle(qId):\n",
    "    resultsList = []\n",
    "    # P50 is \"author\"; P698 is the PubMed ID of the article; P356 is the DOI of the article\n",
    "    query = '''select distinct ?title ?doi ?pmid where {\n",
    "      ?article wdt:P50 wd:''' + qId + '''.\n",
    "      optional {\n",
    "          ?article rdfs:label ?title.\n",
    "          FILTER(lang(?title) = 'en')\n",
    "          }\n",
    "      optional {?article wdt:P698 ?pmid.}\n",
    "      optional {?article wdt:P356 ?doi.}\n",
    "      }'''\n",
    "    #print(query)\n",
    "    r = requests.get(wikidataEndpointUrl, params={'query' : query}, headers=requestHeaderDictionary)\n",
    "    try:\n",
    "        data = r.json()\n",
    "        statements = data['results']['bindings']\n",
    "        for statement in statements:\n",
    "            if 'title' in statement:\n",
    "                title = statement['title']['value']\n",
    "                #print('title=',title)\n",
    "            else:\n",
    "                title = ''\n",
    "            if 'pmid' in statement:\n",
    "                pmid = statement['pmid']['value']\n",
    "            else:\n",
    "                pmid = ''\n",
    "            if 'doi' in statement:\n",
    "                doi = statement['doi']['value']\n",
    "            else:\n",
    "                doi = ''\n",
    "            resultsList.append({'title': title, 'pmid': pmid, 'doi': doi})\n",
    "    except:\n",
    "        resultsList = [r.text]\n",
    "    # delay a quarter second to avoid hitting the SPARQL endpoint too rapidly\n",
    "    sleep(0.25)\n",
    "    return resultsList\n",
    "\n",
    "def retrievePubMedData(pmid):\n",
    "    fetchUrl = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi'\n",
    "    paramDict = {\n",
    "        'tool': toolName, \n",
    "        'email': emailAddress,\n",
    "        'db': 'pubmed', \n",
    "         #'retmode': 'xml', \n",
    "        'rettype': 'abstract', \n",
    "        'id': pmid\n",
    "    }\n",
    "    response = requests.get(fetchUrl, params=paramDict)    \n",
    "    #print(response.url)\n",
    "    if response.status_code == 404:\n",
    "        affiliations = [] # return an empty list if the constructed URL won't dereference\n",
    "    else:\n",
    "        pubData = response.text  # the response text is XML\n",
    "        #print(pubData)  # uncomment this line to see the XML\n",
    "\n",
    "        # process the returned XML, see https://docs.python.org/2/library/xml.etree.elementtree.html\n",
    "        root = et.fromstring(pubData)\n",
    "        try:\n",
    "            title = root.findall('.//ArticleTitle')[0].text\n",
    "        except:\n",
    "            title = ''\n",
    "        names = root.findall('.//Author')\n",
    "        affiliations = []\n",
    "        for name in names:\n",
    "            try:\n",
    "                affiliation = name.find('./AffiliationInfo/Affiliation').text\n",
    "            except:\n",
    "                affiliation = ''\n",
    "            try:\n",
    "                lastName = name.find('./LastName').text\n",
    "            except:\n",
    "                lastName = ''\n",
    "            try:\n",
    "                foreName = name.find('./ForeName').text\n",
    "            except:\n",
    "                foreName = ''\n",
    "            try:\n",
    "                idField = name.find('./Identifier')\n",
    "                if idField.get('Source') == 'ORCID':\n",
    "                    orcid = idField.text\n",
    "                else:\n",
    "                    orcid = ''\n",
    "            except:\n",
    "                orcid = ''\n",
    "\n",
    "            #print(lastName)\n",
    "            #print(affiliation)\n",
    "            affiliations.append({'affiliation': affiliation, 'surname': lastName, 'forename': foreName, 'orcid': orcid})\n",
    "        #print()\n",
    "\n",
    "    # See https://www.ncbi.nlm.nih.gov/books/NBK25497/ for usage guidelines. \n",
    "    # An API key is required for more than 3 requests per second.\n",
    "    sleep(0.5) # wait half a second before hitting the API again to avoid getting blocked\n",
    "    return affiliations\n",
    "\n",
    "def retrieveCrossRefDoi(doi):\n",
    "    authorList = []\n",
    "    crossRefEndpointUrl = 'https://api.crossref.org/works/'\n",
    "    encodedDoi = urllib.parse.quote(doi)\n",
    "    searchUrl = crossRefEndpointUrl + encodedDoi\n",
    "    acceptMediaType = 'application/json'\n",
    "    response = requests.get(searchUrl, headers=generateHeaderDictionary(acceptMediaType))\n",
    "    if response.status_code == 404:\n",
    "        authorList = [] # return an empty list if the DOI won't dereference at CrossRef\n",
    "    else:\n",
    "        try:\n",
    "            data = response.json()\n",
    "            #print(json.dumps(data, indent = 2))\n",
    "            if 'author' in data['message']:\n",
    "                authors = data['message']['author']\n",
    "                for author in authors:\n",
    "                    authorDict = {}\n",
    "                    if 'ORCID' in author:\n",
    "                        authorDict['orcid'] = author['ORCID']\n",
    "                    else:\n",
    "                        authorDict['orcid'] = ''\n",
    "                    if 'given' in author:\n",
    "                        authorDict['givenName'] = author['given']\n",
    "                    else:\n",
    "                        authorDict['givenName'] = ''\n",
    "                    if 'family' in author:\n",
    "                        authorDict['familyName'] = author['family']\n",
    "                    else:\n",
    "                        authorDict['familyName'] = ''\n",
    "                    affiliationList = []\n",
    "                    if 'affiliation' in author:\n",
    "                        for affiliation in author['affiliation']:\n",
    "                            affiliationList.append(affiliation['name'])\n",
    "                    # if there aren't any affiliations, the list will remain empty\n",
    "                    authorDict['affiliation'] = affiliationList\n",
    "                    authorList.append(authorDict)\n",
    "        except:\n",
    "            authorList = [data]\n",
    "    return authorList\n",
    "\n",
    "# ***** BODY OF SEARCH\n",
    "filename = deptShortName + '-employees-with-wikidata.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "#for employeeIndex in range(0, len(employees)):\n",
    "for employeeIndex in range(11, 50): # just do one person for testing\n",
    "    # perform search only for people who weren't already matched\n",
    "    if employees[employeeIndex]['wikidataStatus'] == '0':\n",
    "        matchStatus = 0\n",
    "        print('--------------------------')\n",
    "        results = searchNameAtWikidata(employees[employeeIndex]['name'])\n",
    "        print('Position: ', employees[employeeIndex]['position'], ', Specialities: ', employees[employeeIndex]['specialities'])\n",
    "        print('Born: ', employees[employeeIndex]['birth_date'], ', Herb code: ', employees[employeeIndex]['herb_code'], ', Place: ', employees[employeeIndex]['city'], ', ', employees[employeeIndex]['state'])\n",
    "        if len(results) == 0:\n",
    "            print('No Wikidata name match: ', employees[employeeIndex]['name'])\n",
    "            matchStatus = 7\n",
    "            print()\n",
    "        else:\n",
    "            print('SPARQL name search: ', employees[employeeIndex]['name'])\n",
    "            if len(results) == 1:\n",
    "                if 'error' in results[0]:\n",
    "                    matchStatus = 9\n",
    "                    print('Error message in\n",
    "                          name search:', results[0]['error'])\n",
    "                    break # discontinue processing this person\n",
    "            qIds = []\n",
    "            nameVariants = []\n",
    "            potentialOrcid = []\n",
    "            for result in results:\n",
    "                qIds.append(result['qId'])\n",
    "                nameVariants.append(result['name'])\n",
    "            \n",
    "            testAuthor = employees[employeeIndex]['name']\n",
    "            testOrcid = employees[employeeIndex]['orcid']\n",
    "\n",
    "            if testOrcid == '':\n",
    "                print('(no ORCID)')\n",
    "            else:\n",
    "                print('ORCID: ', testOrcid)\n",
    "            print()\n",
    "            \n",
    "            foundMatch = False # start the flag with the person not being matched\n",
    "            possibleMatch = False # start the flag with there not being a possibility that the person could match\n",
    "            for qIdIndex in range(0, len(qIds)):\n",
    "                potentialOrcid.append('') # default to no ORCID found for that person\n",
    "                print()\n",
    "                print(qIdIndex, 'Wikidata ID: ', qIds[qIdIndex], ' Name variant: ', nameVariants[qIdIndex], ' ', 'https://www.wikidata.org/wiki/' + qIds[qIdIndex])\n",
    "                wdClassList = searchWikidataSingleProperty(qIds[qIdIndex], 'P31', 'item')\n",
    "                # if there is a class property, check if it's a human\n",
    "                if len(wdClassList) != 0:\n",
    "                    # if it's not a human\n",
    "                    if wdClassList[0] != 'Q5':\n",
    "                        print('This item is not a human!')\n",
    "                        break\n",
    "                        \n",
    "                # check for a death date\n",
    "                deathDateList = searchWikidataSingleProperty(qIds[qIdIndex], 'P570', 'string')\n",
    "                if len(deathDateList) == 0:\n",
    "                    print('No death date given.')\n",
    "                else:\n",
    "                    deathDate = deathDateList[0][0:10] # all dates are converted to xsd:dateTime and will have a y-m-d date\n",
    "                    if deathDate < deathDateLimit:\n",
    "                        # if the person died a long time ago, don't retrieve other stuff\n",
    "                        print('This person died in ', deathDate)\n",
    "                        break\n",
    "                    else:\n",
    "                        # if the person died recently, we still might be interested in them so keep going\n",
    "                        print('This person died in ', deathDate)\n",
    "\n",
    "                # check for a birth date\n",
    "                if employees[employeeIndex]['birth_date'] != '': # only check Wikidata if the person has a birthdate\n",
    "                    birthDateList = searchWikidataSingleProperty(qIds[qIdIndex], 'P569', 'string')\n",
    "                    if len(birthDateList) == 0: # do nothing if there are no birthdates retrieved from Wikidata\n",
    "                        print('No birth date given.')\n",
    "                    else:\n",
    "                        birthDate = birthDateList[0][0:4] # get only the first four digits since only years are given\n",
    "                        if birthDate != employees[employeeIndex]['birth_date']:\n",
    "                            print('Wikidata birthdate ', birthDate, ' does not match ', employees[employeeIndex]['birth_date'])\n",
    "                            break\n",
    "\n",
    "                descriptors = searchWikidataDescription(qIds[qIdIndex])\n",
    "                employers = searchWikidataEmployer(qIds[qIdIndex])\n",
    "                #print(descriptors)\n",
    "                if descriptors != {}:\n",
    "                    if descriptors['description'] != '':\n",
    "                        print('description: ', descriptors['description'])\n",
    "                    for occupation in descriptors['occupation']:\n",
    "                        print('occupation: ', occupation)\n",
    "                    for employer in employers:\n",
    "                        print('employer: ', employer)\n",
    "                    if descriptors['orcid'] != '':\n",
    "                        if testOrcid == '':\n",
    "                            # **** NOTE: if the person has an ORCID, it may be possible to find articles via ORCID\n",
    "                            # that aren't linked in Wikidata. Not sure if this happens often enough to handle it\n",
    "                            print('ORCID: ', descriptors['orcid'])\n",
    "                            potentialOrcid[qIdIndex] = descriptors['orcid']\n",
    "                        else:\n",
    "                            # This should always be true if the SPARQL query for ORCID was already done\n",
    "                            if testOrcid != descriptors['orcid']:\n",
    "                                print('*** NOT the same person; ORCID ' + descriptors['orcid'] + ' does not match.')\n",
    "                                break # don't continue the loop (look up references) since it's definitely not a match\n",
    "                            else:\n",
    "                                print('*** An ORCID match! How did it get missed in the earlier SPARQL query?')\n",
    "                                break\n",
    "                else:\n",
    "                    print('No description or occupation given.')\n",
    "\n",
    "                result = searchWikidataArticle(qIds[qIdIndex])\n",
    "                if len(result) == 0:\n",
    "                    print('No articles authored by that person')\n",
    "                else:\n",
    "                    articleCount = 0\n",
    "                    for article in result:\n",
    "                        print()\n",
    "                        print('Checking article: ', article['title'])\n",
    "                        if article['pmid'] == '':\n",
    "                            print('No PubMed ID')\n",
    "                        else:\n",
    "                            print('Checking authors in PubMed article: ', article['pmid'])\n",
    "                            pubMedAuthors = retrievePubMedData(article['pmid'])\n",
    "                            if pubMedAuthors == []:\n",
    "                                print('PubMed ID does not seem to be valid.')\n",
    "                            #print(pubMedAuthors)\n",
    "                            for author in pubMedAuthors:\n",
    "                                nameTestRatio = fuzz.token_set_ratio(author['surname'], testAuthor)\n",
    "                                #print(nameTestRatio, author['surname'])\n",
    "                                if nameTestRatio >= 90:\n",
    "                                    # if the PubMed metadata gives an ORCID for the matched person, record it unless \n",
    "                                    # the ORCID has already been gotten from the Wikidata record\n",
    "                                    if author['orcid'] != '':\n",
    "                                        if testOrcid == '':\n",
    "                                            print('ORCID from article: ', author['orcid'])\n",
    "                                            if potentialOrcid[qIdIndex] == '':\n",
    "                                                potentialOrcid[qIdIndex] = author['orcid']\n",
    "                                        else:\n",
    "                                            if testOrcid != author['orcid']:\n",
    "                                                print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                            else:\n",
    "                                                print('*** An ORCID match!')\n",
    "                                                foundMatch = True\n",
    "                                                matchStatus = 6\n",
    "                                                break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "                                    if author['affiliation'] != '': \n",
    "                                        setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], author['affiliation'])\n",
    "                                        print('Affiliation test: ', setRatio, author['affiliation'])\n",
    "                                        if setRatio >= 90:\n",
    "                                            foundMatch = True\n",
    "                                            matchStatus = 10\n",
    "                                            break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                    else:\n",
    "                                        break # give up on this article because no affiliation string\n",
    "                        # Don't look up the DOI if it's already found a match with PubMed\n",
    "                        if foundMatch:\n",
    "                            break # stop checking articles after a PubMed one has matched\n",
    "                        else:\n",
    "                            if article['doi'] == '':\n",
    "                                print('No DOI')\n",
    "                            else:\n",
    "                                print('Checking authors in DOI article: ', article['doi'])\n",
    "                                doiAuthors = retrieveCrossRefDoi(article['doi'])\n",
    "                                if doiAuthors == []:\n",
    "                                    print('DOI does not dereference at CrossRef')\n",
    "                                for author in doiAuthors:\n",
    "                                    nameTestRatio = fuzz.token_set_ratio(author['familyName'], testAuthor)\n",
    "                                    #print(nameTestRatio, author['familyName'])\n",
    "                                    if nameTestRatio >= 90:\n",
    "                                        if author['orcid'] != '':\n",
    "                                            if testOrcid == '':\n",
    "                                                # DOI records the entire ORCID URI, not just the ID number\n",
    "                                                # so pull the last 19 characters from the string\n",
    "                                                print('ORCID from article: ', author['orcid'][-19:])\n",
    "                                                # only add the ORCID from article if there isn't already one,\n",
    "                                                # for example, one gotten from the Wikidata record itself\n",
    "                                                if potentialOrcid[qIdIndex] == '':\n",
    "                                                    potentialOrcid[qIdIndex] = author['orcid'][-19:]\n",
    "                                            else:\n",
    "                                                if testOrcid != author['orcid']:\n",
    "                                                    print('*** NOT the same person; ORCID ' + author['orcid'] + ' does not match.')\n",
    "                                                    break # don't continue the loop (look up authors) since it's definitely not a match\n",
    "                                                else:\n",
    "                                                    print('*** An ORCID match!')\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 6\n",
    "                                                    break # don't continue the loop (look up authors) since it's an ORCID match\n",
    "\n",
    "\n",
    "                                        if len(author['affiliation']) > 0:\n",
    "                                            for affiliation in author['affiliation']:\n",
    "                                                setRatio = fuzz.token_set_ratio(deptSettings[deptShortName]['testAuthorAffiliation'], affiliation)\n",
    "                                                print('Affiliation test: ', setRatio, affiliation)\n",
    "                                                if setRatio >= 90:\n",
    "                                                    foundMatch = True\n",
    "                                                    matchStatus = 10\n",
    "                                                    break # don't continue the loop (look up authors) since it's an affiliation match\n",
    "                                        else:\n",
    "                                            break # give up on this article because no affiliation string\n",
    "                            if foundMatch:\n",
    "                                break # stop checking articles after a DOI one has matched\n",
    "                        articleCount += 1\n",
    "                        if articleCount > 10:\n",
    "                            checkMore = input('There are more than 10 articles. Press Enter to skip the rest or enter anything to get the rest.')\n",
    "                            if checkMore == '':\n",
    "                                break\n",
    "                    if foundMatch:\n",
    "                        print('***', qIds[qIdIndex], ' is a match.')\n",
    "                        print()\n",
    "                        employees[employeeIndex]['wikidataId'] = qIds[qIdIndex]\n",
    "                        employees[employeeIndex]['orcid'] = potentialOrcid[qIdIndex]\n",
    "                        break # quit checking Q IDs since the person was matched\n",
    "                    else:\n",
    "                        print('No match found.')\n",
    "                print('Employee: ', employees[employeeIndex]['name'], ' vs. name variant: ', nameVariants[qIdIndex])\n",
    "                possibleMatch = True # made it all the way through the loop without hitting a break, so a match is possible\n",
    "                print()\n",
    "            if not foundMatch:\n",
    "                if not possibleMatch:\n",
    "                    matchStatus = 12\n",
    "                else:\n",
    "                    choiceString = input('Enter the number of the matched entity, or press Enter/return if none match: ')\n",
    "                    if choiceString == '':\n",
    "                        matchStatus = 7\n",
    "                    else:\n",
    "                        # NOTE: there is no error trapping here for mis-entry !!!\n",
    "                        choice = int(choiceString)\n",
    "                        matchStatus = 11\n",
    "                        employees[employeeIndex]['wikidataId'] = qIds[choice]\n",
    "                        # write a discovered ORCID only if the person didn't already have one\n",
    "                        if (potentialOrcid[choice] != '') and (employees[employeeIndex]['orcid'] == ''):\n",
    "                            employees[employeeIndex]['orcid'] = potentialOrcid[choice]\n",
    "                    print()\n",
    "                \n",
    "        # record the final match status\n",
    "        employees[employeeIndex]['wikidataStatus'] = str(matchStatus)\n",
    "    \n",
    "    # write the file after each person is checked in case the user crashes the script\n",
    "    filename = deptShortName + '-employees-curated.csv'\n",
    "    fieldnames = ['wikidataId', 'name', 'irn', 'herb_code', 'birth_date', 'position', 'specialities', 'city', 'state', 'wikidataStatus', 'orcid']\n",
    "    writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download various statements and references, then generate write file\n",
    "\n",
    "NOTE: between the previous step and this one, one can add a gender/sex column to the table that will be processed if it exists.  Column header: 'gender'.  Allowed values (from Wikidata): m=male, f=female, i=intersex, tf=transgender female, tm=transgender male"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = deptShortName + '-employees-curated.csv'\n",
    "employees = readDict(filename)\n",
    "\n",
    "# create a list of the employees who have Wikidata qIDs\n",
    "qIds = []\n",
    "for employee in employees:\n",
    "    if employee['wikidataId'] != '':\n",
    "        qIds.append(employee['wikidataId'])\n",
    "\n",
    "# get all of the ORCID data that is already in Wikidata\n",
    "prop = 'P496' # ORCID iD\n",
    "value = '' # since no value is passed, the search will retrieve the value\n",
    "refProps = ['P813'] # retrieved\n",
    "wikidataOrcidData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "#print(json.dumps(wikidataOrcidData, indent=2))\n",
    "\n",
    "# match people who have ORCIDs with ORCID data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataOrcidDataIndex in range(0, len(wikidataOrcidData)):\n",
    "        if wikidataOrcidData[wikidataOrcidDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            if employees[employeeIndex]['orcid'] != wikidataOrcidData[wikidataOrcidDataIndex]['statementValue']:\n",
    "                print('Non-matching ORCID for ', employees[employeeIndex]['name'])\n",
    "            # if there is a match, record whatever data was retrieved\n",
    "            else:\n",
    "                employees[employeeIndex]['orcidStatementUuid'] = wikidataOrcidData[wikidataOrcidDataIndex]['statementUuid']\n",
    "                employees[employeeIndex]['orcidReferenceHash'] = wikidataOrcidData[wikidataOrcidDataIndex]['referenceHash']\n",
    "                # if there is no referenceHash then try to dereference the ORCID\n",
    "                if employees[employeeIndex]['orcidReferenceHash']== '':\n",
    "                    # if there is a match, check whether the ORCID record can be retrieved\n",
    "                    print('Checking ORCID for Wikidata matched: ', employees[employeeIndex]['name'])\n",
    "                    # returned value is the current date if successful; empty string if not\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "                # if there is an existing reference, record the value for the first reference property (only one ref property)\n",
    "                else:\n",
    "                    print('Already an ORCID reference for: ', employees[employeeIndex]['name'])\n",
    "                    # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                    employees[employeeIndex]['orcidReferenceValue'] = '+' + wikidataOrcidData[wikidataOrcidDataIndex]['referenceValues'][0]\n",
    "            # stop checking at the first match.\n",
    "            break\n",
    "    # if the person doesn't match with those whose ORCIDs came back from the query...\n",
    "    if not matched:\n",
    "        # check for access if they have an ORCID (not present in Wikidata)\n",
    "        if employees[employeeIndex]['orcid'] != '':\n",
    "            print('Checking ORCID for unmatched: ', employees[employeeIndex]['name'])\n",
    "            # the function returns the current date (to use as the retrieved date) if the ORCID is found, otherwise empty string\n",
    "            employees[employeeIndex]['orcidReferenceValue'] = checkOrcid(employees[employeeIndex]['orcid'])\n",
    "\n",
    "# get data already in Wikidata about people employed at Vanderbilt\n",
    "prop = 'P108' # employer\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, employerQId, refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with employment data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['employerStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['employerReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['employerReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['employerReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['employerReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrite any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "    # everyone is assigned the employerQId as a value because either they showed up in the SPARQL search for employerQId\n",
    "    # or we are making a statement that they work for employerQId.\n",
    "    employees[employeeIndex]['employer'] = employerQId\n",
    "    if not matchedReference:  # generate the reference metadata if the reference URL wasn't found\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['employerReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['employerReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# *** This is a copy and paste of the employer section above, modified for affiliation\n",
    "\n",
    "# get data already in Wikidata about people affiliated with the department\n",
    "prop = 'P1416' # affiliation\n",
    "refProps = ['P854', 'P813'] # source URL, retrieved\n",
    "wikidataEmployerData = searchStatementAtWikidata(qIds, prop, deptSettings[deptShortName]['departmentQId'], refProps)\n",
    "#print(json.dumps(wikidataEmployerData, indent=2))\n",
    "\n",
    "# match people with affiliation data downloaded from Wikidata\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matchedStatement = False\n",
    "    matchedReference = False\n",
    "    for wikidataEmployerDataIndex in range(0, len(wikidataEmployerData)):\n",
    "        if wikidataEmployerData[wikidataEmployerDataIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matchedStatement = True\n",
    "            employees[employeeIndex]['affiliationStatementUuid'] = wikidataEmployerData[wikidataEmployerDataIndex]['statementUuid']\n",
    "            employees[employeeIndex]['affiliationReferenceHash'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceHash']\n",
    "            # if there is a referenceHash then record the values for the two reference properties: P813, P854'; retrieved, source URL\n",
    "            if employees[employeeIndex]['affiliationReferenceHash']!= '':\n",
    "                # need to add the + in front of dateTime, which is needed by the API for upload\n",
    "                employees[employeeIndex]['affiliationReferenceSourceUrl'] = wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0]\n",
    "                if wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][0] == deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']:\n",
    "                    matchedReference = True\n",
    "                employees[employeeIndex]['affiliationReferenceRetrieved'] = '+' + wikidataEmployerData[wikidataEmployerDataIndex]['referenceValues'][1]\n",
    "            # stop checking if there is an exact match to the reference URL. Otherwise keep looping.\n",
    "            # if there is a later reference that matches ours, it will overwrited any previous reference data\n",
    "            # otherwise, the existing (different) reference data will be retained\n",
    "            if matchedReference:\n",
    "                break\n",
    "        \n",
    "    # everyone is assigned the department as a value because either they showed up in the SPARQL search\n",
    "    # or we are making a statement that they are affiliated with the department.\n",
    "    employees[employeeIndex]['affiliation'] = deptSettings[deptShortName]['departmentQId']\n",
    "    if not matchedReference:  # generate the reference metadata if the reference URL wasn't found\n",
    "        wholeTimeStringZ = datetime.datetime.utcnow().isoformat() # form: 2019-12-05T15:35:04.959311\n",
    "        dateZ = wholeTimeStringZ.split('T')[0] # form 2019-12-05\n",
    "        wholeDateZ = '+' + dateZ + 'T00:00:00Z' # form +2019-12-05T00:00:00Z as provided by Wikidata\n",
    "        employees[employeeIndex]['affiliationReferenceSourceUrl'] = deptSettings[deptShortName]['baseUrl'] + employees[employeeIndex]['category']\n",
    "        employees[employeeIndex]['affiliationReferenceRetrieved'] = wholeDateZ\n",
    "\n",
    "# get all of the data that is already in Wikidata about who are humans\n",
    "prop = 'P31' # instance of\n",
    "value = 'Q5' # human\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions that they are humans and record their statement IDs.\n",
    "# Assign the properties to all others.\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            employees[employeeIndex]['instanceOfUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "    # everybody is assigned a value of 'human'\n",
    "    employees[employeeIndex]['instanceOf'] = 'Q5'\n",
    "\n",
    "# hack of human code immediately above\n",
    "\n",
    "# get all of the data that is already in Wikidata about the sex or gender of the researchers\n",
    "prop = 'P21' # sex or gender\n",
    "value = '' # don't provide a value so that it will return whatever value it finds\n",
    "refProps = [] # no ref property needed\n",
    "wikidataHumanData = searchStatementAtWikidata(qIds, prop, value, refProps)\n",
    "\n",
    "# Find out which people have assertions of sex/gender and record their statement IDs.\n",
    "# Assign the value for the property to all others.\n",
    "# NOTE: Wikidata doesn't seem to care a lot about references for this property and we don't really have one anyway\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataHumanIndex in range(0, len(wikidataHumanData)):\n",
    "        if wikidataHumanData[wikidataHumanIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['sexOrGenderUuid'] = wikidataHumanData[wikidataHumanIndex]['statementUuid']\n",
    "            # use the value in Wikidata and ignore the value in the 'gender' column of the table.\n",
    "            # extractFromIri() function strips the namespace from the qId\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = extractFromIri(wikidataHumanData[wikidataHumanIndex]['statementValue'], 4)\n",
    "    if not matched:\n",
    "        # assign the value from the 'gender' column in the table if not already in Wikidata\n",
    "        if 'gender' in employees[employeeIndex]:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = decodeSexOrGender(employees[employeeIndex]['gender'])\n",
    "        else:\n",
    "            employees[employeeIndex]['sexOrGenderQId'] = ''\n",
    "\n",
    "# get all of the English language labels for the employees that are already in Wikidata\n",
    "labelType = 'label'\n",
    "language = 'en'\n",
    "wikidataLabels = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their labels\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataLabelIndex in range(0, len(wikidataLabels)):\n",
    "        if wikidataLabels[wikidataLabelIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['labelEn'] = wikidataLabels[wikidataLabelIndex]['string']\n",
    "    if not matched:\n",
    "        # assign the value from the 'name' column in the table if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['labels']['source'] == 'column':\n",
    "            # then use the value from the default label column.\n",
    "            defaultLabelColumn = deptSettings[deptShortName]['labels']['value']\n",
    "            employees[employeeIndex]['labelEn'] = employees[employeeIndex][defaultLabelColumn]\n",
    "        else:\n",
    "            # or use the default label value.\n",
    "            employees[employeeIndex]['labelEn'] = deptSettings[deptShortName]['labels']['value']\n",
    "\n",
    "# get all of the English language descriptions for the employees that are already in Wikidata\n",
    "labelType = 'description'\n",
    "language = 'en'\n",
    "wikidataDescriptions = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "\n",
    "# Match people with their descriptions\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    matched = False\n",
    "    for wikidataDescriptionIndex in range(0, len(wikidataDescriptions)):\n",
    "        if wikidataDescriptions[wikidataDescriptionIndex]['qId'] == employees[employeeIndex]['wikidataId']:\n",
    "            matched = True\n",
    "            employees[employeeIndex]['description'] = wikidataDescriptions[wikidataDescriptionIndex]['string']\n",
    "    if not matched:\n",
    "        # assign a default value if not already in Wikidata\n",
    "        if deptSettings[deptShortName]['descriptions']['source'] == 'column':\n",
    "            # then use the value from the default description column.\n",
    "            defaultDescriptionColumn = deptSettings[deptShortName]['descriptions']['value']\n",
    "            employees[employeeIndex]['description'] = employees[employeeIndex][defaultDescriptionColumn]\n",
    "        else:\n",
    "            # or use the default description value.\n",
    "            employees[employeeIndex]['description'] = deptSettings[deptShortName]['descriptions']['value']\n",
    "\n",
    "# Get all of the aliases already at Wikidata for employees.  \n",
    "# Since there can be multiple aliases, they are stored as a list structure.\n",
    "# The writing script can handle multiple languages, but here we are only dealing with English ones.\n",
    "\n",
    "# retrieve the aliases in that language that already exist in Wikidata and match them with table rows\n",
    "labelType = 'alias'\n",
    "language = 'en'\n",
    "aliasesAtWikidata = searchLabelsDescriptionsAtWikidata(qIds, labelType, language)\n",
    "for entityIndex in range(0, len(employees)):\n",
    "    personAliasList = []\n",
    "    if employees[entityIndex]['wikidataId'] != '':  # don't look for the label at Wikidata if the item doesn't yet exist\n",
    "        for wikiLabel in aliasesAtWikidata:\n",
    "            if employees[entityIndex]['wikidataId'] == wikiLabel['qId']:\n",
    "                personAliasList.append(wikiLabel['string'])\n",
    "    # if not found, the personAliasList list will remain empty\n",
    "    employees[entityIndex]['alias'] = json.dumps(personAliasList)\n",
    "\n",
    "# set the departmental short name for all entities\n",
    "for employeeIndex in range(0, len(employees)):\n",
    "    employees[employeeIndex]['department'] = deptShortName\n",
    "\n",
    "# write the file\n",
    "filename = deptShortName + '-employees-to-write.csv'\n",
    "fieldnames = ['department', 'wikidataId', 'name', 'labelEn', 'alias', 'description', 'orcidStatementUuid', 'orcid', 'orcidReferenceHash', 'orcidReferenceValue', 'employerStatementUuid', 'employer', 'employerReferenceHash', 'employerReferenceSourceUrl', 'employerReferenceRetrieved', 'affiliationStatementUuid', 'affiliation', 'affiliationReferenceHash', 'affiliationReferenceSourceUrl', 'affiliationReferenceRetrieved', 'instanceOfUuid', 'instanceOf', 'sexOrGenderUuid', 'sexOrGenderQId', 'gender', 'degree', 'category', 'wikidataStatus', 'role']\n",
    "writeDictsToCsv(employees, filename, fieldnames)\n",
    "\n",
    "print()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set file name in CSV metadata file\n",
    "\n",
    "Prior to writing the data to Wikidata using the `process_csv_metadata_full.py` script, the input file name needs to be changed in the `csv-metadata.json` file to have the correct `deptShortName` for the department. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('csv-metadata.json', 'rt', encoding='utf-8') as inFileObject:\n",
    "    text = inFileObject.read()\n",
    "schema = json.loads(text)\n",
    "schema['tables'][0]['url'] = deptShortName + '-employees-to-write.csv'\n",
    "outText = json.dumps(schema, indent = 2)\n",
    "with open('csv-metadata.json', 'wt', encoding='utf-8') as outFileObject:\n",
    "    outFileObject.write(outText)\n",
    "print('Department to be written:', deptShortName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
